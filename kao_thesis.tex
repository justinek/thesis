% Stanford University PhD thesis style -- modifications to the report style
% This is unofficial so you should always double check against the
% Registrar's office rules
% See http://library.stanford.edu/research/bibliography-management/latex-and-bibtex
% 
% Example of use below
% See the suthesis-2e.sty file for documentation
%
\documentclass[oneside]{report}
\usepackage{Suthesis-2e}
\usepackage{CJKutf8}
\usepackage{longtable}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage[table]{xcolor}
\usepackage{apacite}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{url}
\usepackage{stmaryrd}

%\usepackage{gb4e}

\usepackage{amsmath,epsfig}

%\usepackage[american]{babel}

\usepackage{csquotes}
%\usepackage{xspace}
\usepackage[perpage,symbol]{footmisc}

\usepackage {tikz}
\usepackage{pgf}
\usetikzlibrary{arrows,automata}
%\usepackage[latin1]{inputenc}
\usepackage{tkz-graph}
\tikzset{main node/.style={circle,fill=gray!20,draw,minimum size=1cm,inner sep=0pt},}
\usetikzlibrary {positioning}

\newcommand{\dictionary}{\ensuremath{\mathcal{D}}\xspace}

\definecolor{Green}{RGB}{10,200,100}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}
\dept{Psychology}
\setstretch{1.6}
\begin{document}
\title{Modeling Creative and Social Uses of Language}
\author{Justine T. Kao}
\principaladviser{Noah D. Goodman}
\firstreader{Herbert H. Clark}
\secondreader{Christopher Potts}
\thirdreader{Michael C. Frank} %if needed
%\fourthreader{Dan Jurafsky} %if needed
 
\beforepreface
\prefacesection{Abstract}
%This thesis tells you all you need to know about...

%People use language to accomplish many kinds of goals, from relaying information, to expressing personal attitudes, to entertaining and connecting with those around them. 
%Creative and social uses of language are often 
%Language plays many different roles in our lives. It helps us relay information to one another, express feelings and attitudes, and entertain and connect with those around us. 
This thesis examines the computational basis for how people understand and appreciate creative uses of language. 
In the first half of the thesis, I describe a computational model that can interpret diverse types of creative and figurative language use, such as hyperbole, irony, and metaphor. In the second half, I focus on modeling the social consequences of using language in creative ways, such as strengthening social bonds and evoking humor. Across the chapters, I describe computational models that formalize theories of language comprehension and show that these formalizations
predict people's interpretations of a wide range of creative language use. 
%give rise to the interpretations and responses that people produce.
%I present a computational model that predicts social inferences licensed by figurative language as well a model that explains the humor in word play.


Part of the goal of this thesis is to show that the interpretation of creative and social uses of language arises  through many of the same rational principles that govern standard language understanding. 
In Chapter 2, I formalize basic principles of communication by extending the Rational Speech-Acts (RSA) framework \cite{frank2012predicting} with an explicit representation of speakers' communicative goals. I test the extended model on nonliteral uses of number words such as hyperbole, and show that the model produces interpretations that closely match people's by reasoning about the speaker's communicative goal and emotional attitude. 
%in particular the ide a family of computational models the idea that listeners reason about speakers' communicative goals in order Rational Speech-Acts framework by introducing the idea that  introduce the idea of communicative goals to the RSA framework and formalize a notion of the relevance principle. 
%I show that these basic principles of communication, combined with background knowledge and social reasoning, allows the model to appropriately interpret hyperbolic utterances as well as their associated affects. 
%These results suggest that the affective subtext of creative and figurative uses of language may arise naturally from standard language understanding mechanisms. 
In Chapter 3, I further extend the space of communicative goals considered by the RSA model to include a richer representation of emotions as well as contextually determined topics of conversation. I show that this simple extension allows the RSA model to interpret other figurative uses such as verbal irony and metaphor, suggesting that diverse figurative meanings may arise from the same principles of communication. In Chapter 4, I explore the social consequences of figurative language use by formalizing the idea that listeners reason about common ground during communication. Through a series of simulations, I show that the extended RSA model infers a higher probability of common ground given figurative utterances than given literal ones, suggesting that figurative language may uniquely license inferences that strengthen social bonds. Finally, in Chapter 5, I use a simple model of sentence comprehension to derive quantitative measures of humor that are motivated by both humor theory and general theories of sentence comprehension. 
%
%  Through may be uncertain about the information is in common ground, and I relax the assumption that speakers and listeners share the same background knowledge, and allow the listener to reason about common ground given various utterances. We show that the model infers a higher probability of common ground given figurative utterances than literal ones, suggesting that figurative language may uniquely license inferences that strengthen social bonds. Finally, in Chapter 5, I use a simple model of sentence comprehension to derive quantitative measures of humor that are motivated by both humor theory and general theories of sentence comprehension. 
%With the Rational Speech-Acts framework, we were able to introduce increasingly rich and psychologically realistic components of communication, and to show how they are responsible for different effects.
Taken together, this work sheds light on creative and social uses of language and advances formal approaches to language understanding, such that computational models can explain a broader range of phenomena that enrich our linguistic and social lives.

\prefacesection{Acknowledgments}
Thank you for reading this thesis.
\afterpreface

\chapter{Introduction}

%One cannot know man without knowing the force of words.
%?????????
Human beings are creatures of play. We begin playing from the cradle, from our first box of legos to the latest games on our smartphones. Play has the magical ability to
% draw people in and to engage and connect us, to 
exercise our imaginations and to satisfy our desire to explore, create, and interact with the physical and social world. As we develop, the games we play increasingly often involve abilities that are socially valued and beneficial to our success, such as physical coordination, logic, planning, social reasoning, cooperation, and even deception. 
Despite its association with levity and fun, play is seriously and profoundly useful, enabling us to practice a range of skills that prepare us for situations in the real world. 
%Some of the most enjoyable and interesting kinds of play simulate situations in the real world and introduce us to novel experiences. 
   

%As human beings evolved, we have invented more and more things to play with: toys, card games, video games.
Given the link between play and competency, it is perhaps not a coincidence that one of our favorite things to play with is also one of our most important skills---language. From nursery rhymes to puns to metaphors and poetry, human beings are exposed to and generate various forms of creative and playful uses of language throughout development \cite{ely1994language, carter1999common, carter2004talking, cook1996language, cazden1974play}. 
%Research on first and second language acquisition suggests that playing with language is a way of practicing it and demonstrates linguistic competency, showing that we know the rules well enough to feel comfortable bending them intentionally. Like other types of play, language play is often social and involves reasoning about other people, what they mean when they say things that are out of the ordinary, and how they will react when I bend the rules of language in certain ways. Like other types of play, it is fun. 
%Language is our primary means of communication. We use language to describe events, record facts about the world, share knowledge with one another, and work together to accomplish feats ranging from assembling Ikea furniture to curing diseases and ending wars. Language is an incredibly effective tool for transmitting information, coordinating with others, and accumulating knowledge across space and time. Without such a tool, people would not be able to access rich information beyond what they can glean from their own percepts; society would not be able to implement and enforce laws; and humanity could never have made the technological and scientific advancements that it has.
It is difficult to imagine a world in which the sole purpose of language is to transmit objective information to one another, without any component of play. In such a world, 
we would still be able to describe events, record facts, share knowledge, and work together to accomplish feats ranging from assembling Ikea furniture to curing diseases and ending wars. However, we would probably all suffer from immeasurable boredom. 
%In such a world, it is possible that people would stop communicating altogether. 
Thankfully, in the world we live in, 
using language involves much more than passing definitions and commands to each other and processing them like lines of code in a computer program. 
%language accomplishes goals far beyond the transmission of raw information. 
We use language to express our individuality and subjective experiences, to entertain and be entertained, to create and appreciate works of art. The power and appeal of language lies not only in its ability to communicate raw information, but also in its potential to connect us at a social and emotional level. 

%In addition to its function as a vehicle for objective information,

%Ultimately connecting with other minds is a fundamental human desire, and language is our most effective tool for accomplishing this goal. 

The observation that the purpose of language goes beyond purely referential functions is far from a novel one. Philosophers, literary theorists, linguists, and psychologists have been interested in the emotive, social, and aesthetic functions of language for decades and have addressed them from various angles using a diverse set of methods \cite{aristotle1911trans, holtgraves2013language, halliday1971linguistic, Potts06TARGET, clark1996using, carter1999common, hall2001poetics, cook1996language, friedrich1979poetic, tannen2007talking, veale2012exploding}.
% In particular, \citeA{jakobson1960closing} pointed out the importance of a more serious treatment of what he called ``emotive'' and ``poetic'' functions of language in linguistics. 
In his essay ``Closing statement: Linguistics and poetics,'' linguist and literary theorist Roman \citeA{jakobson1960closing} wrote: ``Language must be evaluated in all the variety of its functions.'' 
However, historically the more intangible functions of language have not been the primary focus of a great deal of empirical work. In particular, approaches that involve formal and quantitative tools have largely avoided the playful and emotive uses of language because they tend to be more variable, more ambiguous, more subjective, and rely more heavily on sources of information that are external to the language itself \cite{joos1950description}. In order to make progress on understanding how language and communication work, many scientists and theorists have focused on core or idealized aspects of language that seem more amenable to formal rigor, and focused less on applying the same tools to examine higher-order effects such as linguistic creativity.
%The idea that the more fundamental/core aspects of communication need to be handled first. 
In addition, there is a prevailing notion in many fields that creative language and creativity in general resist rules and quantification, and that the quest for a formula for creative uses of language can only be a fruitless endeavor \cite{giesen2000non, boden2009computer, veale2012exploding}. 

Fortunately, recent advancements in computational tools and digital resources have blurred the lines between disciplines and introduced new methods to fields that traditionally rely on qualitative approaches. Computational tools are now used to extract patterns in complex data and to generate valuable insights in the humanities and social sciences, giving rise to fields such as digital humanities and computational social science \cite{manovich2011trending, berry2012understanding, jockers2013macroanalysis, lazer2009life, wallach2016computational}. In light of these advancements, the time is ripe for language scientists to broaden our scope and to begin understanding in a precise and quantitative manner the multitude of functions that language performs. This thesis aims to accomplish this goal by using computational models to explore the psychology of creative and social uses of language.

%\section{Language and the functions it serves}
%In his essay ``Closing statement: Linguistics and poetics,'' Russian linguist and literary theorist Roman \citeA{jakobson1960closing} wrote: ``Language must be evaluated in all the variety of its functions.'' Jakobson described six functions in particular: referential, conative, emotive, poetic, phatic, and metalingual. 
%%
%Language that performs the referential function refers to and describes a situation, object, or mental state, such as ``The''
%%
%The conative function engages or addresses the listener directly, and is commonly performed by imperitives such as `` ''.
%%
%The emotive function more directly involves the speaker and performs his goal of expressing an attitude towards the subject of conversation. Language that serves a purely emotive function (e.g. interjections and other sound changes) does not change the utterance's denotative meaning, but contain information about the speaker's attitude and internal state, e.g. "Wow, what a view!"
%%
%The poetic function focuses on the verbal message for its own sake. 
%%
%Language that performs the phatic function is often formulaic and used for the purpose of initiating, prolonging, or ending communication, such as ``Hi, how are you?''
%%
%Finally, the metalingual function refers to the use of language to discuss or describe language itself\footnote{This entire thesis can be said to perform a metalingual function, with referential and poetic functions occasionally thrown in the mix}. 
%
%The kinds of language uses that I discuss in this thesis primarily fulfill the emotive and poetic functions. 

\section{Taking language play seriously}
Why study creative uses of language? Just as studying play in children sheds light on key aspects of cognitive and social development, studying the way that adults play with language may provide insight into how we comprehend language and communicate more generally. Creative uses of language are puzzling because they often deviate from or violate linguistic norms in interesting ways \cite{ricoeur1973creativity, maybin2007everyday}. 
%How are people afble to understand uses of language that intentionally violate well-established linguistic norms? 
Figurative language, for example, is intentionally used to communicate meanings that differ dramatically from a standard, literal meaning (e.g. ``Juliet is the sun'' means that Juliet is a beautiful woman, and not that Juliet is a sphere made of hot plasma) \cite{gibbs1999figurative}.  These types of uses may be ideal for helping us examine key questions in language understanding, such as how we construct linguistic meaning from multiple sources of information, and how we flexibly recover the meanings of unusual or ambiguous utterances. Secondly, if we adopt the standard definition of creativity as the production of something both novel and useful \cite{mumford2003have, boden2009computer}, a natural question to ask is what might the ``uses'' of creative language be. What are the consequences of language play, and how do these effects arise naturally through our cognitive and linguistic mechanisms?
%

%In addition to answering these questions, a practical reason to study linguistic creativity is that it is far from rare \cite{gibbs1994poetics, carter1999common, carter2004talking, cook1996language, tannen2007talking, maybin2007everyday, veale2012exploding}. Creative uses of language constitute the ``long tail'' of the distribution of language use, highly diverse but together making up a large portion of our everyday conversations as well as our most prized literary works. In order to understand the range of linguistic behaviors that humans demonstrate, we need theories that address the creative aspects of language along with its more standard functions. 
%%A great deal of research has shown that play is an important component of early child development. Pretend play, exploratory play. 


%\subsubsection{Why computational models?}
%
%A common thread that runs through the three approaches is the use of computational models to formalize existing psychological and linguistic theories of language understanding. 
In this thesis, I use computational models to formalize theories of language understanding that aim to answer these questions.
The motivation for adopting a computational approach is twofold. 
From a scientific perspective, computational models require the model builder to define assumptions in precise terms and to specify interactions among various components of the model, thus providing a rigorous way to describe the underlying theory without relying on vague definitions and hidden assumptions. Formalizing theories and implementing them in computer programs also makes it convenient for model builders to observe the model's behavior in various situations, simply by providing the model with different inputs and parameter settings. Furthermore, model builders can remove or modify various parts of the model to determine which assumptions and components are necessary to produce outputs that match the observed data. Computational models are therefore useful tools for specifying, testing, and revising scientific theories \cite{frank2013throwing}. 
From an engineering perspective, ideas that are implemented in computer programs are more easily translated into technologies that can be enjoyed and tested by users at a larger scale. Given the prevalence of creative language in our everyday conversations \cite{gibbs1994poetics, carter1999common, veale2012exploding}, building artificial agents that can produce and interpret these uses may be a desirable goal.

The majority of work on computational approaches to language falls within the fields of natural language processing (NLP) and natural language understanding (NLU), which use sophisticated statistical models to extract patterns from large corpora of natural language. While these techniques have been used to develop technologies that are indispensable to modern society, they are not designed to test theories grounded in psychological reality. Because our motivation is to examine the cognitive mechanisms that support creative language interpretation and appreciation, here we aim to more directly draw insights from psychology and linguistic theory, which we then validate using data from behavioral experiments.
%In all three approaches, I draw upon existing informal theories on creative language and formalize them using Bayesian probabilistic models. 
%Part of the goal of this thesis is to draw upon existing theories and formalize them using Bayesian probabilistic models.
%
This motivation leads us to the choice of structured Bayesian probabilistic models. 
Structured probabilistic models are useful for specifying the hypothesized dependencies among variables of interest and using Bayesian inference to recover the values of unobserved variables \cite{mackay2003information, pearl2014probabilistic}. 
In recent years, Bayesian models have been applied to construct and test formal theories across a wide range of topics in psychology and cognitive science \cite{tenenbaum2011grow, goodman2008rational, oaksford2001probabilistic, yuille2006vision}. For the specific purposes of this thesis, two features of these models are particularly desirable. First, the models allow us to explicitly represent and make use of people's rich prior knowledge, which seems intuitively necessary for understanding and appreciating creative uses of language. Second, the models are tolerant of uncertainty and produce graded results, which is a useful property given that creative language often involves ambiguities and subtleties that would be difficult to capture using hard-and-fast rules.

Finally, Bayesian models of language understanding assume that people use their powers of rational probabilistic inference to comprehend language, which is consistent with a long tradition of rational approaches to language comprehension. 
%At the perceptual level, research has shown that people's perception of acoustic signals can be modeled as the rational integration of various cues \cite{clayards2008perception, feldman2009influence, kleinschmidt2015robust}. At the sentence-processing level, there is strong evidence that language comprehenders integrate noisy evidence to infer coherent sentence meanings in a rational manner \cite{levy2008noisy, gibson2013rational}. At the pragmatics level, communication has long been viewed as having a rational basis, where interlocutors assume each other to be rational, cooperative agents and produce and interpret utterances accordingly \cite{grice20134, garcia2001gricean, cohen1985speech}. 
Across many levels of linguistic analysis, research has suggested that language users can be modeled as ``ideal comprehenders'' who infer linguistic meanings given the available evidence in an approximately Bayesian manner \cite{feldman2009influence, kleinschmidt2015robust, levy2008noisy, frank2012predicting}.
Recently, a family of Bayesian models called Rational Speech-Acts (RSA) models formalizes the rational basis of communication to explain a range of phenomena in language understanding \cite{frank2012predicting, goodman2013knowledge, goodman2014probabilistic}.
Given this tradition, one contribution this thesis makes is to show that the interpretation and appreciation of creative uses of language can be achieved through many of the same rational principles that govern standard language understanding. 
In particular, we present three approaches that show how assumptions of rationality lead to socially meaningful and evocative interpretations of creative language. 
In the first approach, we formalize standard principles of communication using an extended version of the Rational Speech-Acts model and show that it produces appropriate interpretations of creative language such as hyperbole, irony, and metaphor as well as their subtexts. 
%We focus on figurative language because it is an extreme case where the conventional semantics of words are intentionally violated to evoke certain effects. 
In the second, we further extend the RSA model to include inferences regarding interlocutors' beliefs about each other. This model predicts rich social inferences that are uniquely licensed by figurative language. Finally, in the third approach, we adopt a simple model of sentence processing that integrates noisy linguistic evidence in a rational manner, and show that we can derive measures from this model that predict people's judgments of humor. Together, these three approaches produce fine-grained, quantitative predictions that may advance our psychological and linguistic theories for creative and social uses of language.

%A common thread that runs through the three approaches is the use of computational models to formalize existing psychological and linguistic theories of language understanding. 

%This suggests that higher-order linguistic phenomena such as humor could arise from general language processing strategies.


%We show that RSA models formalize the principles of informativity and rationality, and that introducing communicative goals further allows us to formalize a principle of relevance to model nonliteral uses. Maximizing amount of information transmitted. Take the different goals of communication seriously. Our work provides an innovative way of using models of basic cognitive mechanisms such as sentence processing to explain high-level emotional effects such as humor. Incorporating tools from information theory, we formalized and tested the theoretically-driven hypothesis that funny puns are characterized by  ambiguous meanings each strongly supported by distinct parts of the sentence.  
%A tension between creativity and rationality.


%In the rest of this chapter, I will discuss key characteristics of creative and social uses of language and the functions they serve. I will then briefly review Bayesian modeling paradigms in language understanding as well as some of their contributions and current limitations. Finally, I will outline ways in which these two aspects can be combined to develop formal theories of creative language, leading to an overview of the rest of this thesis. 

%At the end of this chapter, I will then provide an overview of the rest of the thesis.
%
%\section{Creative language as game}
%In his essay ``Closing statement: Linguistics and poetics,'' Russian linguist and literary theorist Roman \citeA{jakobson1960closing} wrote: ``Language must be evaluated in all the variety of its functions.'' Jakobson described six functions in particular: referential, conative, emotive, poetic, phatic, and metalingual. 
%%
%Language that performs the referential function refers to and describes a situation, object, or mental state, such as ``The''
%%
%The conative function engages or addresses the listener directly, and is commonly performed by imperitives such as `` ''.
%%
%The emotive function more directly involves the speaker and performs his goal of expressing an attitude towards the subject of conversation. Language that serves a purely emotive function (e.g. interjections and other sound changes) does not change the utterance's denotative meaning, but contain information about the speaker's attitude and internal state, e.g. "Wow, what a view!"
%%
%The poetic function focuses on the verbal message for its own sake. 
%%
%Language that performs the phatic function is often formulaic and used for the purpose of initiating, prolonging, or ending communication, such as ``Hi, how are you?''
%%
%Finally, the metalingual function refers to the use of language to discuss or describe language itself\footnote{This entire thesis can be said to perform a metalingual function, with referential and poetic functions occasionally thrown in the mix}. 
%
%The kinds of language uses that I discuss in this thesis primarily fulfill the emotive and poetic functions. 
%
%I propose a view of creative and social uses of language as a game between speaker and listener.  the  as a way of playing games with language. Games are enjoyable. Often games are social, and we get to practice modeling the minds of other people, predicting what they will do, and figuring out why they did what they did so we can  revise and update our model of them. Also, we like games because we experience rewards when we figure something out. It is often inherently pleasurable.
%
%People begin playing games with language from a very young age. 
%
%Chris Potts' expressives somewhere?
%
%\section{Language comprehension and rational inference}
%Reasoning under uncertainty. Language comprehension involves rational probabilistic inference. It's problem of inferring an unknown, latent variable meaning given the evidence at hand. Bayesian inference is a good way to do this.
%
%Rational inferences at many levels about phonetics, semantics, sentence processing, pragmatics. At the pragmatics level, Grice.
%
%Limitations?
%
%
%
%A great deal of research in the past half a century has focused on identifying rational principles in...
%Grice, game-theoretic, ideal observer approach, noisy channel models.


%\section{Goals in communication}

%\section{Towards formal theories of creative language}
%\section{Goal}



\section{Overview}
Earlier in this chapter, I posed two questions regarding creative uses of language: (1) How do we derive the meanings of utterances that deviate from linguistic norms? (2) What are the motivations for and consequences of using language in creative ways? This thesis is organized around these two motivating questions.  
The first half of the thesis examines the computational basis of how people understand nonliteral, figurative uses of language using pragmatic reasoning and principles of communication. The second half examines two social motivations for using creative language: to foster social closeness and to evoke humor. 
%Finally, I will synthesize the work and describe directions for future research.

%with a simple case of figurative language where the literal semantics is relatively straightforward: 
In Chapter 2, we present a case study of number words to show that basic principles of communication can explain how people arrive at nonliteral interpretations of language. We introduce and formalize a notion of the principle of relevance, where listeners assume that speakers choose utterances that are maximally informative with respect to their communicative goals. We extend the basic Rational Speech-Acts (RSA) model to include inferences about speakers' communicative goals, in particular the goal to communicate the speaker's attitudes and emotions. We test the model on nonliteral uses of number words: hyperbole (e.g. saying ``It's a million degrees outside'' when it is 92 degrees) and pragmatic halo (e.g. saying ``It's 70 degrees outside'' when it is 72 degrees). 
%The model integrates empirically measured background knowledge, principles of communication, and reasoning about communicative goals to explain the computational basis of nonliteral language understanding. 
%We introduced the idea that speakers choose different utterances given different communicative goals, which may include the goal to communicate subjective emotional attitude. 
We show that the extended RSA model predicts people's interpretations with high accuracy and captures the rhetorical effect of hyperbole, suggesting that this approach may shed light on the computational basis of nonliteral communication.
%%qualitative and 
%quantitative fit
%across literal, imprecise, and hyperbolic interpretations. 

%
%may have the goal of communicating subjective emotional attitude, choose utterances that We formalized a notion of I explored the consequences of incorporating affective goals --- a speaker's goal to communicate subjective emotional attitude --- in formal models of pragmatic reasoning.
%
%
%We examine how people use pragmatic reasoning to interpret nonliteral uses of numeric utterances such as hyperbole (``It's a million degrees outside'') and pragmatic halo (. the how people interpret figurative language using Rational Speech-Acts model and introduces the idea of communicative goals. We formalize a notion extends  simple communicative goal: affect. This model... focus on a simple case of nonliteral language that involves numeric utterances , which iwhich is the nonliteral interpretation of number words. 

Chapter 3 shows that the model described in Chapter 2 can be generalized to explain other types of figurative language such as irony and metaphor. We start by reviewing several types of figurative language and articulate the need for a coherent, formal theory of figurative communication. We show that extending the space of communicative goals to include emotional valence and arousal allows the model described in Chapter 2 to capture ironic interpretations. We then show that the same model explains a variety of effects observed in the interpretation of metaphor, suggesting that viewing metaphorical utterances as communicative acts may contribute a different perspective to the existing literature.
%Specifically, we showed that interpretations of the same metaphor vary systematically based on the preceding context. Our model captures these contextual effects by reasoning about the topic of conversation and quantitatively predicts people's fine-grained interpretations across various metaphors and contexts. 
By testing our model on a broader set of figurative utterances, we 
provide evidence that diverse figurative interpretations can arise from 
the same 
basic principles of communication. 

In Chapter 4, we describe an extension to the RSA model that incorporates uncertainty about speakers and listeners' beliefs about each other. By explicitly reasoning about these beliefs given various utterances, the model draws rich social inferences from nonliteral utterances, such as the inference that the speaker has high confidence in the common ground he shares with the listener if he uses a nonliteral utterance. These inferences are supported by existing evidence in the literature, and may provide a rational explanation for why people choose to communicate non-literally. We show that our model provides a principled explanation for how speakers may use figurative language to accomplish social goals such as highlighting common ground and social intimacy. 

Chapter 5 focuses on another important motivation for creative language use: humor. Why are some creative uses of language funny, and what predicts the funniness of a linguistic input? Instead of figurative language, in this chapter we use puns as a case study to examine linguistic humor in detail. We describe a simple model of sentence processing from which we derive theoretically-motivated, quantitative measures of humor. We show that these measures correlate with people's funniness judgments on a set of puns and regular sentences, which provides empirical evidence that humorous language is characterized by internally incongruous meanings. We use our model and measures to argue that the experience of humor may arise from general language processing strategies.

Finally, Chapter 6 summarizes and synthesizes the work described and suggests directions for future research. 

%\section{Notes on terminology and notation}
%
%QUD, communicative goals, topic of conversation, interchangeable. 
%
%

\chapter{Nonliteral Understanding of Number Words}

Human communication is rife with nonliteral and figurative uses of language. How do people go so far beyond the literal meaning of an utterance to infer the speaker's intended meaning? In this chapter, we focus on the nonliteral interpretation of number words, in particular hyperbole (interpreting unlikely numbers as exaggerated and conveying affect) and pragmatic halo (interpreting round numbers imprecisely). We provide a computational model of number interpretation as social inference regarding the communicative goal, meaning, and affective subtext of an utterance. We show that our model predicts humans' interpretation of number words with high accuracy, incorporating principles of communication and empirically measured background knowledge to interpret loose and hyperbolic uses of language.\footnote{This chapter is based on \citeA{kao2014nonliteral}}.
%This modeling framework provides a unified approach to nonliteral language understanding more generally.
%\\\\Significance statement: Human communication is rife with nonliteral language, ranging from metaphor to irony to hyperbole. How do people go so far beyond the literal meaning of an utterance to infer the speaker's intended meaning? We present a computational model that understands hyperbolic and other nonliteral uses of number words (e.g. ``That watch costs ten thousand dollars"). Our model integrates empirically measured background knowledge, principles of communication, and reasoning about communicative goals to explain the computational basis of nonliteral language understanding. This framework sheds light on the nature of communication, marking a significant advancement in the flexibility and richness of formal models of language understanding.

\section{Introduction}
%
%``I personally believe we developed language because of our deep inner need to complain.'' --- Jane Wagner


Imagine a friend describing a new restaurant where she recently dined. Your friend says, ``It took 30 minutes to get a table." You are likely to interpret this to mean she waited approximately 30 minutes. Suppose she says: ``It took 32 minutes to get a table." You are more likely to interpret this to mean exactly 32 minutes. Now, suppose she says: ``It took a million years to get a table." You will probably interpret this to mean that the wait was shorter than a million years, but importantly that she thinks it took much too long. One of the most fascinating facts about communication is that people do not always mean what they say; speakers often use imprecise, exaggerated, or otherwise literally false descriptions to communicate experiences and attitudes, and a crucial part of the listener's job is to understand an utterance even when its literal meaning is false. People's ability to interpret nonliteral language poses a critical puzzle for research on language understanding.

% Some sentences very similar to cogsci paper.
A rich body of literature in psychology and linguistics has examined how people use and understand nonliteral language \cite{roberts1994people, dews1999obligatory, glucksberg2001understanding, gibbs1999figurative}. However, most of the work has been qualitative, with little focus on analyzing aspects of an utterance that predict the quantitative details of people's figurative interpretations. Here we present a computational model that formalizes and integrates three general principles of language and communication to explain the basis of nonliteral language understanding. First, speakers and listeners communicate with the assumption that their interlocutors are rational and cooperative agents; second, listeners assume that speakers choose utterances to maximize informativeness with respect to their communicative goals; third, speaker and listener utilize common ground---their shared knowledge of the world---to communicate effectively. 
%
The first principle has been formalized by a recent body of work on Rational Speech-Acts (RSA) models, which views pragmatic language understanding as probabilistic inference over recursive social models and explains a range of phenomena in human pragmatic reasoning \cite{frank2012predicting, goodman2013knowledge, bergen2012s, jager2009pragmatic}. 
We go beyond the previous formal work and address the second principle by extending the RSA framework.
We first extend the space of potential interpretations to include subjective dimensions such as affective opinion.
We then assume that the listener is uncertain about the speaker's communicative goal and jointly infers both the goal and the intended meaning. Since the interpretation space has multiple dimensions, a speaker's goal may be to maximize the probability of successfully conveying information along one dimension of meaning but not another. This makes it possible for a literally false utterance to be optimal as long as it is informative along the target dimension.
%
These elements of the model have important connections to Gricean pragmatics \cite{grice20134, clark1996using} and relevance theory \cite{sperber1986relevance}, in particular the argument that listeners infer the meaning of metaphors as well as other forms of loose talk by assuming that speakers maximize relevance \cite{wilson2006metaphor, sperber1985loose}. 
Finally, we address the third principle of communication by empirically measuring people's background knowledge to understand the interaction between nonlinguistic and linguistic knowledge in shaping language understanding. By applying this computational approach to a case study on number words, we show that nonliteral interpretations can arise from basic principles of communication without positing dedicated processing mechanisms for nonliteral language.

At the core of RSA models, a listener and a speaker recursively reason about each other to arrive at pragmatically enriched meanings. Given an intended meaning $m$, speaker $S_1$ reasons about a literal listener $L_{0}$ and chooses utterance $u$ based on the probability that $L_{0}$ will successfully infer the intended meaning \cite{bergen2012s}:
\begin{equation}
S_1 (u|m) \propto L_{0} (m|u) \cdot e^{-C(u)}
\end{equation}
%
Here $C(u)$ is the psychological cost of an utterance, potentially determined by factors such as the utterance's frequency, availability, and complexity. The exponential results from using a Luce-choice rule to model utterance choice, which is used extensively in models of decision-making \cite{sutton1998reinforcement}. A pragmatic listener $L_1$ then reasons about $S_1$ and uses Bayes' Rule to infer the meaning $m$ given utterance $u$, where $P(m)$ is the prior probability of a meaning\footnote{While in principle speaker and listener can recurse to arbitrary depth, here we stop at recursive depth 1.}:
\begin{equation}
L_1 (m|u) \propto P(m)S_1 (u|m)
\end{equation}
Since the RSA framework operates under the assumption that speakers optimize informativeness, it predicts that choosing an utterance whose literal meaning directly contradicts the intended meaning is never optimal. However, this contradictory use is precisely the case in nonliteral language. For example, people understand the utterance ``It took a million years to get a table" to mean that the wait time was long but not, in fact, a million years, resulting in a contradiction between literal and interpreted meaning. This suggests that the basic RSA model is incomplete and requires additional elements to explain nonliteral communication.

Previous work has examined people's communicative reasons for using figurative language and suggested that certain goals, such as conveying emotion and emphasis, are commonly satisfied by nonliteral language \cite{roberts1994people}. A natural extension is thus to add an affective dimension to the meaning of utterances, which has interesting connections to previous work on expressives \cite{Potts06TARGET}. However, simply adding this dimension is insufficient; it is still unclear how people infer affect from an utterance whose literal semantics is unconnected to affect (such as number terms). 
Here we additionally extend the RSA framework to represent alternative communicative goals, such that a speaker can want to convey information about one dimension but not another. We show that the combination of these two extensions is sufficient to give rise to nonliteral understanding of language. 

We explore the case where the interpretation space has two dimensions: the state of the world and the speaker's affect or opinion\footnote{In what follows we describe the subtext dimension as ``affect,'' but it could be other kinds of speaker attitude, \emph{mutatis mutandis}.}. The speaker is now modeled as: 
\begin{equation}
S_1(u | s, a, g) \propto \sum_{s', a'}\delta_{g(s,a)=g(s',a')}L_0(s',a'|u)\cdot e^{-c(u)}
\end{equation}
where the intended meaning includes two dimensions $s$ (the state of the world) and  $a$ (the speaker's affect). The function $g$ projects the listener's inferred meaning onto relevant dimensions, meaning the speaker's communicative goal is to be informative (only) along this ``topic'' dimension. A literal listener interprets utterances literally without reasoning about the speaker, while a pragmatic listener performs joint inference on both the speaker's goal and her intended meaning:
\begin{equation}
L_1(s,a | u) \propto \sum_g P_S(s) P_A(a|s) P_G(g) S_{1} (u| s, a, g)
\end{equation}
The listener utilizes nonlinguistic background knowledge of the probability of a state ($P_S$) and the probability of having a particular affect given a state ($P_A$), which we measure empirically (see Experiment 3a and 3b).
Based on the listener's linguistic knowledge, the literal semantics of utterance $u$ conveys information about state $s$ and nothing about affect $a$. However, the common knowledge that affect is usually associated with certain states of the world allows the listener to believe information about $a$ given an assertion about $s$. If it is known that the speaker's goal is to convey affect, and not the state, then the pragmatic listener will discount information about $s$ but retain information about $a$---a nonliteral interpretation is obtained.
Even when the pragmatic listener is not certain of the speaker's goal, a joint inference of goal, state, and affect can also result in nonliteral interpretation.
Common knowledge of a domain and joint reasoning about communicative goals thus allows the speaker to communicate additional dimensions of meaning without explicitly describing these dimensions.

The incorporation of goal inference and multiple dimensions of meaning is a major change to the existing RSA framework that critically allows it to accommodate nonliteral language understanding. As a case study, we focus on the interpretation of number words. We chose number words because they have precise literal meanings that can be easily modeled, and apply to domains (such as prices) that lend themselves to quantitative measurement.
We aim to capture two well-known phenomena regarding number interpretation: hyperbole and pragmatic halo. Hyperbole is a figure of speech that uses exaggeration to convey emphasis and emotion \cite{mccarthy2004there}. Despite being literally false, hyperbolic utterances are readily understood and serve purposes such as establishing social closeness and expressing opinions \cite{roberts1994people, mccarthy2004there, gibbs2000irony, gibbs1991psychological}. Pragmatic halo refers to people's tendency to interpret round numbers such as $100$ imprecisely and sharp numbers such as $103$ precisely \cite{lasersohn1999pragmatic}. The halo effect has been formalized in game theoretic models as a rational choice given different utterance costs and a possibility of pragmatic slack \cite{bastiaanse2009rationality, krifka2007approximate}. Other research has shown that speakers' tendency to choose simple number expressions decreases when more precise information is relevant to the listener \cite{der2002truthfulness}, suggesting that higher-level pragmatic considerations such as communicative goals directly impact the production and interpretation of round versus sharp numbers. Our model uses alternative communicative goals coupled with differential utterance costs to model the pragmatic halo effect. We show that our framework for pragmatic inference makes quantitative predictions for both hyperbole and pragmatic halo in the interpretation of number words.
%------------------------------------------------
\section{Materials and Methods}
\subsection{Model}
Let $u$ be an utterance. The meaning of $u$ has two dimensions: the actual price state $s$ and the speaker's affect $a$. We defined the set of price states $S=\{50, 51, 500, 501, 1000, 1001, 5000, 5001, 10000, 10001\}$. We assumed that the set of utterances $U$ is identical to $S$. We defined the set of affect states $A=\{0, 1\}$ (0 means no affect and 1 means with affect---this binarization is purely for simplicity). Given $S$ and $A$, the set of possible meanings $M$ is given by $M = S \times A$. We denote each meaning as $s, a$, where $s \in S$ and $a \in A$. 

The speaker $S_1$ is assumed to be a planner whose goal is to be informative about a relevant topic. We write the goal and its topic as $g$.
$S_1$ chooses utterances according to a softmax decision rule that describes an approximately rational planner \cite{sutton1998reinforcement}:
\begin{equation}\label{eq:speakerprob}
S_1(u | s, a, g) \propto e^{ U_1(u |  s, a, g)}
\end{equation}
We wish to capture the notion that the speaker aims to be informative about a topic of discussion while minimizing cost.
If the topic is represented by a projection $g:M\rightarrow X$ from the full space of meanings to a relevant subspace, then the speaker cares only about the listener's distribution over the subspace,
\begin{equation}
 L_0(x|u) = \sum_{s',a'} \delta_{x{=}g(s',a')}L_0(s',a'|u).
\end{equation}
Following the Rational Speech-Acts model, we formalize informativity of an utterance as the negative surprisal of the intended meaning under the listener's distribution; here the listener's distribution over the topical subspace $X$.  Hence:
\begin{equation}
U_1(u | s, a, g) = \log L_0(g(s,a)|u) - C(u),
\end{equation}
where $C(u)$ represents the utterance cost.
Substituting into equation \ref{eq:speakerprob}, this gives:
\begin{equation}\label{eq:speakersimplified}
S_1(u | s, a, g) \propto \sum_{s', a'}\delta_{g(s,a)=g(s',a')}L_0(s',a'|u)\cdot e^{-C(u)}
\end{equation}
In our situations, the speaker may have the goal to communicate along the price dimension, affect dimension, or both. 
This gives three possible projections $r$:
\[
  \begin{array}{l l}
    r_{s}(s, a) = s\\
    r_{a}(s, a) = a\\
    r_{s,a}(s, a) = s, a.
     \end{array}
     \]
The speaker may also want to communicate the price either exactly or approximately (we assume that no such distinction exists for affect, since we have already binarized it). 
When the speaker wants to communicate the price approximately, she projects numbers to their closest round neighbors. For example, such a speaker will represent the prices $51$ and $1001$ as $50$ and $1000$, respectively.  This gives two projections (exact and approximate), $f$, defined as:
%
\[
  \begin{array}{l l}
    f_{e}(s) =  s \\
    f_{a}(s) = \text{Round}(s),\\
  \end{array}
  \]
where Round$(s)$ denotes the multiple of $10$ which is closest to $s$.
%
The two types of projections, $f$ and $r$, can be composed to make the goal $g$ of the speaker: $g(s, a) = r(f(s), a)$, which results in $2 \times 3 = 6$ possible goals (though note that $r_a(f_e(s), a)$ and $r_a(f_a(s), a)$ are equivalent).

A literal listener $L_0$ provides the base case for recursive social reasoning between the speaker and listener. $L_0$ interprets $u$ literally without taking into account the speaker's communicative goals:
\begin{equation}
L_0(s,a |u) = \left\{ 
  \begin{array}{l l}
    P_A(a|s) & \quad \text{if $s$ = $u$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
 \end{equation}	
The pragmatic listener $L_1$ performs Bayesian inference to guess the intended meaning given the priors $P_S$ and $P_A$ and his internal model of the speaker. To determine the meaning, the listener will marginalize over the possible goals under consideration.
\begin{equation}\label{eq:listenerdict}
L_1({s,a}|u) \propto \sum_{g }P_S(s)P_A(a|s)P_G(g)S_{1}(u | s, a, g)
\end{equation}
%
The prior probability of $s$ is taken from an empirically derived price prior $P_S$, and the probability of $a$ given $s$ is taken from an empirically derived conditional affect prior $P_A$ (see Experiments 3a and 3b). The probability distribution $P_G$ is defined to be uniform. We used $C(u) = 1$ when $u$ is a round number (divisible by $10$) and treated the sharp/round cost ratio as a free parameter that we fit to data (see Experiment 1). We obtained a posterior distribution for all possible meanings $s, a$ given an utterance $u$. Raw data for model predictions are here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/data/model-predictions.csv}}. Figure \ref{hyperbole-figure5a} shows the full posterior distributions for all utterances.

\subsection{Experiment 1: Halo and hyperbole}
120 participants were recruited on Amazon's Mechanical Turk. We restricted participants to those with IP addresses in the United States (same for all experiments reported). Each participant read 15 scenarios in which a person (e.g. Bob) buys an item (e.g. a watch) and is asked by a friend whether the item is expensive. Bob responds by saying ``It cost $u$ dollars," where $u \in \{50, 50 \pm k, 500, 500 \pm k, 1000, 1000 \pm k, 5000, 5000 \pm k, 10000, 10000 \pm k\}$, where $k$ was randomly selected from the set $\{1, 2, 3\}$ for each trial. We refer to this set of utterances as $U$. Given an utterance $u$, participants rated the probability of Bob thinking that the item was expensive. They then rated the probability of the item costing the following amounts of money: $50, 50 \pm k, 500, 500 \pm k, 1000, 1000 \pm k, 5000, 5000 \pm k, 10000, 10000 \pm k$, where $k$ was randomly selected from $\{1, 2, 3\}$ for each trial. We refer to this set of prices as $S$. Ratings for each price state were on a continuous scale from ``impossible" to ``extremely likely," represented as real values between 0 and 1. There are a total of 30 possible trial configurations (3 Items $\times$ 10 Utterances). We randomized the order of the trials as well as the names of the buyers (same for all experiments). See stimuli for Experiment 1 here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/materials/experiment1.html}}.
 
We normalized participants' ratings across price states for each trial to sum up to 1. The average normalized ratings across participants for each item/utterance pair is shown in Figure \ref{hyperbole-figure5b}, and the data can be found here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/data/experiment1-normalized.csv}}. To adjust for humans' biases against using the extreme ends of the slider bars, we performed a power-law transformation on the model's distribution: We multiplied the predicted probability for each meaning by a free parameter $\lambda$ and renormalized the probabilities to sum up to $1$ for each utterance. 
%
We jointly fit $\lambda$ and the model's cost ratio $C$ to optimize correlation with the behavioral data. The best fit was with $\lambda=0.36$ and $C=1.3$, resulting in a correlation of $r = 0.974$ ($95 \%$ CI = $[0.9675, 0.9793]$). The range of cost ratios that produces correlations within this confidence interval is $[1.1, 3.7]$, which is quite broad, suggesting that the overall model fit is not very sensitive to the cost ratio. To further capture the details of the halo effect, we jointly fit $\lambda$ and $C$ within this range to a measure that is more sensitive to utterance cost: We computed the difference between the probabilities of exact versus fuzzy interpretations for each utterance, which gives us each utterance's bias towards exact interpretation. We then computed the difference in this bias for sharp versus round numbers, which gives us a ``halo'' score for each sharp/round pair. We fit $\lambda$ and $C$ to minimize the mean squared error between the model and humans' halo scores. We found that the cost ratio that best captures the magnitude and pattern of the halo effect found in participants' data is $3.4$, while $\lambda = 0.25$. This produces an overall correlation of $0.9677$ with human data from Experiment 1. All figures and analyses that we report in the main text are with these parameter values. 

For the analysis reported in Figure \ref{hyperbole-figure3a}, we computed the probability of a participant interpreting an utterance $u$ as hyperbolic by summing up ratings for each interpreted price state $s$ where $u>s$. Since our analysis of hyperbole does not involve utterance costs, we collapsed across round and sharp versions of utterances and price states. For example, ``1001" interpreted as 1000 does not count as hyperbole. Since 50 and 51 are the lowest available price states, the probabilities for hyperbolic interpretation of utterances ``50" and ``51" are 0. We computed the average probability of a hyperbolic interpretation across subjects for each utterance. We then showed the hyperbole effect with a linear regression model, using prior probabilities for the utterances' literal meanings as predictor and probabilities for hyperbolic interpretation as response. Results indicated that participants were more likely to interpret utterances as hyperbolic when their literal meanings have lower prior probabilities ($F(1, 10) = 44.06, p < 0.0001$). For Figure \ref{hyperbole-figure3b}, we analyzed the pragmatic halo effect by computing each subject's bias for interpreting an utterance $u$ exactly versus fuzzily. Bias was measured by subtracting the probability of a fuzzy interpretation from the probability of an exact interpretation. We then obtained the average bias for each utterance across subjects. We showed that the average bias for exact interpretation is significantly higher for sharp utterances than for round utterances ($F(1, 28)=18.94,  p < 0.001$). 

\subsection{Experiment 2: Affective subtext}
160 participants were recruited on Amazon's Mechanical Turk. Each participant read 30 scenarios in which a person (e.g. Bob) buys an item that costs s dollars and is asked by a friend whether the item is expensive. Bob responds by saying ``It cost $u$ dollars," where $u \in U$ and $u \geq s$. Participants then rated how likely Bob thinks the item was expensive on a continuous scale ranging from ``impossible" to ``absolutely certain," represented as real values between 0 and 1. There are a total of 180 trial configurations (3 Items $\times$ 60 $\{u, s\}$ pairs where $u\geq s$). The stimuli for Experiment 2 can be found here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/materials/experiment2.html}}; the raw data here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/data/experiment2-raw.csv}}. Since our analysis of affective subtext does not involve utterance cost, for the analyses reported in Figure \ref{hyperbole-figure4a} and \ref{hyperbole-figure4b}, we collapsed round and sharp versions of each utterance and price state such that there are a total of 45 utterance/price state pairs under consideration. Utterances $u$ for which $u=s$ are considered literal; utterances $u$ for which $u>s$ are hyperbolic. For the analysis reported in Figure \ref{hyperbole-figure4b}, we obtained average ratings of affect for each utterance given that it is literal or hyperbolic. A linear regression model showed that hyperbolic utterances are rated as having significantly higher affect than literal utterances across price states ($F(1, 25) = 12.57, p < 0.005$). 

\subsection{Experiment 3a: Price prior}
To obtain people's prior knowledge of the price distributions for electric kettles, laptops, and watches, 30 participants were recruited from Amazon's Mechanical Turk. Each participant rated the probability of someone buying an electric kettle, laptop, and watch that cost $s$ dollars ($s \in S$), without any linguistic input from the buyer. Ratings for each price state were on a continuous scale from ``impossible" to ``extremely likely," represented as real values between 0 and 1. The stimuli for Experiment 3a can be found here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/materials/experiment3a.html}}. We normalized participants' ratings across price points for each trial to sum up to 1. The average normalized ratings for each item were taken as the prior probability distribution of item prices. These price distributions were used in the model as $P_S$ to determine the prior probability of each price state. The normalized ratings can be found here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/data/experiment3a-normalized.csv}}.

\subsection{Experiment 3b: Affect prior}
To obtain people's prior knowledge of the probability of affect given a price state, 30 participants were recruited from Amazon's Mechanical Turk. Each participant read 15 scenarios where someone had just bought an item that cost s dollars ($s \in S$) without any linguistic input from the buyer. They then rated how likely the buyer thinks the item was expensive on a continuous scale from ``impossible" to ``absolutely certain," represented as real values between 0 and 1. The stimuli for Experiment 3b is here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/materials/experiment3b.html}}. The average ratings for each price state were taken as the prior probability of an affect given a price state and used in the model as $P_A$. The data can be found here\footnote{\texttt{http://stanford.edu/\textasciitilde justinek/hyperbole-paper/data/experiment3b-raw.csv}}.


\section{Results}
We tested our model on number words that refer to the prices of three types of everyday items: electric kettles, watches, and laptops. We selected these items because they have distinct price distributions, $P_S$, which we measured empirically by asking participants to rate the probability of various prices for the three items (see Experiment 3a). We also obtained an affect prior, $P_A$, by asking participants to rate the probability of a speaker thinking that an item is too expensive given a price state (Experiment 3b). Using these priors, which capture purely nonlinguistic knowledge, we aimed to model people's interpretations of utterances such as, ``The electric kettle cost $u$ dollars." 

\begin{figure}
\centering
%\begin{minipage}[b]{.49\textwidth}
\scalebox{0.38}{\includegraphics{Figures/figure1-revised.pdf}}
%\end{minipage}\hfill
\caption{Model predictions of interpretations given utterances. Each bar in the first three rows shows the probability of a type of interpretation given an utterance. Exact interpretations are more likely given sharp rather than round utterances; fuzzy interpretations are slightly more likely given round utterances; hyperbolic  interpretations are more likely given more extreme utterances. The final row shows the probability of an affective interpretation.} 
\label{hyperbole-figure1}
\end{figure}
\begin{figure}
\centering
\scalebox{0.4}{\includegraphics{Figures/figure5a-revised.pdf}}
\caption{Posterior price state distributions predicted by the model given utterances. Each panel shows the interpretation distribution of an utterance.}
\label{hyperbole-figure5a}
\end{figure}

\begin{figure}
\centering
\scalebox{0.4}{\includegraphics{Figures/figure5b-revised.pdf}}
\caption{Price state distributions rated by participants given utterances. Each panel shows the interpretation distribution of an utterance. Error bars are standard errors.}
\label{hyperbole-figure5b}
\end{figure}


\subsection{Model simulations}
Using the price priors and affect priors measured for each of the three items, we obtained the meaning distributions predicted by the model for all utterances (see Figure \ref{hyperbole-figure5a}). Figure \ref{hyperbole-figure1} summarizes this distribution into different types of interpretations. The first three are model interpretations regarding the price state: exact (e.g.,  ``1000" interpreted as $1000$), fuzzy (e.g. ``1000" interpreted as 1001), and hyperbolic (e.g. ``1000" interpreted as 100). Round utterances (divisible by $10$) such as ``500" and ``1000" are interpreted less exactly and more fuzzily than their sharp counterparts, which captures pragmatic halo. Utterances whose literal meanings are less likely given the price prior are more likely to be interpreted hyperbolically (e.g. ``1000" is more likely to be interpreted hyperbolically for electric kettles than laptops), which captures a basic feature of hyperbole. Affective interpretation refers to the probability that an utterance conveys the speaker's opinion that the price is expensive. Utterances whose literal meanings are associated with higher affect priors (such as ``10000" and ``10001") are more likely to be interpreted as conveying affect, which predicts the affective subtext of hyperbole. 

To build intuition for these predictions, consider a pragmatic listener who reasons about a speaker and analyzes her choice of utterance. The pragmatic listener hears ``10,000 dollars" and knows that its literal meaning is extremely unlikely. Given that the speaker reasons about a literal listener who interprets ``10,000 dollars" literally and believes that the speaker very likely thinks it is expensive, ``10,000 dollars" is an  informative utterance if the speaker's goal is to communicate an opinion that the kettle is expensive (without concern for the actual price). Since the pragmatic listener uses this information to perform joint inference on the speaker's communicative goal and the meaning of the utterance, he infers that ``10,000 dollars" is likely to mean less than 10,000 dollars but that the speaker thinks it is too expensive.

\subsection{Behavioral experiments}
We conducted Experiment 1 to evaluate the model's predictions for the interpreted price. Participants read scenarios in which a buyer produces an utterance about the price of an item he bought, for example: ``The electric kettle cost 1000 dollars." Participants then rate the likelihood that the item cost $s$ dollars for $s \in S$ (see Experiment 1). Figure \ref{hyperbole-figure5b} shows humans' interpretation distributions across all utterances.  Participants were more likely to interpret utterances as hyperbolic when their literal meanings have lower probabilities under the item's prior price distribution ($F(1, 10) = 44.06, p < 0.0001$). To examine the halo effect, we computed the difference between the probability of an exact interpretation and the probability of a fuzzy interpretation for each utterance. This difference is signficantly smaller for round numbers than for sharp numbers ($F(1, 28)=18.94,  p < 0.001$), which indicates that round numbers tend to be interpreted less precisely than sharp numbers. To quantitatively evaluate the model's fit, we compared model and human interpretation probabilities across all utterances and showed that model predictions are highly correlated with human interpretations of number words ($r=0.968, p<0.0001$) (Figure \ref{hyperbole-figure2a}; see Materials and Methods for details). 

\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure2a-revised.pdf}}
\caption{Model predictions v.s. average human responses from Experiment 1. Each point represents an utterance and price state pair ($u, s$). The x-coordinate of each point is the probability of the model interpreting utterance $u$ as meaning price state $s$; the y-coordinate is the empirical probability. Correlation between model and human interpretations is $0.968$ ($95\%$ confidence region in grey).}
\label{hyperbole-figure2a}
\end{figure}

\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure2b-revised.pdf}}
\caption{Comparison of models with different communicative goals and human interpretations for the utterance ``The electric kettle cost 1,000 dollars." A model that considers both affect and precision goals (full model) most closely matches human data.}
\label{hyperbole-figure2b}
\end{figure}


To show how each component of the proposed model is necessary to capture effects observed in the human data, we explore a series of simpler comparison models. For illustration, Figure \ref{hyperbole-figure2b} compares model interpretations of the utterance ``The electric kettle cost 1,000 dollars" given inference over different communicative goals. A model that does not consider alternative goals interprets the utterance entirely literally. Note that even though such a model has information about the affect dimension (i.e. $P_A$), without goal inference it is unable to produce nonliteral interpretations because it assumes that the speaker only wants to maximize informativeness along the same dimension as the utterance, i.e. the price state. A model that considers a speaker whose goal may be to communicate precisely or imprecisely interprets the utterance as meaning either 1000 or 1001. A model that considers a speaker whose goal may be to communicate the price state \emph{or} her affect prefers price states with higher prior probabilities. Finally, a model that considers the full range of goals demonstrates hyperbole and halo effects that closely match humans' interpretations. To demonstrate that our model is able to usefully incorporate nonlinguistic knowledge to infer the meaning of utterances, Figure \ref{hyperbole-figure3a} shows the hyperbole effect as measured by the probability that an utterance $u$ is interpreted as price state $s$ such that $u > s$. A full model that uses empirically measured price priors captures humans' interpretations, while a model that takes a uniform distribution over price states does not. To demonstrate that our model is able to utilize utterance costs and goal inference to capture pragmatic halo, Figure \ref{hyperbole-figure3b} shows the halo effect as measured by the bias towards exact interpretation for sharp versus round numbers. A full model that assigns higher utterance costs to sharp numbers captures the significant difference in humans' biases for sharp versus round numbers, while a model where utterance costs are uniform does not. These analyses suggest that extending the RSA framework to include goal inference, incorporating empirically measured background knowledge, and including information about utterance costs all contribute to the model's ability to understand nonliteral language.
\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure3a-revised.pdf}}
\caption{Probability of hyperbolic interpretation given utterances. Leftmost panel shows human data (error bars are standard errors). A full model that uses price priors measured in Experiment 3a demonstrates similar hyperbole effects and distinguishes among item types; a model that uses uniform price priors does not.}
\label{hyperbole-figure3a}
\end{figure}

\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure3b-revised.pdf}}
\caption{Halo effect as measure by bias towards exact interpretation for round/sharp utterance types. Humans' bias towards exact interpretation is significantly higher for sharp numbers. A full model that assigns higher cost to sharp numbers captures this result; a model that uses uniform utterance cost does not.}
\label{hyperbole-figure3b}
\end{figure}


Does the model capture the rhetorical effect of hyperbole? We conducted Experiment 2 to examine humans' interpretation of affect in hyperbolic versus literal utterances. Participants read scenarios in which a speaker bought an item that cost $s$ dollars and says it cost $u$ dollars, where $u \geq s$. They then rate how likely it is that the buyer thinks the item was too expensive (see Experiment 2). We focused on the affect of an item being too expensive because previous findings suggest that hyperbole is more often used to communicate negative rather than positive attitudes \cite{roberts1994people, mccarthy2004there}. Results showed that utterances $u$ where $u > s$ are rated as significantly more likely to convey affect than utterances where $u {=} s$ ($F(1, 25)=12.57, p < 0.005$). This suggests that listeners infer affect from hyperbolic utterances above and beyond the affect associated a priori with a given price state. Quantitatively, we compared model and human interpretations of affect for each of the 45 utterance and pice state pairs $(u, s)$ where $u \geq s$. While there is a significant amount of noise in the human judgments (average split-half correlation is $0.833$), the model predicts human interpretations of the utterances' affective subtext significantly better than chance ($r=0.775, p < 0.00001$), capturing most of the reliable variation in these data (Figure \ref{hyperbole-figure4a}). 
\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure4a-revised}}
\caption{Model predictions of affect v.s. human responses from Experiment 2. Each point represents an utterance and price state pair $(u, s)$. For pairs where $u = s$, the utterance is literal; for $u > s$, the utterance is hyperbolic. The x-coordinate of each point is the model's prediction of the probability that the utterance/price state pair conveys affect; the y-coordinate is participants' affect ratings (error bars are standard error). Correlation between model and humans is $0.775$ ($95\%$ confidence region in grey).}
\label{hyperbole-figure4a}
\end{figure}

\begin{figure}
\centering
\scalebox{0.35}{\includegraphics{Figures/figure4b-revised}}
\caption{Probability of interpreting a hyperbolic/literal utterance as conveying affect. For the same price state, humans infer higher probability of affect given hyperbolic utterances than literal. A model that uses affect priors measured in Experiment 3b captures this result; a model that uses uniform affect priors does not.}
\label{hyperbole-figure4b}
\end{figure}

To demonstrate how our model explains this effect, Figure \ref{hyperbole-figure4b} shows probabilities of affect given a price state and a literal or hyperbolic utterance. The human data shows that higher actual price states are associated with higher probabilities of affect. Within the same price state, hyperbolic utterances are interpreted as conveying more affect than literal utterances. These effects are replicated by the full model, but not by a model that takes in a uniform affect prior. This analysis suggests that the rhetorical effect of hyperbole is driven in part by people's shared knowledge about prices and associated affect. 



%----------------------------------------------------------------------------------------
%	MATERIALS AND METHODS
%----------------------------------------------------------------------------------------

%% Optional Materials and Methods Section
%% The Materials and Methods section header will be added automatically.

%\begin{materials}

\section{Discussion}
We presented the first computational model of nonliteral understanding that quantitatively predicts people's hyperbolic and imprecise interpretations of number words.  Our behavioral results show that complex patterns in number interpretation depend on common knowledge between speaker and listener, consideration of communicative efficiency, and, critically, reasoning about the speaker's communicative goal. Our model represents an explicit, computational-level hypothesis about how these factors are integrated to give rise to the particular, graded interpretations that people arrive at. 
The model's quantitative predictions closely match humans' judgments, including cases of hyperbole, a complex phenomenon previously beyond the scope of computational models.

The current approach has important connections to theories of communication and linguistic meaning. Our speaker aims to be informative, as in Gricean theories of communication, but only with respect to a particular goal or topic---realizing a kind of relevance principle. This relevance is critical for deriving non-literal interpretations in our model. While our model is currently limited to two dimensions of meaning and corresponding goals, in future work we hope to capture dimensions central to other figures of speech such as irony and metaphor, thus extending our model to explain nonliteral language more broadly. We believe that our framework significantly advances the flexibility and richness of formal models of language understanding, such that some day probabilistic models will explain \emph{everything} (hyperbolically speaking). 

%\end{materials}

\chapter{Formalizing the Pragmatics of Figurative Language}

In the previous chapter, we showed that a Rational Speech-Acts model extended to accommodate inferences about the speaker's communicative goal\footnote{In the previous chapter, we used the term ``communicative goals'' to refer to a topic of conversation that the speaker aims to address. In this chapter, we will consistently use the term ``question under discussion (QUD)'' to refer a very similar concept.} is able to predict people's interpretations of nonliteral interpretations such as hyperbole and pragmatic halo. In this chapter, we show that the model described in the previous chapter can be generalized to capture interpretations of other distinct types of figurative uses such as verbal irony and metaphor. We argue that despite apparent differences among subtypes of figurative language, the same computational framework flexibly produces fine-grained interpretations for a range of figurative utterances. We use this as evidence suggesting that the rich and often affectively-laden meanings expressed by figurative language can be explained by basic principles of communication \footnote{This chapter is based on \citeA{kao2015let} and \citeA{kao2014formalizing}}.
%
%People use language to convey information that goes far beyond an utterance's literal meaning. In particular, figurative language such as hyperbole, irony, and metaphor showcases people's ability to 
%%incorporate background knowledge and social reasoning to 
%infer relevant and true information from utterances that are false under their literal semantics. 
%% and context to derive linguistic meaning.
%%For example, metaphors highlight specific similarities between disparate concepts; hyperbolic statements communicate speakers attitudes and opinions without explicitly stating them. 
%%While figurative utterances are often literally false, people are highly adept at inferring relevant and true information from these utterances. 
%%What is the cognitive and linguistic basis of figurative language understanding?
%%People routinely go beyond the literal meanings of utterances to infer speakers' intended meanings in context. In particular, figurative language such as hyperbole, irony, and metaphor showcase people's ability to use background knowledge and context to derive linguistic meaning.
%Although figurative language is prevalent in communication and has been studied across many fields, how people arrive at appropriate interpretations of figurative utterances remains unclear.
%%using contextual and background information
%Here we describe a computational model that formalizes figurative communication as recursive social reasoning between speaker and listener. Our model extends and overcomes limitations of basic Rational Speech-Acts (RSA) models, a family of computational models that capture many phenomena in human pragmatic reasoning but are restricted to the interpretation of literally true utterances.
%%Our model incorporates uncertainty about the speaker's communicative goal, otherwise known as the question under discussion (QUD). 
%We show that an RSA model extended to accommodate inferences about the question under discussion (QUD) is able to predict people's interpretations of hyperbole, irony, and metaphor. We argue that despite apparent differences among subtypes of figurative language, the same pragmatic framework flexibly produces fine-grained interpretations for a range of figurative uses. We use this as evidence suggesting that the rich and often affectively-laden meanings expressed by figurative language can be explained by basic principles of communication.
%%Understanding others' thoughts through the words they speak requires integrating a host of information sources to create meaning. figurative language  --- sentences whose intended meanings differ significantly from their literal meanings --- raises questions about the linguistic, cognitive, and social faculties that enable people to go beyond linguistic input and reason about others' intentions in understand utterances in context. 

\section{Introduction}
People do not always mean what they say---we implicate, exaggerate, make metaphors, and even wax poetic. 
One of the most challenging puzzles in communication is how people go beyond the words a speaker produces to infer the underlying meaning a speaker intends. 
Although intended meanings differ from overt meanings in many instances of language use, the effect is most striking in the case of figurative language. 
%The ability to understand figurative language is necessary in a world where people do not always mean what they say. 
From ``Juliet is the sun'' to ``It took a million years to write this paper,'' figurative language such as hyperbole, sarcasm, and metaphor evokes sensations of poetic beauty or humor by intentionally producing meanings that differ dramatically from (and often explicitly contradict) their literal meanings \cite{glucksberg2001understanding, pilkington2000poetic, lakoff2009more, roberts1994people}. 
%People often use language to convey information that goes far beyond a sentence's literal meaning. Hyperbolic statements communicate speakers' attitudes and opinions by exaggerating the truth (``It took a million years to write this paper''); ironic or sarcastic statements communicate meanings contrary to what is encoded in the literal semantics (``Paper-writing is my favorite activity''); and metaphors highlight hidden similarities between distinct categories by equating them (``Paper-writing is a marathon''). 
This radical departure from the standard encoded meanings of utterances is partly why figurative language has intrigued philosophers, linguists, psychologists, and literary theorists for decades, sparking debates about how we understand figurative language, why we appreciate it, and what it reveals about the human mind \cite{glucksberg2001understanding, papafragou1996figurative, li2010using, kreuz1993empirical}.

Although early accounts of figurative language treat it as a literary flourish that primarily serves an ornamental role \cite{gibbs1994poetics}, more recently the dominant view is that figurative language is highly prevalent in everyday use and reflects much of how we perceive and experience the world. This shift traces back to \citeA{lakoff2008metaphors}'s influential work on conceptual metaphor, which identified conceptual metaphors that form the basis of many of our common expressions (e.g. describing time as ``long'' makes use of the conceptual mapping between \textsc{time} and \textsc{space}). A great deal of work that followed continued to surface the figurative underpinnings of everyday language and cognition \cite{gibbs1994poetics, lakoff2009more, katz1998figurative}. At the same time, despite the now broadly accepted idea that figurative language can occur in many (sometimes mundane) ways in our daily use, figurative language is still intimately connected to creativity through its ability to reveal hidden similarities between concepts and to add flavor and interest to communication. %\jtk{maybe more on creativity...}

Given its prevalence and importance, figurative language poses a key challenge to theories of communication in which speakers are construed as rational and cooperative agents. In the field of pragmatics, communication is viewed as being governed by a set of maxims and rational principles, which together explain many diverse phenomena in language understanding \cite{grice20134, levinson1983pragmatics}. However, figurative language use does not appear to adhere to these rational principles in a straightforward manner. 
%principles appear to break down when analyzing figurative language. 
If a rational speaker's goal is to transfer information as efficiently as possible to a listener, why would she choose to produce an utterance whose literal meaning directly contradicts her intended meaning? Furthermore, how could a listener receive an overtly false utterance and recover the speaker's intended meaning, without abandoning the assumption that the speaker behaves in a cooperative and informative manner?

%Given the puzzling nature of figurative language and the tension it represents between creativity in thought and rationality in communication
While researchers agree that figurative language is puzzling, there has been disagreement about how precisely we should approach the problem of figurative meaning \cite{gibbs2012interpreting, gibbs2001evaluating}. Some researchers assume that different types of figurative use may require distinct processing mechanisms, and
%discard the notion that there is a single unified phenomena of figurative language, and instead 
focus on examining how people understand specific tropes such as metaphor and irony \cite{tourangeau1982understanding, colston2002irony}. Other researchers maintain that figurative language can be explained by more general theories of communication
%, and that figurative language lies on a spectrum of other common uses that deviate from literal meanings to varying degrees 
\cite{searle1979metaphor, sperber2008deflationary}. In this chapter, we take the latter view and argue that interpretations of figurative language are shaped by basic principles of communication. Furthermore, we show that figurative meaning is made possible precisely because these principles allow listeners to combine information sources in a systematic manner and make inferences that go beyond the overt literal meanings of words. We also argue that a precise, quantitative (and hence computational) approach is desirable in order to formalize these principles and evaluate them against empirical data. 

Although computational models exist for specific subtypes of figurative language, especially metaphor \cite{falkenhainer1989structure, kintsch2000metaphor}, to our knowledge there has not yet been a computational framework that formalizes pragmatic theories and explains figurative language in the general case. Constraint-based approaches (e.g. \citeA{katz2001moment}) aim to provide a general theory of figurative language by specifying various constraints that influence interpretation of a figurative utterance in context. However, these approaches do not describe precisely how the factors interact or posit a principled way of explaining those interactions. We believe that a computational, theory-driven framework could clarify how various factors work together to shape figurative meaning and situate figurative language in a more general theory of language understanding.

Our goal In this chapter is to propose an explicit and testable theory of figurative language and to empirically evaluate it using a set of diverse examples. We describe a computational model that formalizes a central principle in the Gricean view of communication: Listeners reason about speakers and assume them to be rational agents who choose utterances that are maximally informative and efficient \cite{grice20134}. Critically, we also make explicit the idea that listeners expect speakers' utterances to address a (possibly implicit) topic of conversation, thus formalizing a notion of relevance \cite{sperber1986relevance}. 
%Because listeners can be uncertain about the exact topic of conversation, they also reason about the topic of conversation. 
By jointly inferring the topic and interpretation given an utterance, a listener can arrive at meanings that are relevant to a topic but that deviate from the utterance's literal meaning, thus producing figurative interpretations through the standard course of language understanding. 
%Although figurative statements are often false under their literal semantics (Juliet is not literally the sun, and it is infeasible to take a million years to write a paper), people are highly adept at inferring relevant and true information from these utterances. 
%%Because the literal meanings of these utterances are insufficient for uncovering the intended meanings, understanding figurative language requires integrating a host of information sources to create meaning. 
%This ability to go beyond direct evidence (the words) to infer unobserved information (the meanings) is a hallmark of human intelligence that underlies many aspects of how we understand and interact with the world. How do our linguistic, cognitive, and social faculties work together to allow us to fluently and accurately understand the communicative intent behind figurative utterances?

%\ndg{this paragraph seems to say the same thing a few times, but also isn't very clear about what a literal or figurative meaning actually are. we don't actually want to define them yet, but we also don't want people to bump on this... maybe start with the observation that intended, and received, meanings often differ from overt meaning; this seems to be most radically true in the case of figurative language.}

%Figurative language is important because...
%\ndg{we should argue that a precise, quantitative (and hence computational) approach is to be desired. these exist (somewhat) for the specific cases (eg SME) but not for the pragmatic theories.}

%Taking the approach of analyzing general communicative principles that shape interpretations of figurative language, our goal In this chapter is to propose an explicit and testable theory of figurative language understanding that can be validated against empirical data. We describe a computational model that integrates several pragmatic elements (e.g. assumptions that speakers are rational and cooperative; assumptions that utterances tend to be informative and relevant to the topic of conversation; representations of common knowledge and prior beliefs; and inferences about speakers' subjective attitudes) to produce appropriate interpretations of figurative utterances. 

%\ndg{give a bit more intuition about how our model will work. it formalizes the gricean view, but makes the topic explicit and central (a la relevance). the key insight is that by jointly inferring the topic and interpretation, a listener can deviate from literal meaning through the standard course of utterance interpretation.}

In what follows, we first review core empirical phenomena in figurative language, highlighting existing research and open questions regarding factors that shape figurative language understanding. Next, we describe the ways in which many of these factors can be integrated through a framework of pragmatic reasoning. We introduce Rational Speech-Acts (RSA) models, a family of computational models that formalizes communication as recursive social reasoning \cite{frank2012predicting, goodman2013knowledge}. We then show that natural but critical extensions can be made to RSA models to account for inferences about the topic of conversation via a principle of relevance. We evaluate the extended model on three types of figurative language: hyperbole, verbal irony, and metaphor, and present behavioral data and modeling results. Finally, we discuss the insights this model reveals about figurative language understanding as well as future research that our modeling approach licenses. We argue that a general model of figurative language enables us to more precisely examine the ways in which semantics and principles of communication interact to generate rich linguistic meaning.


%An ocean of ink has been spilled attempting to answer this question, across many disciplines including psychology, linguistics, philosophy, computer science, and literary theory
%\section{Figurative Language: The Phenomena}
\section{Previous Approaches to Figurative Language}
%In this section, we will review previous approaches to studying figurative language in psycholinguistics and pragmatics. 
\subsection{Trope-Specific Mechanisms}
Much of the  empirical research on figurative language focuses on cognitive mechanisms that underly interpretations of specific types of figurative use. In order to explain how metaphors are processed and understood, psychologists proposed various ways in which people align shared properties and analogous relations across different domains, including the domain interaction model \cite{tourangeau1982understanding}, structure mapping model \cite{gentner1997alignment, gentner1983structure}, and category assertion model \cite{glucksberg2003psycholinguistics}. These models offer different predictions of metaphor interpretation in various contexts and are often distinguished through carefully designed behavioral experiments. In order to explain other types of figurative language, separate accounts are then posited, such as pretense theory \cite{clark1984pretense} and mention theory \cite{sperber1981irony} for verbal irony.
While these trope-specific approaches significantly advance our understanding of the cognitive factors invoked by particular types of figurative language, they require an array of distinct mechanisms in addition to those involved in standard language understanding. 
Furthermore, these approaches often leave open the question of how particular figurative utterances trigger these specialized mechanisms in the first place.
%, or why, if figurative utterances require an additional processing step, it is often just as easy to understand these utterances as their literal paraphrases in certain contexts.
\subsection{General Theories of Figurative Language}
A different approach to studying figurative language focuses on how people use general communicative principles to arrive at contextually appropriate interpretations. This approach is more commonly found in the pragmatics literature, which we review below. 
%\cite{grice20134, searle1979metaphor, sperber2008deflationary, ortony1993metaphor, tendahl2008complementary}. We briefly review two main bodies of work that have taken this approach to explain figurative language.
\subsubsection{Standard Pragmatic View}
A classic theory of figurative interpretation in the pragmatics literature is the standard pragmatic view, which analyzes figurative utterances using standard Gricean maxims. \citeA{grice20134} proposed that speakers produce utterances that adhere to principles of quality (truthfulness), quantity (informativeness), relevance, and manner (e.g. brevity, orderliness, clarity), and that listeners interpret utterances with the assumption that speakers follow these maxims in a cooperative manner. Under this view, figurative utterances are understood through a three-step process: (1) determine the literal meaning of the utterance (2) determine whether the literal meaning violates the quality maxim by being untruthful (3) reanalyze the utterance to identify implied or figurative meanings that would allow the utterance to adhere to the Gricean maxims \cite{grice20134, searle1979metaphor}.

Although the standard pragmatic view is appealing in that it fits parsimoniously within a more general theory of language understanding, it has met with several criticisms. One of the critiques is the fact that many figurative statements do not violate the quality maxim because their literal meanings are true to begin with. ``No man is an island,'' for example, is a literally true statement in addition to a metaphorically meaningful one  \cite{gibbs1992metaphor}. By relying on the violation of the quality maxim, the standard pragmatic view does not provide a satisfying explanation for how figurative meanings arise from these types of utterances. 
An even more common criticism of the standard pragmatic view is that it requires the listener to first access the literal meaning of the utterance, verify that the literal meaning is false, compute potential figurative interpretations, and then select the interpretation that best satisfies conversational maxims. Given the extra steps involved, this theory predicts that people should take longer to interpret figurative utterances than literal utterances. However, many experiments in the psycholinguistics literature have shown that the figurative meanings of irony and metaphor can be accessed as quickly or even more quickly than their literal meanings given supporting contexts \cite{glucksberg2003psycholinguistics, gildea1983understanding, gibbs1992metaphor}. These empirical findings suggest that literal meanings do not have to be explicitly computed and rejected before appropriate figurative interpretations emerge, leading to decreased support for the standard pragmatic view in recent years.

\subsubsection{Relevance Theory}
%The central claim of relevance theory is that human cognition is governed by a tendency to maximize relevance 
%\citeA{sperber2008deflationary} introduced the communicative principle of relevance, which is that ``every act of inferential communication conveys a presumption of its own optimal relevance.''
Instead of using Grice's four maxims as the guiding principle for figurative interpretation, relevance theory proposes that the principle of relevance is sufficient for explaining a range of phenomena in communication and cognition more generally. 
Relevance theorists claim that the interpretation of all language involves maximizing the relevance of the interpretation to a contextually determined topic \cite{sperber2008deflationary, tendahl2008complementary, sperber1986relevance}. 
%As a result, interpretations of the same utterance can vary dramatically given different topics.
%Suppose Ann and Bob are discussing their friend Cam. Ann asks, ``Does Cam have a fever?'' and Bob replies, ``He is boiling.'' Ann will interpret Bob's utterance to mean that Cam has a very high temperature. If, on the other hand, Ann asks, ``Is Cam upset?'' and Bob replies, ``He's boiling,'' Ann should interpret Bob's utterance to mean that Cam is very angry. The word ``boiling'' receives different figurative meanings in these two contexts. Ann arrives at the appropriate interpretation by assuming that Bob's utterance provides maximally relevant information regarding her question. 
Under this view, figurative uses such as hyperbole and metaphor are not distinct from literal language, but rather lie on a continuum of ``loose uses'' that require listeners to apply the principle of relevance to recover the intended meanings. 
This view situates figurative language within a general  theory of communication and has been argued to provide a complementary perspective to cognitive linguistics in the study of metaphor \cite{tendahl2008complementary}. However, one concern with relevance theory is that the concept of relevance, while intuitively appealing, has not been clearly operationalized or tested in a quantitative manner to determine its specific role in language understanding. 

In this chapter, we formalize several general principles of communication and show that they capture fine-grained interpretations of specific figurative tropes. The first principle is that speakers and listeners communicate with the assumption that their interlocutors are rational and cooperative agents, where speakers aim to be maximally informative and efficient in their choice of utterance \cite{grice20134}. The second principle involves a particular notion of relevance, namely that speakers choose utterances to maximize informativeness with respect to their communicative goals and the topic of conversation. The third principle is simply that speakers and listeners utilize their shared knowledge of the world to produce and interpret utterances. 

In addition to formalizing general pragmatic principles, we also aim to make connections with trope-specific findings by identifying communicative goals that are best addressed by different types of figurative uses.

%Here we present a computational model that formalizes and integrates three general principles of language and communication to explain the basis of nonliteral language understanding. First, speakers and listeners communicate with the assumption that their interlocutors are rational and cooperative agents; second, listeners assume that speakers choose utterances to maximize informativeness with respect to their communicative goals; third, speaker and listener utilize common ground---their shared knowledge of the world---to communicate effectively. 


%\ndg{maybe some of the details of standard and relevance theories (and arguments about them) should be saved for the next section?}

%\section{A proposal}

%In the next chapter, I will describe such a formal model and argue that it may illuminate our understanding figurative communication.
%
%
%ARGUE FOR NOT ENOUGH EXPLICITNESS.
%While both the standard pragmatic and relevance theoretic view provide useful frameworks for understanding the pragmatics of figurative communication, certain factors that are important in general communication remain underspecified. For example, while relevance theory considers which interpretations are maximally relevant to the question under discussion, it does not provide a clear operationalization of relevance. 
%
%neither the standard pragmatic nor relevance theoretic view explicitly take into account the speaker's desire to be informative. There is also little consideration of the shared encyclopedic knowledge associated with different utterances, or representation of the listener's prior beliefs. WWe believe that an adequate model of figurative language understanding should flexibly integrate these components to determine an appropriate interpretation. In addition, figurative language is often used to  express subjective experiences and emotional attitudes \cite{riloff2005exploiting, roberts1994people}.
%Since these emotional subtexts contribute greatly to the appeal of figurative language, it is important for a theory of figurative language understanding to include analyses of these effects.
%
%Here we adopt an approach closer to the latter, with the goal of proposing a general pragmatic framework that explains the basis of figurative communication.


\section{Figurative Language: The Phenomena} 
%\todo{include list of types of figurative language?}


%Under this definition, an utterance such as ``That chocolate cake is to die for'' is figurative, because the speaker does not mean that the chocolate cake is literally worth giving up one's life. Perhaps less obviously, an indirect speech act such as ``Can you pass the salt?'' is also figurative, because the intended meaning of the utterance differs from the literal meaning of the question---the speaker does not mean to inquire about the listener's ability to pass the salt.
%At first glance, this definition seems straightforward and corresponds with our intuitions regarding which usages of language are literal and which are figurative; however, it grows murky upon closer inspection. Suppose a speaker Bob says, ``I arrived late and the theater was full.'' Since it is implausible that the entire space of the theater was occupied from floor to ceiling, the sentence's strict literal meaning appears to be false. Instead, Bob most likely intends to communicate that he had difficulty finding an empty seat at the theater. This is an example of ``loose talk,'' also described as pragmatic slack, where a speaker uses a proposition $Q$ (e.g. the theater was full) in order to communicate a set of propositions that can be derived from $Q$ (e.g. there were a lot of people at the theater, and Bob was unable to find a seat), without being committed to the truthfulness of $Q$ \cite{sperber1985loose, lasersohn1999pragmatic, bach1994semantic}. 
%%Based on the naive definition of figurative language, the utterance ``The theater was full'' can be characterized as not strictly literal and thus figurative.
%%A speaker wishes to communicate a set of propositions $P_1 \dots P_n$ (e.g. there were a lot of people at the auditorium; we weren't able to find seats), which are derivable from a stricter proposition $Q$ (e.g. the auditorium was full). In order to minimize effort while effectively communicating $P_1 \dots P_n$, the speaker chooses to utter $Q$ without fully committing to its truth. Relevance theorists argue that the relevance principle---the assumption that speakers choose utterances to maximize relevance---is what allows listeners to correctly infer $P_1 \dots P_n$ without necessarily believing that $Q$ \cite{sperber1985loose}.
% 
%On the other hand, consider an utterance such as, ``Bob is always late,'' produced by an annoyed speaker. In order for the literal meaning of the utterance to be true, for all cases in which Bob can be either late or on time, Bob must be late in $100\%$ of the cases. However, one can easily interpret the utterance to mean that the speaker thinks Bob is very often (but not literally always) late, thus arriving at an interpretation that differs from the literal meaning of the utterance.
%Under these analyses, the intended meanings of both of these utterances (``The theater was full'' and ``Bob is always late'') differ from their ``literal'' meanings. Indeed, \citeA{sperber1985loose} claim that there is no discontinuity between loose and figurative uses of language; both exploit the principle of relevance in order to express what is derivable from the utterance without committing to the truth of the literal meaning of the utterance. At the same time, a sentence such as ``Bob is always late'' feels qualitatively different from a sentence such as ``The theater was full'' and is more easily recognized as hyperbole. In fact, it has been observed that some utterances are intuitively recognized as ``figurative'' while other are not \cite{coulson2005blending}, which suggests that figurative language may be a psychologically meaningful category distinct from most other loose uses.


%The definition of figurative language is further complicated when taking polysemy into account \cite{gibbs2012interpreting}. Consider the different senses of the word ``full.'' Suppose someone says, ``I am full'' after a meal. Since the speaker does not mean that the space inside her body is literally filled up, the sentence is not true under one particular literal meaning of ``full.'' However, one might argue that ``full'' has several different senses, and that the sense of being satisfied with food is the sense that is used literally in this utterance. One could then further modify the literal semantics of ``full'' for an utterance such as ``My heart is full (of emotion),'' such that the literal meaning of ``full'' includes a physical sensation in one's chest that roughly corresponds to experiencing emotion. In other words, one could keep adding different senses to the literal meaning of words until no usage is ever figurative. 
%Once one becomes sufficiently careful, it appears that the boundary between literal and figurative language is extremely fuzzy. At the same time, the fact that a great deal of research has targeted figurative language as a distinct category of language use, and that some utterances are intuitively recognized as ``figurative'' while other are not \cite{coulson2005blending}, suggests that such figurative language is a psychologically meaningful category. 
%%Where does this intuitive sense of ``figurative-ness'' come from? Do sentences have well-defined literal meanings, and is the distinction between literal and figurative meanings psychologically meaningful?  
%Instead of seeking to define the precise boundary between literal and figurative meanings, here we propose a weaker and less controversial definition. We suggest that the ``figurativeness'' of an utterance depends upon how Perhaps this intuition is based not on a clearcut boundary between literal and figurative meanings, but rather on a graded \emph{distance} between interpretations that the same utterance generates. Consider an utterance such as ``His car cost ten thousand dollars.'' Given most contexts that one can easily bring to mind, a reasonable interpretation of this utterance is that the car cost exactly $\$10,000$, or the car cost approximately $\$10,000$, and it is difficult to come up with a third interpretation. These two interpretations are not very different from each other. However, consider an utterance such as ``His cup of coffee cost ten thousand dollars.'' This utterance could also be interpreted as the coffee cost exactly $\$10,000$ or approximately $\$10,000$. However, one can easily take into account background knowledge (a cup of coffee almost never costs $\sim \$10,000$) and speaker intention (the speaker may want to express an attitude about the coffee and not its exact price) to conclude that the coffee cost $ << \$10,000$ and that the speaker believes the coffee is too expensive. Regardless of which interpretation is strictly literal, it is clear that the third interpretation is quite different from the other two. The fact that the utterance about the coffee generates distinct interpretations and the utterance about the used car generates similar interpretations may give rise to the intuitive sense that distinguishes figurative from normal utterances. In what follows, we will label an utterance as ``figurative'' if it intuitively generates distinct interpretations and leave it to future work to quantify this sense of ``distinctiveness.'' 

%Despite many efforts to draw a distinction between literal and figurative language, the line remains blurry \cite{honeck1986verbal, coulson2005blending}. In fact, many researchers have argued that the line does not exist, partly due to the fact that literal meaning itself is not a single cohesive notion \cite{gibbs1994poetics, lakoff1986meanings, giora2002literal, ariel2002demise}. Instead of seeking to define the precise boundary between literal and figurative meanings, here we will focus on cases that are rather uncontroversially categorized as ``figurative.'' In order to identify these cases, we first review the various types of language use that researchers have included within the category of figurative language and extract overlapping cases.

%%
Despite general consensus that figurative language consists of utterances that differ in various ways from their ``literal'' or standard meanings \cite{gibbs1999figurative}, researchers have not always agreed on which tropes should be included in the category of figurative language \cite{honeck1986verbal, kreuz1993empirical}. Here we identify different types of figurative language, describe related empirical phenomena, and review factors that have been shown to affect figurative interpretation.

\subsection{Types of figurative language}
%In part due to the difficulty of defining figurative language, 

%\citeA{lanham1991handlist} created a list of nearly $1000$ rhetorical terms, while 
\citeA{kreuz1993empirical} identified eight figures that they believed form the basic categories of figurative language: \emph{indirect requests}, which are commands phrased as comments or questions; 
%(e.g. ``It would be great if you could keep this a secret'')
\emph{idioms}, where the intended meaning of the utterance cannot be derived from the individual words' typical meanings; %(e.g. ``Ann ended up spilling the beans'')
\emph{irony}, where the intended meaning is opposite in polarity from the utterance's literal meaning; 
%(e.g. ``Ann is the best secret keeper ever'', in a situation where Ann clearly failed to keep a secret)
\emph{understatement}, where the speaker intentionally says something that is less extreme or intense than is actually the case;  %(e.g. ``Bob seems a tiny bit upset at Ann'', when Bob is clearly furious)
\emph{hyperbole}, where a speaker intentionally says something that is more extreme or intense than is actually the case; 
%(e.g. ``Bob won't forgive Ann in a million years'')
\emph{metaphor}, where concepts from distinct domains are implicitly compared or equated with each other; 
%(e.g. ``Bob's anger is a tornado'')
\emph{simile}, where concepts from distinct domains are explicitly compared; 
%(e.g. ``Bob's anger is like a fire'')
and \emph{rhetorical questions}, which are questions that do not require an answer.
%(e.g .``What was Ann thinking giving away that secret?'')
 \citeA{gibbs1999figurative} agreed with most of the figures while excluding \emph{rhetorical questions} and including \emph{metonymy}, \emph{proverbs}, and \emph{oxymora}. Based on these lists and on the amount of attention each figurative trope has received in the psycholinguistics literature, In this chapter we will focus on \emph{hyperbole}, \emph{irony}, and \emph{metaphor} as three of the most generally recognized and broadly studied figurative tropes. In what follows, we will describe each of the three tropes and review relevant theoretical and empirical research.
%%
\subsubsection{Hyperbole}
A hyperbole is an exaggerated statement that purposefully presents its subject as more striking or extreme than it actually is \cite{roberts1994people, mccarthy2004there}. \citeA{gibbs1994poetics} makes a distinction between hyperbole and overstatement, where the former is purposefully produced for rhetorical effect. Rhetoric studies in ancient Greece regarded hyperbole as a major figure of speech, often used to persuade and demonstrate power \cite{smith1969mystery}. Hyperbolic statements often include extreme case formulations (e.g. ``It was the biggest storm in the history of the universe'') or implausible descriptions (e.g. ``It's a thousand degrees outside.'') These demonstrations of non-veridicality require the listener to produce what \citeA{fogelin2011figuratively} called a ``corrective'' response that is more in line with reality.
 For a hyperbolic statement to be interpreted successfully, the listener must recognize the non-veridicality of the statement, thus entering an activity of joint pretense \cite{clark1996using}. 
 In a modern analysis of a corpus of spoken English, \cite{mccarthy2004there} found that hyperbole occurs frequently in everyday conversations and is often used in humorous and other affective contexts. 
 %\citeA{norrick1982semantics} proposed that hyperbole is characterized by three properties: its affective dimension, its pragmatic nature, and its function as a vertical-scale metaphor where the comparison is between different positions on a scale rather than between discrete concepts. 
  %
%\subsubsection{Understatement}
%An understatement is used to purposefully describe something as being less extreme than it actually is \cite{roberts1994people}: for example, saying ``It seems to be drizzling a bit'' in the middle of a storm. As is the case for hyperbole, the listener must also recognize the statement's non-veridicality and produce a corrective response. A hyperbole and an understatement are similar in the sense that both have literal meanings that contrast with reality in terms of degree \cite{mccarthy2004there}. In both cases, speaker and listener must have common knowledge about the state of the world to know that the statement is not literally true---e.g. it must be mutually believed that it is raining very hard in order for the listener to successfully interpret the understatement.

%
\subsubsection{Verbal irony}
An ironic statement describes something as contrary to what it actually is: for example, saying ``Such beautiful weather we are having'' in the middle of a storm \cite{roberts1994people, gibbs1999figurative}. 
%Irony is thought to be related to hyperbole because it also involves a vertical scale (niceness of the weather), where the literal meaning's position on the scale (``beautiful'') is  different from the position of the intended meaning (``terrible''). 
Like hyperbole, irony also requires the listener to recognize the non-veridicality of the utterance and enter into joint pretense. However, the required corrective response is one of ``kind'' (e.g. from ``beautiful'' to ``terrible'') instead of degree (e.g. from ``drizzling'' to ``pouring'') \cite{mccarthy2004there}. 
The listener and speaker must share sufficient common ground in order to ensure an appropriate corrective response. To explain how irony is identified and interpreted, \citeA{clark1984pretense} proposed the pretense theory of irony, where the speaker is pretending to be an ignorant or injudicious person who believes the weather to be beautiful, thus expressing a derogatory attitude towards such a misinformed person.  
%say irony involves setting up a pretend world that is contrasted with the actual world to highlight the incongruity between what is and what might have been. 
\citeA{sperber1981irony}, on the other hand, proposed the echoic reminder theory, which suggests that irony is used to remind listeners of jointly held beliefs, social norms, or expectations that are being disrespected. While the pretense and mention theories disagree some respects, they agree that the expression of attitude is central to the mechanism of verbal irony understanding \cite{sperber1984verbal}.
%Finally, it has been observed that ironic statements that are literally positive but express negative opinions (e.g. saying ``You're a great friend'' ironically to a friend who betrayed you) occur more frequently and are easier to comprehend than statements that are literally negative but express positive opinions (e.g. saying ``You're a terrible friend'' ironically to a friend who helped you). This phenomena, termed the asymmetry of affect, has been explained using pretense theory, 

%this asymmetry is due to the fact that ironic statements Since most social norms are positive, it follows naturally that ironic statements  Irony draws attention to this contrast and more often involves using a positive statement to express a negative attitude. 
% 
\subsubsection{Metaphor}
Metaphors are utterances that compare ideas or concepts from different domains in order to highlight commonalities.
For example, ``Juliet is the sun'' expresses Juliet's beauty and importance by comparing her to the sun, which also has many positive attributes.
% ``My lawyer is a shark'' communicates the lawyer's ruthlessness; and ``Art washes away from the soul the dust of everyday life'' allows Picasso to compare ``art'' to a cleansing fluid and ``the soul'' to a physical object that collects dust, which gracefully accomplishes two poetic metaphors at once. 
Metaphoric utterances are prevalent in both literary and everyday language \cite{gibbs1999figurative, roberts1994people}. 
%One can find traces of metaphoricity even in mundane utterances such as ``I waited for a long time,'' where the spatial term ``long'' is used to describe the abstract domain of time \cite{lakoff1993contemporary}. 
Due in part to its ubiquity and in part to the belief that metaphor is intimately tied to our ability to create mappings between concrete experiences and abstract concepts \cite{lakoff2008metaphors}, metaphor is by far the most widely studied trope in cognitive science and related fields \cite{gibbs2012interpreting}. Understanding a metaphor often requires identifying overlapping features or aligning analogical structures between two domains \cite{gentner1997alignment}. Using metaphors to understand a novel domain can significantly affect people's inferences, making it an important mechanism for learning and reasoning \cite{thibodeau2011metaphors, gentner1997reasoning}. %Evidence that metaphors are often processed as quickly as literal statements suggests that metaphor understanding does not require first accessing literal meanings   \cite{glucksberg2003psycholinguistics, gibbs2012interpreting}. 
From the communication perspective, researchers have suggested that metaphorical language may be a more economical and efficient way of expressing complex meanings  \cite{ortony1975metaphors, boerger2005variations,glucksberg1989metaphors}. As \citeA{glucksberg1990understanding} write, ``Metaphors are  used  to  communicate a complex, patterned  set of properties in a shorthand that is understood  by the  members of a speech community  who share relevant mutual  knowledge" (pp. 16). 
%In communication tasks where pairs of participants are separated by a screen and asked to refer to abstract geometrical objects, participants often prefer to describe objects metaphorically in terms of other known objects rather than use literal analytical descriptions \cite{clark1986referring, glucksberg1989metaphors}. %Perhaps unsurprisingly, \citeA{fussell1989effects} found that these analogical and figurative descriptions tend to be shorter than literal descriptions. However, they also found that figurative descriptions are used significantly more often when the intended audience is one's self, where common ground is maximal, than when the intended audience is a different person. This suggests that people may be balancing efficiency and clarity when choosing figurative versus literal descriptions. 
A potential benefit of speaking in metaphor is thus to utilize common ground to communicate complex ideas more efficiently. %or necessarily involve different processing mechanisms from literal language  
%

%%
%\subsubsection{Indirect request}
%An indirect request is a command phrased as a comment or question \cite{roberts1994people}: for example, ``It is rather cold in here'' can be used as a request for the listener to close a window or turn down the air-conditioning. Indirect requests are often used to be polite \cite{roberts1994people}. However, highly indirect requests are not necessarily perceived as the most polite; instead, indirect requests that break away significantly from conventional forms are perceived as lacking pragmatic clarity and thus less polite \cite{blum1987indirectness}. Indirect requests can vary in terms of conventionality, where some utterances are interpreted as requests only in specific contexts (e.g. when the listener has control over the temperature in the room), whereas other utterances are highly conventionalized and almost always interpreted as requests (e.g. ``Can you pass the salt?''). \citeA{gibbs1983people} found that people do not compute the literal meanings of highly conventional indirect requests and instead are biased towards the figurative interpretations. \citeA{clark1979responding} proposed that when the answer to a question is mutually obvious to the speaker and listener, the listener should interpret the question as an indirect request instead of as a genuine question. 
%
%\subsubsection{Rhetorical question}
%%%
%A rhetorical question is a question that does not require an answer and is instead used for effect \cite{roberts1994people}: for example, ``How many times have I told you not to leave your dirty clothes on the floor?'' uttered by an upset parent. Rhetorical questions are often used to emphasize, express negative affect, and persuade \cite{roberts1994people}. If interpreted literally, rhetorical questions have answers that are either obvious (e.g ``Am I your maid?'') or nearly impossible to provide (e.g. ``How many times have I told you?''). If interpreted correctly, rhetorical questions effectively function as pseudo-assertions (e.g. ``I am not your maid'') as well as answers to their own questions (e.g. ``I have told you too many times'') \cite{schmidt1977so}. This requires the speaker and listener to have the mutual belief that the question does not require and in fact may not have an actual answer. It also requires the listener to go beyond the surface form of the question and infer that the speaker's true communicative intent is not to elicit an answer but rather to express a point.
%%%

%\subsection*{Effects of figurative language}
%
%%Figurative utterances have two distinct qualities of interest. First, their intended meanings . Second, they are often used with the intention to produce particular effects and in order to accomplish discourse goals beyond relaying objective information about the world.   
%
%In addition to differing in various ways from their literal or standard meanings, different types of figurative language share a distinctive quality: they are often used to accomplish goals beyond relaying objective information about the world and to produce rhetorical effects \cite{roberts1994people}.   
%%\cite{gibbs1999figurative, gibbs2012interpreting}
%% examined the discourse goals that motivate people to use various figurative tropes. They identified a taxonomy of goals, including to convey emotion, to emphasize, to be humorous, or to be eloquent, and find that metaphor
%%
%%These are, in order, to clarify ($82\%$), to add interest ($71\%$), to compare similarities ($35\%$), to provoke thought ($35\%$), and to be eloquent ($35\%$).
%%
%\citeA{colston1998you} showed that hyperbole and irony are often used to express surprise. Other work has shown that verbal irony can heighten or soften criticism \cite{colston1997salting}, elicit emotional reactions \cite{leggitt2000emotional}, highlight group membership \cite{gibbs2000irony}, and express affective attitudes \cite{roberts1994people}.
%% \cite{colston1998you} that hyperbole and irony are more often used with friends and may signal social intimacy between speaker and listener \cite{gibbs2000irony, pexman2004does, kreuz1996use}. 
%In addition, researchers have suggested that metaphors are often used to express subjective attitudes towards the subject \cite{ortony1979beyond}, and that subjective sentences frequently contain figures of speech such as metaphor and hyperbole \cite{riloff2005exploiting}. 
%%\todo{elaborate}
%It is possible that the intuitive judgment of figurativeness involves recognizing that the speaker's intent is not to communicate objective information about the world, but rather to produce one or more of these rhetorical effects. 
%As a result, it may be important to consider the affective subtexts and social information that figurative language communicates above and beyond most loose uses of language. 

\subsection*{Factors that shape figurative interpretation}
%It may be useful to further classify these eight types of figurative language in order to make the commonalities among them more salient. I will introduce the concepts of horizontal comparison, vertical comparison, and comparison of surface form. Metaphor and simile are horizontal comparisons in the sense that the concepts being compared (e.g. ``Juliet'' and ``the sun'') occupy different parts in a semantic space projected onto two dimensions. Hyperbole, understatement, and irony are vertical in the sense that the concepts being compared (e.g. ``drizzling'' / ``pouring'' ; ``lovely''/ ``terrible'') occupy similar points in the semantic space but vary vertically in intensity \cite{norrick1982semantics}. I will call indirect requests and rhetorical questions ``formal'' comparisons, where sentences in the form of questions or comments carry the same illocutionary force as sentences in the form of requests or declarative statements.

In reviewing these three figurative tropes, some common features emerge. First, each example from these three tropes produces multiple interpretations that are distinct and highly different from each other (e.g. ``It's a thousand degrees outside'' can mean that it's literally a thousand degrees, or that it's very hot, around $100^oF$). Second, the intended meanings of these utterances are related to their literal meanings in non-arbitrary ways (e.g. a thousand degrees and 90 degrees are both unexpectedly high; ``beautiful'' and ``terrible'' both describe an extreme attitude towards the weather; the sun and Juliet are both very important and appealing to Romeo).
%Third, these utterances tend to express speakers' subjective experiences and attitudes rather than objective information about the world. 
Finally, a great deal of common ground is required to successfully interpret these utterances. 
%For example, interpretation of an utterance such as ``Such beautiful weather we are having,'' depends upon the speaker and listener's mutual beliefs about the relevant state of the world (e.g. it is raining), their shared background knowledge (e.g. sunshine is usually preferable to rain), and mutual awareness of potential discourse goals (e.g. the speaker wants to convey her opinion about the weather). Because the interpretation of such utterances depends upon these different flavors of common ground, it tends to be highly sensitive to changes in context. 
In what follows, we will examine in more detail the various factors that shape figurative interpretation.

%For example, suppose a chemist needs to make a chemical solution using water that is $100$ degrees celsius. She asks her lab technician, ``How's the temperature?'' and he replies, ``The water is boiling.'' The chemist will interpret this utterance to mean that the water is at boiling point. Now suppose a mother is about to give her baby a bath and asks her husband to test the temperature of the bathwater. He replies, ``The water is boiling.'' Given the mother assumes the utterance to be relevant to the topic and specific task at hand, she will interpret the utterance to mean that the water is too hot, but unlikely that it is $100$ degrees celsius, thus arriving at a figurative, hyperbolic interpretation. Now suppose Sam says, out of the blue, ``Bob is boiling.'' Without knowing what entity Bob is or the context in which Sam produced the utterance, it is difficult to arrive at an interpretation of the utterance.  If what is relevant is Bob's temperature, this utterance means that Bob has a fever. If the topic is Bob's emotional state, this means that Bob is angry. A relevance theory account of metaphor is that all meanings, not just figurative, figurative ones, are selected based on relevance to the topic at hand.

%%%
%\section{What is figurative language?}


%However, most theories of figurative language are informal and not fully precise, leaving room for discrepancies in how researchers interpret empirical data. %TODO: references. 
%Furthermore, different theories focus on disparate aspects of figurative language (e.g. the standard pragmatic view focuses on recognition; the structure-alignment model focuses on interpretation of relational metaphors, etc.), making it difficult to compare predictions about the same phenomenon. Figurative language understanding involves many moving pieces---literal meaning, common ground, contextual information, speaker intention, feature attribution, subjective attitude and affect, etc. A model that explicitly describes how all of the pieces interact may help resolve some of the discrepancies among theories and provide a unified framework for examining various types of figurative language. In particular, several important questions are left unanswered by the theories reviewed above. I will explore a few of the questions in this section and introduce a formal computational framework in Chapter 3 that may shed light on the answers.
%In this section, we first review some classic ideas in pragmatics. We then describe two main bodies of work that specifically examine the pragmatics of figurative language understanding. Finally, we suggest that a complete theory of figurative communication should more carefully account for certain important factors that affect general language understanding.

%Finally, the interpretations of these utterances are highly context dependent. I argue that all of these different flavors of information will be important when we consider how rich interpretations emerge from figurative utterances.
%
%
%These three types of language use all exhibit three distinct qualities that we are particularly interested in: (1) the intended meanings of the utterances differ (often dramatically) from the literal meanings (2) the utterances often express speakers' subjective experiences and attitudes rather than objective information about the world (3) the interpretations of these utterances are highly context dependent. 

\subsubsection*{Literal meaning}
Figurative utterances communicate meanings that differ dramatically from their literal meanings. However, these intended, figurative meanings are still related to the literal meanings  \cite{coulson2005blending}. One cannot simply say \emph{any} sentence and expect the context to make one's intended meaning clear---for example, saying ``I had eggs for breakfast today'' in the middle of a storm will unlikely be successful at communicating that the weather is terrible. Instead, an utterance such as ``Such beautiful weather we're having'' can be interpreted as expressing a negative attitude towards the weather because its literal meaning draws attention to the weather as well as to the speaker's attitude towards it. As a result, the literal semantics of utterances should play a role in determining the figurative meanings that people derive, and a model of figurative interpretation should be able to represent and make use of an utterance's literal semantics.  
%important to represent the literal semantics of utterances and consider its role in deriving figurative meanings.
%The puzzle, then is \emph{how} the intended meaning of a figurative utterance could be derived from its literal semantics. 
%reasonable to assume that the intended meaning of a figurative utterance can be derived from its literal meaning. 

%important to consider the various ways in which we can formulate literal meaning and analyze the ways in which it contributes to figurative meaning.

%The traditional view of literal meaning draws a clear distinction between literal and figurative meanings. 
%According to \citeA{frege1984sense}, the literal meaning of a sentence is its interpretation given no information about who said the sentence, when, or why. This view led to assumptions about the literal meaning of a sentence as being determined only by the meanings of its component words and how they are composed, independent of any extra-linguistic information. As a response against this traditional view, \citeA{searle1978literal} argued that literal meaning is not entirely independent of extra-linguistic information and instead relies heavily on background knowledge. For example, the literal meanings of ``Sally cut the cake'' and ``Sally cut the grass'' depend on the manners in which cake and grass are usually cut, which is encoded in background knowledge \cite{gibbs1984literal}. Searle draws a distinction between this kind of background knowledge, which he believes is needed to determine literal meanings, and the context in which sentences are uttered, which helps determine contextual meanings. However, other linguists and philosophers argue that Searle ``demands too much from literal meaning'' and conflates the literal meaning of a sentence with its intended speaker meaning \cite{dascal1981contextualism, katz1981literal, gibbs1984literal}. 
%
%Another way literal meaning has been conceptualized is through conventionality \cite{davies1996philosophy}. Perhaps literal meanings are those that are more conventional, such that without additional context, one chooses the sentence's most conventional meaning by default \cite{recanati2002literal}. However, equating literalness with conventionality is problematic when we consider idioms. For example, the conventional meaning of ``kick the bucket'' (to die) is certainly different from our intuitive sense of its ``literal'' meaning (to come into contact with a bucket with one's foot). \citeA{giora1997understanding} proposed that instead of focusing on the literal/figurative distinction, a more useful dimension along which to analyze meaning is salience. She introduces the graded salience hypothesis, where more salient meanings---defined as more frequent, context-independent, and prominent---are accessed first and are ``default,'' regardless of whether or not they are literal. While the idea of salience many explain when and why certain meanings are accessed automatically independent of context and literalness \cite{giora1999priority}, it is not always clear precisely how salience should be measured or operationalized \cite{gibbs2012interpreting}.
%Despite vibrant discussions of these and related matters in the past few decades, the problem of literal meaning is still largely unresolved \cite{recanati2004literal, cruse2004meaning, carston2008linguistic, coulson2005blending}

%\todo{add a note here about how the literal meanings we're looking at are rather simple and uncontroversial?}
%This exploration of various views on literal meaning suggests that the problem is highly complex, and a thorough and satisfying account is possibly beyond the scope of this document. 

\subsubsection*{Encyclopedic knowledge}
One way in which literal meaning gives rise to intended meaning is through encyclopedic knowledge, 
which goes beyond the strict semantics of utterances to include stereotypes, conventions, and a community's beliefs and practices \cite{taylor2003linguistic, langacker1987foundations}. 
%, known as an encyclopedic approach to meaning
%One of the most important insights in the study of language use is that interlocutors make heavy use of common ground during communication \cite{clark1996using}. 
%This common ground includes the extra-linguistic information and world knowledge shared among interlocutors, which can be intimately tied to linguistic meaning. 
%Some researchers propose that the meaning of a word itself includes encyclopedic knowledge. 
%\citeA{searle1978literal} argued that literal meaning is not entirely independent of extra-linguistic information and instead relies heavily on encyclopedic knowledge. For example, the literal meanings of ``Sally cut the cake'' and ``Sally cut the grass'' depend on the manners in which cake and grass are usually cut, which is encoded in background encyclopedic knowledge \cite{gibbs1984literal}. %Searle draws a distinction between this kind of background knowledge, which he believes is needed to determine literal meanings, and the context in which sentences are uttered, which helps determine contextual meanings. 
%However, other linguists and philosophers argue that Searle ``demands too much from literal meaning'' and conflates the literal meaning of a sentence with its intended speaker meaning \cite{dascal1981contextualism, katz1981literal, gibbs1984literal}. 
%
%Despite the fact that the distinction between literal meaning and encyclopedic knowledge is not always clear, 
This extralinguistic information plays an important role in the interpretation of language, including figurative uses. For example, suppose Ann asks Bob, ``Is Cam an honest person?'' and Bob replies, ``He's a politician.'' Although Bob's answer is indirect, Ann will likely interpret it to mean that Bob does not believe Cam to be an honest person. This interpretation arises because while the dictionary meaning of ``politician'' is  \emph{a person who is professionally involved in politics}, the encyclopedic meaning of the word can encompass many more features and connotations, such as \emph{dishonest} and \emph{corrupt}. Bob's utterance not only asserts Cam's profession (the literal, dictionary meaning of ``politician''), but also attributes features associated with that profession to Cam. 
Ann is able to successfully interpret  Bob's utterance, and Bob is able to successfully use this utterance, because they both have access to the relevant encyclopedic meaning of ``politician.''
%---the network of background knowledge shared among people in a community, which includes the stereotype that politicians may be dishonest or corrupt \cite{taylor2003linguistic, langacker1987foundations}. 
Naturally, Ann's interpretation is sensitive to the contents of the background knowledge they share. If Ann and Bob belong to a community where politicians are associated with honesty, then Ann would interpret Bob's reply to mean that he believes Cam is an honest person. 
%Similarly, ``It's a thousand degrees outside'' is interpreted as ``It's unbearably hot outside'' partly based on the encyclopedic knowledge that ``a thousand degrees'' is exceedingly hot, and that one is unlikely to survive under that temperature. 
As a result, the encyclopedic knowledge that interlocutors share can heavily influence the interpretation of figurative utterances.

\subsubsection*{Prior beliefs}
In addition to encyclopedic knowledge, language interpretation depends upon the listener's prior beliefs and expectations about various states of the world. \citeA{hormann1983tun} showed that people's interpretation of quantifiers such as ``several'' and ``few'' vary based on the kinds of objects to which they refer. For example, ``several crumbs'' is interpreted to mean \emph{around $10$ crumbs}, while ``several mountains'' is interpreted to mean \emph{around $5$ mountains}. \citeA{clark1991words} explains this phenomenon using the ``principle of possibilities:'' to interpret language, people use their prior expectations about which situations or worlds are possible as well as the likelihood of those worlds. 
%To interpret ``several crumbs'' and ``several mountains,'' people consider the number of crumbs and mountains that typically inhabit a scene or situation. 
Since a typical situation involving crumbs is likely to contain more crumbs than a typical situation involving mountains, the interpretation of ``several'' results in a higher number in ``several crumbs''  than in ``several mountains.''

%\jtk{describe example with ``it's a million degrees outside''}

Given that prior beliefs affect the interpretation of superficially straightforward terms such as ``several,'' it is unsurprising that prior beliefs factor into the interpretation of figurative language as well. In the example dialogue with Ann and Bob, Ann's interpretation of the utterance ``He's a politician'' is sensitive to her prior beliefs about Cam. Suppose Ann does not know what Cam does for a living. She will have learned two facts about Cam from Bob's utterance: Cam is a politician, and Cam is not an honest person. Suppose, on the other hand, Ann knows that Cam is a professional politician. She will not have learned anything new about Cam's profession from Bob; however, even though she knows that politicians are commonly believed to be dishonest, Bob's utterance makes her more certain that Bob thinks Cam \emph{in particular} is dishonest, because that is the most informative and relevant interpretation given her question. Finally, suppose Ann knows that Cam is not professionally involved in politics at all. Instead of updating her beliefs about Cam using the dictionary meaning of ``politician,'' she will rely on its encyclopedic meaning to conclude that Cam is dishonest (but not professionally involved in politics), resulting in a metaphorical interpretation. These examples show that interpretation of the same utterance in the same local context can vary in a rich and subtle manner based on the speaker and listener's prior beliefs and expectations about the topic. 

\subsubsection*{Local context}

A great deal of psycholinguistics research has shown that figurative language is highly sensitive to the preceding context, affecting both the speed and product of interpretation \cite{katz2001moment, giora2003our, coulson2005blending}. 
%\todo{more examples from the literature?} 
If Ann had asked, ``Is Cam a persuasive speaker?'' instead of ``Is Cam an honest person?'' then Bob's utterance would be interpreted as a compliment about Cam's eloquence\footnote{Note that in this case Bob's utterance still carries the connotation that Cam is not to be trusted, even though Ann's did not explicitly ask about Cam's honesty.}.
In the pragmatics literature, context has been construed as information that specifies the topic of conversation. \citeA{roberts1996information} refers to this topic as the ``question under discussion'' (hereafter QUD), and argues that utterances are expected to be relevant to the QUD and are interpreted with respect to it. 
%The QUD can be determined by an explicit question, for example Ann's question about Cam's honesty, which guides her interpretation of Bob's response because she expects Bob to communicate information that is relevant to her question.  
Often, the QUD does not take the form of an explicit question and is not clearly specified to the listener. In such a case, the listener must jointly infer the QUD as well as the speaker's intended meaning given an utterance.
Taking into account the importance of local context in shaping interpretation, a model of figurative language understanding should flexibly integrate this type of contextual information via inferences about the QUD.
%A speaker may produce an utterance in order to introduce a new QUD, which the listener must then infer based on the utterance itself as well as her expectations about which QUDs the speaker may wish to introduce. As a result, part of the listener's task in language comprehension is to jointly infer the QUD and the utterance meaning given the meaning.
%that a speaker's utterance addresses
%The local context---whether constructed by an explicit question, a situation, or a salient goal---sets up a ``question under discussion'' (QUD) to which the information conveyed by the speaker is intended to be maximally relevant. In Liz and Sam's original dialogue, the QUD was Bob's honesty; in this scenario, it is Bob's persuasiveness; in the balcony scene of \emph{Romeo and Juliet,} it is Juliet's admirable qualities. 


\subsubsection*{Pragmatic reasoning}
A critical insight in communication is that a speaker does not produce utterances in a social vacuum; he considers the listener's beliefs, goals, and disposition in order to determine which utterance is most effective in a given situation  \cite{clark1982audience}. In turn, a listener considers the speaker's beliefs, goals, and disposition (as well as the speaker's representation of the listener's beliefs, goals, and disposition) in order to select the most likely meaning of an utterance \cite{clark1996using, levinson2000presumptive, grice20134}. 
%This recursive social reasoning between speaker and listener is responsible for many phenomena in pragmatics and language understanding \cite{horn2006implicature, levinson2000presumptive}.
%Furthermore, listeners assume speakers to be rational and cooperative agents who aim to be informative, known as the Cooperative Principle \cite{grice20134, clark1996using, levinson2000presumptive}. When interpreting an utterance, a listener uses these assumptions of rationality and informativeness to reason about what meaning a speaker could want to convey that would lead him to choose a particular utterance.
This recursive social reasoning between listener and speaker is responsible for many phenomena in pragmatics and language understanding \cite{horn2006implicature, levinson2000presumptive}.

In addition, listeners can make many powerful inferences about utterances by representing speakers as rational and intentional agents who choose utterances in order to accomplish a specific communicative goal. 
Consider again the conversation between Ann and Bob. Ann has several hypotheses about Bob's communicative goal and the QUD his utterance aims to address, which could be Cam's honesty, profession, or persuasiveness as a speaker. The likelihood of each QUD depends on the context (e.g. Ann's question) and the beliefs shared by speaker and listener (e.g. Ann's prior knowledge of Cam's profession). 
%Ann's question (the context) and as well as  and  which is likely given  Ann's question. His goal could be to inform Ann of Cam's profession, which is likely if Ann does not know Cam's profession, but less likely if Cam's profession is in common ground. 
Given each QUD, Ann can make inferences about what information Bob intends for her to glean from his utterance. 
In addition to depending on the QUD, the array of implicatures derived from an utterance is also affected by the alternative utterances that the speaker could have said \cite{horn2006implicature, bergen2012s}. The fact that Bob could have said ``Yes, he's a persuasive speaker'' but chose to say ``He's a politician'' makes it likely that Bob wants to address QUDs that go beyond Cam's persuasiveness. %Furthermore, the fact that Cam chose the metaphor ``He's a politician'' instead of ``He's a salesman,'' both of which convey persuasiveness, suggests that Bob wants to communicate specific features about Cam such as deceptiveness and cunning, rather than pushiness. 
Reasoning about the speaker's choice of utterance and available alternatives allows the listener to use basic principles of communication derive rich figurative meanings as well as their subtleties. A theory of figurative language as a communicative act should thus incorporate the speaker's intent as well as how the listener reasons about this intent in various communicative contexts.

\subsubsection{Putting it all together}
%My review of the literature has revealed three general principles that are needed to explain figurative language understanding. 
%First, listeners reason about speakers and assume that they are rational and cooperative agents who wish to be informative. Second, speaker and listener use their shared background knowledge (common ground) to communicate, which includes specific knowledge of certain states of the world as well as general encyclopedic knowledge. Third, listeners are sensitive to the question under discussion (QUD) created by the local context and assume that speakers choose utterances that are relevant to the QUD. 
While researchers have suggested that the construction of meaning involves an interplay of the components outlined above \cite{coulson2005blending, gibbs1984literal, clark1996using, stalnaker2002common}, to our knowledge there is no formal model that explicitly describes the relationships among these components and integrates them to produce quantitative and fine-grained predictions that can be evaluated against empirical data. 
%\todo{elaborate on why it's useful to have a formal model}
Such a formal model would be useful for clarifying specific assumptions in the theory, testing the detailed effects of inputs such as encyclopedic knowledge and prior beliefs, and comparing alternative explanations.

In the next section, we propose a computational framework in the tradition of Rational Speech-Acts models, which formalizes pragmatic reasoning and the recursive nature of communication \cite{frank2012predicting}. In addition to pragmatic reasoning, we incorporate the other factors described above to capture figurative interpretation. We show that these components together produce appropriate interpretations of figurative utterances as well as rich affective and social subtexts. 
%\todo{add note that these components are not unique to figurative language understanding}
%the following components: reasoning about the speaker's choice of utterance, with assumptions of rationality and informativeness; the literal meaning of utterances; shared background knowledge between speaker and listener; specific prior beliefs; local contextual information; affective subtext. 


%A listener who reasons about a speaker can make many more powerful inferences about an utterance by thinking about \emph{why} he chose a particular utterance and what communicative goal it satisfies \cite{gibbs2012interpreting}. 
%It seems plausible that listeners also need to reason about the speakers' intent in order to interpret figurative utterances in various communicative contexts.

\section{Probabilistic Models of Language Understanding}

%In the last chapter, I proposed that in order to successfully interpret a figurative utterance, a listener needs to (1) reason about a rational and informative speaker (2) consider the literal meaning of an utterance 
%(3) consider relevant background knowledge associated with utterance
%(4) consider relevant prior beliefs 
%(5) consider the local context and question under discussion 
%(6) understand the subtext or attitudes expressed by the utterance. 
%Here I will review the basic RSA framework and introduce an extension that incorporates these components.
%listeners reason about speakers and assume that they are rational and cooperative agents who wish to be informative. Second, speaker and listener use their shared background knowledge (common ground) to communicate, which includes specific knowledge of certain states of the world as well as general encyclopedic knowledge. Third, listeners are sensitive to the question under discussion (QUD) created by the local context and assume that speakers choose utterances that are relevant to the QUD.

%\subsection{Basic Rational Speech-Acts Model}

In recent years, a family of computational models have emerged that use probabilistic tools to formalize principles of communication, called Rational Speech-Acts (RSA) models \cite{frank2012predicting, goodman2013knowledge, goodman2014probabilistic}. 
These models formalize the Cooperative Principle to explain how people arrive at pragmatically enriched meanings of utterances through recursive social reasoning. By representing listeners as agents who reason about the intentions of a rational and cooperative speaker, these models predict pragmatic enrichments that allow the listener to make inferences beyond the strict literal meaning of an utterance. To date, RSA models have been used to explain Horn implicatures \cite{bergen2012s}, vagueness and context-sensitivity in gradable adjectives \cite{lassiter2014context}, generic language\cite{tesslergeneric}, the pragmatic use and interpretation of prosody \cite{bergen2015strategic}, and more \cite{goodman2014probabilistic}. 
%\section{The RSA framework}
%This is accomplished 
% A recent body of work called Rational Speech-Acts (RSA) theory formalizes the first principle regarding informativity and rationality \cite{goodman2014probabilistic, frank2012predicting, goodman2013knowledge}. RSA theory uses probabilistic models to 
%through the recursive reasoning between speaker and listener, which allows the listener to reason about what intentions a rational and cooperative speaker could have had to lead him to choose a particular utterance. 

The basic structure of RSA models is usually involves three ``agents:'' a naive literal listener $L_0$, a speaker $S_1$, and a sophisticated, pragmatic listener $L_1$. $S_1$ reasons about $L_0$ and determines which utterance $u$ to choose in order to efficiently communicate a meaning $m$ to $L_0$. The more sophisticated listener $L_1$ then reasons about which meaning $m$ most likely led $S_1$ to choose $u$ and uses Bayesian inference to recover $m$ given $u$, thus capturing the recursive reasoning between speaker and listener.

Formally, $L_0$'s interpretation of an utterance $u$ is given by the following equation, where utterances always receive a literal interpretation:

\begin{equation}
L_0(m | u) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if $m$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
The probability that $S_1$ will choose an utterance $u$ given an intended meaning $m$ is given by the following equation:
%Suppose Sam (to be more ``formal,'' we will call him $S_1$) intends to communicate meaning $m$. He reasons about Liz (we will call her $L_0$) and considers how likely it is that she will infer $m$ from $u$. 
\begin{equation}
S_1 (u|m) \propto e^{\lambda U(u | m)} 
\end{equation}
%
where $U(u | m)$ is a utility function that specifies the negative surprisal of the meaning $m$ under $L_0$'s interpretation (i.e. how likely $L_0$ will receive the meaning $m$ given utterance $u$). In addition, $\lambda$ is a rationality parameter that determines $S_1$'s tendency to choose utterances that have maximal utility.
%informativeness, or negative surprisal the utility of where $L_0 (m|u)$ is the probability that $L_0$ will arrive at meaning $m$ given utterance $u$, and $Cost(u)$ is the psychological cost of producing utterance $u$ given its length, difficulty, or availability. 
The term $e^{U(u | m)}$ thus implements the Luce-choice rule \cite{luce2005individual}, which is widely used to model rational decision-making.

The pragmatic listener $L_1$ then uses Bayes' rule to infer $S_1$'s intended meaning given a generative model of $S_1$'s utterance choice:

\begin{equation}
L_1 (m|u) \propto P(m)S_1 (u|m)
\end{equation}
where $P(m)$ is the prior probability of the meaning $m$, and $S_1(u|m)$ is $L_1$'s model of what utterance $S_1$ would choose given a meaning $m$. Since we believe that people engage in pragmatic inference when interpreting language, we use $L_1$ to model people's interpretation of utterances\footnote{In principle, the speaker and listener can recursively reason about each other to arbitrary depth. However, rich pragmatic effects can emerge from depths 1 and 2, which is reason to believe that this framework may be psychologically plausible for modeling pragmatic language understanding.}.
%It is important to note, however, that these models operate on the computational level of Marr's levels of analysis \cite{marr1982vision} and do not make specific claims about the particular algorithms employed in language processing.

\begin{figure}
\centering
\scalebox{0.3}{\includegraphics{Figures/frankgoodman2012.png}}
\caption{Example set of objects in the language game described in Frank \& Goodman (2012).}
\label{frank-science}
\end{figure}

\citeA{frank2012predicting} tested the RSA framework on humans' pragmatic judgments in a simple reference game. In this paradigm, participants see three objects and are asked to choose which object the speaker is referring to (see Figure~\ref{frank-science}). The speaker can only use one word to identify the intended object, which often results in ambiguous references. For example, the word ``blue'' may refer to either the blue square or the blue circle. \citeA{frank2012predicting} asked participants how likely a speaker is to use a particular word to refer to an object: for example, using the word ``blue'' or ``square'' to refer to the blue square. This experiment yields the likelihood term $S_1(u|m)$. Other participants were asked how likely a speaker is to refer to a particular object using an unknown word, which measures $P(m)$, or what the authors refer to as the object's contextual salience. Using these two pieces of information, the RSA model computes $L_1(m|u)$, which is the probability that the referent is a particular object given a particular utterance. The model  correctly predicts that listeners are more likely to judge the word ``blue'' as referring to the blue square, even though the word is technically ambiguous. This is because a sophisticated listener who reasons about the speaker knows that if the speaker had meant the blue circle, he would have used ``circle'' instead because it is more informative. The model's predictions matched participants' judgments extremely well ($r = 0.99, p < 0.0001$), suggesting that people may be incorporating the speaker's choices and prior probabilities of meanings in a similar rational manner. Using a simple reference game paradigm, this work showed that incorporating recursive social reasoning and prior knowledge allows the listener to go beyond the strict literal meaning of a word to infer the intended meaning in context.

%\begin{figure}[t]
%\centering
%\includegraphics[width=8.7cm]{frankgoodman2012.png}
%%\includegraphics[width=8.7cm]{model_effects.pdf}
%\caption{Stimuli from \citeA{frank2012predicting}}
%\label{frankgoodman}
%\end{figure}

\citeA{goodman2013knowledge} made more explicit the fact that in addition to formalizing the rationality principle, the RSA model can also flexibly capture background knowledge and common ground. To illustrate, imagine that Bob has three apples, which Ann cannot see. Bob says, ``Some of the apples are red.'' Ann makes the inference that \emph{not all} of the apples are red, because if all of the apples are red, then Bob would have said ``All of the apples are red'' in order to be maximally informative. The pragmatic strengthening of ``some'' to ``some but not all''---termed scalar implicature---can arise based on the same principles that allow a listener to infer \emph{blue square} from  ``blue'' in Figure~\ref{frank-science} \cite{frank2012predicting}. However, what happens when the speaker and listener both know that the speaker's knowledge of the world is incomplete? Suppose Bob can only see two of the three apples. To choose an utterance that is maximally informative, Bob needs to consider the possible states of the world and compute the expected utility of different utterances. His choice of utterance is captured with this equation:
\begin{equation}
S_1 (u | s, a) = \sum_o S_1(u | o, a) P(o | a, s) 
\end{equation}
where $u$ is the utterance, $s$ is the true state of the three apples, $a$ is Bob's perceptual access to the three apples, and $o$ is what he observed. Given that Ann knows Bob's perceptual access to the apples, (i.e.  $a$ is common knowledge between Ann and Bob), her inference is captured by the following:
\begin{equation}
L_1(s | u, a) \propto S_1(u | s, a)P(s)
\end{equation}
This model closely matches participants' interpretations of utterances given different combinations of observations and perceptual access ($r = 0.96$). This result suggests that by explicitly incorporating common ground about what the speaker knows and does not know, listeners can interpret utterances in principled ways even when the speaker has imperfect knowledge of the world.

While the RSA framework provides an intuitive and empirically validated way to model the interaction between literal meaning and background knowledge, it requires significant and theoretically important extensions to explain figurative communication.
%RSA models have been shown to be extremely flexible and productive for modeling language understanding. 
In most of the cases that RSA handles, the pragmatically strengthened interpretations produced by $L_1$ do not stray very far from the literal meanings of utterances. While interpreting ``blue'' to mean \emph{blue square} requires pragmatic enrichment, the interpreted meaning is simply more specific than the literal meaning, and not distinct from the literal meaning as is the case in many figurative uses. 
One of the key assumptions in the basic RSA model is that $S_1$ chooses an utterance that most efficiently communicates the intended meaning to $L_0$. 
Since $L_0$ interprets utterances literally, in this basic setup it is never optimal for $S_1$ to choose an utterance whose literal meaning directly contradicts the intended meaning.
%However, this type of contradictory use often characterizes figurative language.
For example, suppose $S_1$ wants to communicate that the weather is \emph{terrible}. According to the basic RSA model, $S_1$ reasons about the literal listener $L_0$ to choose the utterance that will most likely convey this information. 
Because $L_0$ is a literal listener, she would interpret the utterance ``The weather is amazing'' to mean that $S_1$ believes the weather is literally \emph{amazing}. She would thus \emph{not} arrive at the interpretation that the speaker believes the weather is \emph{terrible}. 
As a result, $S_1$ has no reason to say ``The weather is amazing'' to communicate that the weather is \emph{terrible} (because $L_0$ would not receive the intended meaning). Consequently, a pragmatic listener who reasons about why the speaker chose various utterances will not interpret ``The weather is amazing'' to mean that the weather is \emph{terrible}. 

The fact that speakers in the basic RSA model is restricted to communicating about the state of the world (in this case, the weather) means that it is unable to explain many cases of figurative language use, because the literal listener would not be able to recover that information. However, intuition as well as prior research suggests that in many cases of language use, the speaker's goal is to communicate not the full world state, but rather a subset or specific dimension of the world state, such as the speaker's attitude towards the weather. We explain this intuition and our formalization of it in more detail in the following section.


%\section{Extensions to RSA}
%We extend the RSA framework specifically to address the ways in which general background knowledge and relevance to the question under discussion shape language understanding. 
%\subsection{Reasoning about dimensions of meaning}
%The basic RSA models already incorporate some degree of background knowledge through prior beliefs. For example, to compute the probability that Bob is a wolf given the utterance ``Bob is a wolf,'' the pragmatic listener must consider the prior probability that Bob is a wolf. However, believing that Bob is a wolf is more than believing that Bob is a large wild animal that often hunts in groups. Once you believe that Bob is a wolf, you are more likely to believe that Bob is furry, fierce, loyal, fast, hungry ... any number of things that you associate with wolves. These beliefs are graded; you might have a strong belief that any given wolf is fierce, but only a weak belief that any given wolf is loyal.  This network of background knowledge forms a rich multi-dimensional representation of what it means to be a wolf. Note that while these other dimensions of meaning may not be part of the core ``literal'' meaning of the word ``wolf,'' they are associative and easily accessible. %TODO: cite
%As a result, we assume that the basic-level literal listener has access to these dimensions of meaning. The literal listener's interpretation is now given by the following: 
%
%\begin{equation}
%L_0(m_0, \vec {m} |u) = \left\{ 
%  \begin{array}{l l}
%    P(\vec {m} | m_0) & \quad \text{if $m_0$ = $u$}\\
%    0 & \quad \text{otherwise}
%  \end{array} \right.
%\end{equation}
%%
%where $m_0$ is the core literal meaning, and $\vec {m}$ is a vector of various associated encyclopedic meanings. This provides a formal way to enrich literal encoded meaning with associated background knowledge. However, introducing multiple dimensions of meaning alone is insufficient for explaining figurative language understanding. While the literal listener has access to the associated meanings, she still interprets utterances literally. Given the utterance ``Bob is a wolf,'' the literal listener will believe that Bob is a fierce, furry, and loyal wolf with some probability ($P(\vec m | m_0)$); however, she does \emph{not} believe that Bob is a fierce person or any kind of person at all, because she believes that he is a wolf with $100\%$ probability.  
%
%\subsection{Reasoning about relevance}
%
%For figurative meaning to arise, the speaker and pragmatic listener must reason about which dimension of meaning is relevant to the question under discussion. We formalize the principle of relevance by introducing a function $Q$, which projects the meaning that a literal listener derives from an utterance onto only the relevant dimension. This leads to the following utility function for speaker $S_1$:
%\begin{equation}
%U(u | m_0, \vec {m}, Q) = \log \sum_{m_0,\vec {m}'} \delta_{Q(\vec m)=Q(\vec m')} L_0(m_0, \vec m' |u)
%\end{equation}
%%
%Given this utility function, the speaker's choice of utterance is the following:
%\begin{equation}
%S_1(u | m_0, \vec m, Q) \propto e^{\lambda U(u | m_0, \vec m, Q)},
%\end{equation}
%where $\lambda$ is an optimality parameter \cite{luce2005individual}. 
%%
%The pragmatic listener $L_1$ then performs Bayesian inference to guess the intended meaning given prior knowledge and her internal model of the speaker. Since she is uncertain about the precise question under discussion, she marginalizes over the possible QUDs under consideration:
%$$
%L_1 (m_0, \vec m | u) \propto P(m_0) P(\vec m | m_0) \sum_{Q}{P (Q) S_1 (u|m_0, \vec m, Q)}
%$$
%This equation now includes multiple dimensions of meaning, the QUD, a model of the speaker's choice given he wants to be relevant and informative, and the listener's prior beliefs. Something quite magical happens when all of these elements are combined. Since the literal listener is likely to believe that Bob is fierce if she believes that Bob is a wolf, the speaker is motivated to say ``Bob is a wolf'' to get her to believe that Bob is a wolf and thus fierce. Furthermore, since the speaker only cares to communicate Bob's fierceness and not which species Bob belongs to, he does not mind that the literal listener will believe that Bob is actually a wolf. The pragmatic listener knows this about the speaker and also knows that Bob is very unlikely to actually be a wolf. Combining these pieces of information, the pragmatic listener ultimately believes that Bob is a fierce person, which is the intuitive interpretation of the sentence ``Bob is a wolf.'' By incorporating QUD inference, the model allows local context to constrain and shape the dimensions of meaning that the listener derives from an utterance. 
%
%Rational Speech-Acts (RSA) models are a family of probabilistic models that formalize the Cooperative Principle to model how people arrive at pragmatically enriched meanings of utterances \cite{goodman2014probabilistic, frank2012predicting, goodman2013knowledge}. Under the RSA framework, speaker and listener recursively reason about each other to communicate. A speaker reasons about how to get a particular meaning across to a naive listener; a more sophisticated listener then reasons about the speaker and uses Bayesian inference to recover the intended meaning.
%
%Suppose a speaker $S_1$ intends to communicate meaning $m$. He reasons about a naive listener $L_0$ and considers how likely it is that she will infer $m$ from $u$. We assume that $S_1$ incurs a ``cost'' by uttering $u$, which is proportional to the physical or cognitive effort required to produce $u$. In accordance with many models of decision-making, we compute the probability that $S_1$ will choose utterance $u$ given meaning $m$ and cost $c$ using the Luce choice rule \cite{luce2005individual}:
%\begin{equation}
%S_1 (u|m) \propto L_{0} (m|u) \cdot e^{-Cost(u)}
%\end{equation}
%%
%However, a real listener does not naively interpret utterances in a social vacuum; she considers \emph{why} the speaker chose utterance $u$ to communicate meaning $m$. This more sophisticated listener, whom we shall call $L_1$, uses Bayes' Rule to infer $m$ based on her model of how $S_1$ chooses his utterances. This is captured by the following equation:
%\begin{equation}
%L_1 (m|u) \propto P(m)S_1 (u|m)
%\end{equation}
%where $P(m)$ is the prior probability of the meaning $m$. This allows $L_1$ to incorporate background knowledge of $m$ in her interpretation. In principle, the speaker and listener can recursively reason about each other to an arbitrary depth. However, rich pragmatic effects can emerge from depths 1 and 2, which is reason to believe that this framework may be psychologically plausible for modeling pragmatic language understanding. 

\subsection{Rational Speech-Acts Model with QUD inference}

We extend the RSA framework to accommodate the idea that speakers may only want to communicate specific dimensions of the world that are relevant to the conversation at hand, or to the QUD. We show that this extension enables literal meaning, encyclopedic knowledge, prior beliefs, and contextual information to work together to produce figurative interpretations. 
%shape language understanding through reasoning about relevance to the QUD.
%question under discussion, or QUD \cite{roberts1996information}. A QUD picks out an immediate topic under discussion, which the speaker is likely to address in order to maintain discourse coherence and achieve the goals of the conversation. Note that the QUD does not have to be established by an explicit question, although it can be; instead, a question under discussion is more generally a contextual element that ``proffers a set of relevant alternatives which the interlocutors
%commit themselves to addressing'' \cite{roberts1996information}. In other words, a QUD helps determine the speaker's  communicate goal by constraining the set of things that the speaker may want to address.

The basic RSA models already naturally incorporate aspects of background knowledge and prior beliefs. For example, consider the utterance: ``Cam is a wolf.'' To compute the probability that Cam is a wolf given this utterance, the pragmatic listener considers the prior probability of Cam being a wolf. On the other hand, believing that Cam is a wolf is more than believing that Cam is a large wild animal that often hunts in groups. Once you believe that Cam is a wolf, you are also likely to believe that Cam is furry, fierce, loyal, fast, hungry, etc. These beliefs are graded; one may have a strong belief that any given wolf is fierce, but only a weak belief that any given wolf is loyal.  This network of encyclopedic knowledge forms a rich multi-dimensional representation of what it means to be a wolf. Note that while these other dimensions of meaning may not be part of the core ``literal'' meaning of the word ``wolf,'' they are easily accessible through association and are closely tied to the literal meaning. As a result, we assume that the literal listener $L_0$ also has access to these dimensions of meaning. 

We now revise the literal listener's interpretation given the assumption that she has access to these encyclopedic meanings. Given a world state $s$, associated encyclopedic meanings $\vec{A}$, and an utterance $u$, the $L_0$'s interpretation of $u$ is now given by the following: 

\begin{equation}
L_0(s, \vec {A} | u) = \left\{ 
  \begin{array}{l l}
    P(\vec {A} | s) & \quad \text{if $s$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
We thus provide a formal way of enriching literal meaning with encyclopedic knowledge. However, incorporating encyclopedic knowledge alone is insufficient for explaining figurative language understanding. Although the literal listener now has access to the associated encyclopedic meanings, she still assigns $0$ probability to all interpretations that are incompatible with the literal meaning of the utterance. Given the utterance ``Bob is a wolf,'' the literal listener will believe that Bob is a fierce, furry, and loyal wolf with some probability $P(\vec A | s)$; however, she does \emph{not} believe that Bob is a fierce person or any kind of person at all, because she believes that he is a wolf with probability $1$.

For figurative meaning to arise, the speaker and pragmatic listener must reason about the QUD and which dimension of meaning is relevant to it.
We formalize relevance to the QUD by introducing a function $Q$, which projects the meaning that a literal listener derives from an utterance onto only the dimension that is under discussion. In other words, the speaker does not care about whether the literal listener derives true information regarding any of the other dimensions; the utility of an utterance is determined only by whether information along the QUD dimension is communicated  to the literal listener. This leads to the following revised utility function for speaker $S_1$:
\begin{equation}
U(u | s, \vec {A}, Q) = \log \sum_{s', \vec {A}'} \delta_{Q(s, \vec A)=Q(s', \vec A')} L_0(s', \vec A' |u)
\end{equation}
%
Based on this utility function, the speaker's choice of utterance is specified by the following:
\begin{equation}
S_1(u | s, \vec A, Q) \propto e^{\lambda U(u | s, \vec A, Q)},
\end{equation}
%where $\lambda$ is a rationality parameter that determines the speaker's tendency to choose the optimally informative utterance \cite{luce2005individual}. 
%
Consistent with the basic RSA models, the pragmatic listener $L_1$ performs Bayesian inference to guess the intended meaning given prior knowledge and her internal model of the speaker. Since $L_1$ is uncertain about the precise QUD that the speaker is trying to address, she marginalizes over the possible QUDs under consideration:
\begin{equation}
L_1 (s, \vec A | u) \propto P(s) P(\vec A | s) \sum_{Q}{P (Q) S_1 (u|s, \vec A, Q)}
\label{L1}
\end{equation}
This equation now includes prior beliefs about likely world states $P(s)$, encyclopedic knowledge associated with those world states $P(\vec A | s)$, the QUD $Q$, and a model of the speaker's choice of utterance given that he wants to be informative and relevant to the QUD. 

Something quite magical happens when all of these elements are combined. We illustrate this with the example utterance ``Cam is a wolf'' and a set of QUDs that includes Cam's personality characteristics. Since the literal listener is likely to believe that Cam is fierce if she believes that Cam is a wolf, the speaker is motivated to say ``Cam is a wolf'' to get her to believe that Cam is a wolf and thus fierce. A speaker who only cares to communicate Cam's fierceness (the QUD dimension) and not which species Cam belongs to will not mind that the literal listener will believe that Cam is actually a wolf. The pragmatic listener then simulates the speaker's choice of utterance given different QUDs. Combining this simulation with the prior belief that Cam is very unlikely to actually be a wolf, the pragmatic listener ultimately believes that Cam is a fierce person, which is the intuitive interpretation of the utterance ``Cam is a wolf.'' 
This simple example suggests that by incorporating QUD inference with encyclopedic knowledge, the pragmatic listener modeled by RSA is free to produce interpretations of utterances that are not literally true. We will refer to this RSA model extended to include QUD inference as the qRSA model.
%In particular, we will show that the model captures several desired effects in the interpretation of \emph{hyperbole}, \emph{irony}, and \emph{metaphor}: (1) figurative interpretation (2) sensitivity to encyclopedic knowledge (3) sensitivity to prior beliefs (4) sensitivity to utterance cost (5) sensitivity to local context (6) sensitivity to alternative utterances. %\todo{think about order of these effects} 

\section{Modeling Figurative Language}
We describe three domains in which we empirically tested qRSA model: \emph{hyperbole}, \emph{irony}, and \emph{metaphor}. Through experiments and model comparison, we show that the qRSA model captures several desired effects in figurative interpretation.

Our first attempt to test the qRSA model on figurative language focused on cases where the literal semantics are simple to quantify and relatively uncontroversial: number words. Although numbers have precise meanings in mathematics, they are interpreted in various nonliteral ways in natural language. For example, ``It's 90 degrees'' is likely to be interpreted as meaning it's approximately $90$ degrees, while ``It's 92 degrees'' is more likely to be interpreted as exactly $92$ degrees. This tendency to expand the interpretation of round numbers to include neighboring numbers is known as pragmatic halo \cite{lasersohn1999pragmatic}. 
Even more dramatically, an utterance such as ``It's 1000 degrees outside'' is likely to receive a hyperbolic interpretation: it is very hot outside, but the temperature is much less than 1000 degrees.
%Here we describe behavioral experiments and specific implementations of the qRSA model that demonstrate interesting pragmatic effects in hyperbole, irony, and metaphor. The work on hyperbole has been previously published in \citeA{kao2014figurative} and will only be briefly summarized; the experiments and models for irony and metaphor will be described in detail.
%
%\subsection{Hyperbole}
%%A hyperbole is an exaggerated statement that purposefully presents its subject as more extreme than it actually is \cite{roberts1994people, mccarthy2004there}. 
%%%Rhetoric studies in ancient Greece regarded hyperbole as a major figure of speech, often used to persuade and demonstrate power \cite{smith1969mystery}. 
%%In a modern analysis of a corpus of spoken English, \cite{mccarthy2004there} found that hyperbole occurs frequently in everyday conversations and is often used to express emotions and provoke humorous responses. 
%%\citeA{norrick1982semantics} proposed that hyperbole is characterized by three properties: its affective dimension, its pragmatic nature, and its function as a vertical-scale metaphor where the comparison is between different positions on a scale rather than between discrete concepts. For a hyperbolic statement to be interpreted successfully, the listener must recognize the non-veridicality of the statement, thus entering an activity of joint pretense \cite{clark1996using}.  Hyperbolic statements often include extreme case formulations (e.g. ``It was the biggest storm in the history of the universe'') or implausible descriptions (e.g. ``It was a thousand degrees outside.'') These demonstrations of non-veridicality then require the listener to produce what \citeA{fogelin2011figuratively} called a ``corrective'' response that is more in line with reality.
%Our first attempt , we focus on the nonliteral interpretation of number words. We chose number words be- cause they have precise literal meanings that can be easily modeled, and apply to domains (such as prices) that lend themselves to quantitative measurement. 
%
%\begin{figure}[t]
%\centering
%\scalebox{0.33}{\includegraphics{Figures/figure2a-revised.pdf}}
%\caption{Model predictions v.s. average human interpretations. Each point represents an utterance and price state pair ($u, s$). The x-coordinate of each point is the probability of the model interpreting utterance $u$ as meaning price state $s$; the y-coordinate is the empirical probability. Correlation between model and human interpretations is $0.968$.}
%\label{model_fit}
%\end{figure}
%
%\begin{figure}[t]
%\centering
%\scalebox{0.33}{\includegraphics{Figures/figure4a-revised.pdf}}
%\caption{Model predictions of affect v.s. human ratings. Each point represents an utterance and price state pair $(u, s)$. For pairs where $u = s$, the utterance is literal; for $u > s$, the utterance is hyperbolic. The x-coordinate of each point is the model's prediction of the probability that the utterance/price state pair conveys affect; the y-coordinate is participants' affect ratings (error bars are standard error). Correlation between model and humans is $0.775$.}
%\label{affect}
%\end{figure}
%
%
In \citeA{kao2014nonliteral}, we examined how people arrive at the appropriate interpretations and affective subtexts of numeric utterances, specifically in the domain of prices.
%We conducted experiments using the prices of three everyday items---electric kettles, watches, and laptops. 
To examine the effect of prior beliefs on nonliteral interpretation, 
we asked participants to rate the probabilities that different items (electric kettles, watches, and laptops) cost various amounts of money (e.g. \$50, \$51, \$1,000, \$10,000). To measure people's encyclopedic knowledge about prices, we asked participants to rate the probability that someone would think an item that costs \$$x$ is expensive (e.g., a watch that costs \$1,000), thus explicitly introducing subjective attitudes (judgments of expensiveness) as a dimension of meaning.
%We chose expensiveness as the associated dimension of interest, because utterances about cost seem to naturally evoke judgments of expensiveness \todo{explain this more}.   
%Using the empirically measured prior beliefs and encyclopedic knowledge, the qRSA model produced interpretations for each utterance. 
The model reasons about different types of QUDs that the speaker may wish to address, including the exact price of the item, the approximate price, and the speaker's subjective judgment about the price. 
%By assuming that the speaker may wish to address QUDs beyond the exact price of the item, the model produces nonliteral interpretations and captures both pragmatic halo and hyperbole. 
To interpret an utterance such as ``The laptop cost 1000 dollars,'' the model reasons as follows: the price could have been \$999, and the speaker may have chosen to say ``1000 dollars'' because he only cares to communicate the approximate price to the listener, and ``a thousand'' is more efficient to produce than ``nine hundred and ninety nine.''  To interpret an utterance such as ``The electric kettle cost 1000 dollars,'' the model reasons that a kettle is highly unlikely to cost \$1000; instead, the speaker probably chose to say ``1000 dollars'' because he only cares to communicate his attitude towards the price, and a literal listener is most likely to infer that he thinks it is expensive given an extremely high price. %saying an extremely high number is the most likely to communicate ``e t prior probabilities of watches are a basic feature of hyperbole: utterances whose literal meanings are less likely given the price prior are more likely to be interpreted hyperbolically. For example, ``The watch cost 1000 dollars'' is more likely to be interpreted hyperbolically than ``The  laptop cost 1000 dollars.'' 

To quantitatively evaluate the model's predictions, we asked participants to interpret various numeric utterances (e.g., Bob said: ``The electric kettle cost 1000 dollars.'' How likely is it that the electric kettle cost $x$ dollars?) For all utterances, we compared the model's predictions with participants' responses, and found that they are highly correlated ($r=0.968, p<0.0001$). %(Figure \ref{model_fit}), 
This result suggests that the qRSA model is able to combine linguistic information, background knowledge, and reasoning about the speaker's goals to produce nonliteral, hyperbolic interpretations. 

%\begin{figure}
%\centering
%\scalebox{0.4}{
%\includegraphics{figure2b-revised.pdf}}
%\caption{Comparison of models with different communicative goals and human interpretations for the utterance ``The electric kettle cost 1,000 dollars.'' A model that considers both affect and precision goals (full model) most closely matches human data.}
%\label{goals}
%\end{figure}

In addition to producing the appropriate corrective response to hyperbolic utterances, the model also captures the affective subtext of hyperbole. We conducted a separate experiment to examine peoples' interpretation of affect in hyperbolic versus literal utterances. Participants read scenarios in which a speaker bought an item that cost $s$ dollars and says it cost $u$ dollars, where $u \geq s$. They then rated how likely it is that the buyer thinks the item was too expensive. Results showed that cases where $u > s$ (hyperbolic utterances) are rated as significantly more likely to convey affect than utterances where $u {=} s$ (literal utterances) ($F(1, 25)=12.57, p < 0.005$). Moreover, if a watch actually cost $100$ dollars and Sam produces a hyperbolic utterance such as ``The watch cost $1000$ dollars,'' participants are more likely to believe that Sam thinks the watch is expensive than if the watch \emph{actually} cost $1000$ dollars and Sam produces an identical (but in this case literal) utterance: ``The watch cost $1000$ dollars.'' This result suggests that listeners infer affect from hyperbolic utterances above and beyond the affect associated  with a given price state. Quantitatively, we compared model and human interpretations of affect for each of the 45 utterance and price state pairs $(u, s)$ where $u \geq s$. While there is a significant amount of noise in the human judgments (average split-half correlation is $0.833$), the model predicts human interpretations of the utterances' affective subtext significantly better than chance ($r=0.775, p < 0.00001$), capturing most of the reliable variation in these data. %(Figure \ref{affect}).
 
Results from \citeA{kao2014nonliteral} suggest that by incorporating inferences about the speaker's communicative goals, the qRSA model successfully interprets hyperbolic utterances and recovers the appropriate affective subtext. 
However, in this initial exploration of applying the qRSA model to figurative language, we only considered a very simple kind of encyclopedic knowledge, namely how likely it is for a state of the world (a price) to be associated with a negative feeling (expensiveness).
This simplification overlooks the range of attitudes and emotions that speakers could express using figurative utterances. 
In the next section, we explore how expanding the space of encyclopedic knowledge to include more complex emotions accounts for people's interpretations of ironic utterances.
%The model's quantitative predictions closely match humans' judgments on hyperbole, a complex phenomenon previously beyond the scope of computational models.

%\todo[inline]{Propose that the same formal architecture may work for other uses of figurative language that are quite different}
\subsection{Verbal Irony}
%\todo[inline]{Introducing a richer model of affect gives irony}
An ironic statement describes something as contrary to what it actually is \cite{roberts1994people, gibbs1999figurative}. For example, a speaker who says ``Such beautiful weather we are having'' in the middle of a storm means that the weather is \emph{not} beautiful and expresses a negative attitude towards it. %Like hyperbole, verbal irony also requires the listener to recognize the non-veridicality of the utterance and enter into joint pretense. However, the required corrective response is one of ``kind'' (e.g. from ``lovely'' to ``terrible'') instead of degree (e.g. from ``pouring'' to ``drizzling'') \cite{mccarthy2004there}. 
How do people appropriately interpret these superficially positive or negative utterances? Can our model use QUD inference to interpret an utterance when its literal meaning is not just an exaggerated version of the intended meaning, but rather its opposite?
%\cite{kao2014figurative} showed that this model---which we will refer to as qRSA---produces figurative interpretations of hyperbolic utterances that closely match humans'; however, they considered only a simplified affect space, namely the presence or absence of negative feeling. This overlooks the range of attitudes and emotions that speakers could express with figurative utterances. In particular, since verbal irony involves expressing negative meanings with positive utterances and vice versa, a richer space of affect that includes both positive and negative emotions may be key. 
In this section, we will examine the consequence of expanding the set of encyclopedic knowledge we consider to an empirically derived space of emotions. We show that this minimal change enables the qRSA model to capture many of the rich inferences resulting from verbal irony.

In what follows, we will examine interpretations of potentially ironic utterances in an innocuous domain---the weather. We chose the weather as the victim of irony for several reasons. First, attitudes toward the weather do not tend to vary significantly across individuals, meaning we can measure people's perceptions and attitudes about different weather states without too much concern about individual differences, as would be the case with more polarizing topics such as movies, art, or politics. 
%, and remarks about the weather have often been used as examples in previous discussions of verbal irony\todo{cite}. 
Second, we can visually represent the weather to participants with minimal linguistic description in order to obtain measures of nonlinguistic contextual knowledge. For example, we can show participants a picture of a blue, cloudless sky and ask them to judge how likely it is that someone would perceive the weather to be \emph{amazing} or \emph{terrible}. In addition, given the critical role that prior beliefs play in understanding irony, we can vary the probability of different beliefs by easily manipulating the context (e.g. showing a picture of a gray, cloudy sky instead of blue sky) to observe how the same utterance is interpreted differently given different prior expectations. 
%This offers to our knowledge the first fine-grained manipulation and quantitative measure of context in studies of irony. 

In what follows, we first explore how an enriched space of encyclopedic knowledge affects the qRSA model and show that it produces ironic interpretations in a simple simulation. We then present two behavioral experiments that examine people's interpretations of utterances given different weather contexts. We show that by including two affective dimensions in our encyclopedic knowledge---valence and arousal---our model produces interpretations that closely match people's. 
%Finally, we discuss implications of our model for informal theories of irony and its relationship to other types of figurative language understanding.

\subsubsection{Model}

%In this section, we revisit the qRSA model and compare different spaces of affect to test the conditions for producing ironic interpretations. 

Following the qRSA model described previously, a speaker chooses an utterance that most effectively communicates information regarding the QUD to a literal listener. We consider a meaning space that consists of the variables $s, \vec A$, where $s$ is the state of the world, and $\vec A$ represents the speaker's (potentially multidimensional) affect towards the state. 
Following the formulation described in the modeling section, we formalize a QUD as a projection from the full meaning space to the subset of interest to the speaker, which could be $s$ or any of the dimensions of $\vec A$. 
We specify the speaker's utility as information gained by the listener about the topic of interest---the negative surprisal of the true state under the listener's distribution given an utterance, $u$, along the QUD dimension, $Q$. 
%This leads to the following utility function: 
%\begin{equation}
%U(u | s, A, Q) = \log \sum_{s', A'} \delta_{Q(s, A)=Q(s', A')} L_{0}(s', A' |u)
%\end{equation}
%where $L_{0}$ describes the literal listener, who updates her prior beliefs about $s, A$ by assuming the utterance to be true of $s$. 
%The speaker's choice of utterance $u$ given state $s$, his affect $A$ towards the state, and the QUD is then described by the following:
%$S_1(u | s, A, Q) \propto e^{\lambda U(u | s, A, Q)}$,
%where $\lambda$ is the rationality parameter.
%%
A pragmatic listener $L_{1}$ takes into account prior knowledge and his internal model of the speaker to determine the state of the world as well as the speaker's affect, marginalizing over the possible QUDs under consideration (see Equation~\ref{L1}).
%$$
%L_{1}(s, A | u) \propto P(s) P(A | s) \sum_{q}{P (q) S (u|s, A, q)}
%$$
%
We characterize the interpretation of an utterance as the resulting posterior distribution over world states and speaker affects, $L_1(s, \vec A | u)$.

We performed the following simulations to examine the model's behavior using affect spaces, $\vec A$, that differ in complexity and structure.  
We assume that the state of the weather $s$ can be one of five possible ordered values: \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, and \textit{amazing}. We consider two different weather contexts: apparently bad weather and apparently amazing weather, which are each specified by a prior distribution over these states (see gray dotted lines in Figure~\ref{sim12}). We then examine how the model interprets the sentence ``The weather is terrible'' in each of the two weather contexts, given different affect spaces.

\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/sim12.pdf}}
\caption{Model interpretations of ``The weather is terrible'' given different prior beliefs about the weather state and affect dimensions. Gray dotted lines indicate prior beliefs about weather states given a weather context; blue lines indicate interpretations when reasoning only about the speaker's valence; orange lines indicate interpretations when reasoning about both valence and arousal.}
\label{sim12}
\end{figure}


We first consider a one-dimensional affect space, where the dimension is emotional valence, and the values are whether the speaker feels negative or positive valence towards the state.  
The blue lines in Figure~\ref{sim12} show the model's interpretation of ``The weather is terrible'' using this one-dimensional affect space. 
The model is capable of non-literal interpretation: it produces a hyperbolic interpretation (that the weather is merely \textit{bad}) given ``The weather is terrible'' in the bad weather situation. However, it produces a literal interpretation (that the weather is \textit{terrible}) in the amazing weather situation. This is because a pragmatic listener who only considers emotional valence does not believe that the speaker has any reason to choose a negative utterance to express positive affect (because the utterance communicates no true information). As a result, a pragmatic listener that only considers one dimension of affect--emotional valence---is unlikely to infer a positive world state from a negative utterance (and vice versa), thus failing to evidence verbal irony. 

%We now consider a more complex affect space with two dimensions---valence and arousal---to observe its consequence on interpretation. 
This model simulation reveals a critical puzzle in the interpretation of verbal irony. What true information \emph{could} a speaker communicate about a positive world state using a negative utterance? Affective science identifies two dimensions, termed valence and arousal, that underly the slew of emotions people experience \cite{russell1980circumplex}. 
For example, \emph{anger} is a negative valence and high arousal emotion, while \emph{contentment} is a positive valence and low arousal emotion. 
%These two dimensions are not independent, with low arousal implying more neutral valence. 
%\todo{JTK: is this non-independence important/necessary to note, or is it confusing?}
%\todo{NDG: is this dependence built into the model's prior? if so, does it matter? if not, we should explore it...} 
Could speakers leverage the arousal dimension to convey high arousal and positive affect (e.g. excitement) using utterances whose literal meanings are associated with high arousal but negative affect (e.g. ``The weather is terrible!'')? We test the consequences of incorporating a dimension of emotional arousal in the space of affects a listener considers. The orange lines in Figure~\ref{sim12} show simulations of the qRSA model with a two-dimensional affect space: whether the speaker feels negative/positive valence and low/high arousal towards the weather state. Given strong prior belief that the weather state is \textit{bad}, the model interprets ``The weather is terrible'' to mean that the weather is likely to be \textit{bad}, again producing a hyperbolic interpretation. However, given strong prior belief that the weather is \textit{amazing}, the model now places much greater probability on the ironic interpretation of ``The weather is terrible,'' meaning that the weather is likely \textit{amazing}. This is because, with the enriched two-dimensional affect space, the pragmatic listener realizes that the speaker may be using ``terrible'' to communicate high emotional arousal. 
%Note that this result is not simply due to the model falling back on the prior: given the same priors, the model interprets the neutral utterance ``The weather is ok'' as the weather state being \textit{neutral} and not \textit{amazing}.
These simulations suggest that a more psychologically realistic, two-dimensional affect space enables the qRSA model to interpret ironic utterances in addition to hyperbolic ones. 


%\todo{NDG: perhaps we could have marker size in fig 1 correspond to interpreted arousal? never mind, we don't have that for the first sim...}

%\begin{figure}
%    %\centering
%    %\begin{minipage}{0.45\textwidth}
%        %\centering
%        \includegraphics[width=320pt, height=250pt]{Figures/image-grid}
%        \caption{Weather images shown to participants in Experiments 1 and 2.}
%        \label{images}
%    %\end{minipage}
%\end{figure}
    %\begin{minipage}{0.45\textwidth}
       % \centering
To quantitatively test whether the qRSA model with expanded affect space can capture a range of ironic interpretations, we need appropriate prior distributions as well as data for human interpretations.
We conducted Experiment 1a to measure prior beliefs over weather states ($P(s)$) for a range of weather contexts, as well as the likelihood of various emotions towards each weather state. Information about emotions associated with each weather state allows us to empirically derive the affective space and priors, $P(\vec A | s)$, for this domain.
In Experiment 1b, we collected people's ratings of how a speaker perceives and feels about the weather given what she says in a weather context (e.g. ``The weather is terrible!'' when the context clearly depicts sunny weather).

 \begin{figure}
 \centering
        \includegraphics[width=380pt, height=250pt]{Figures/irony-state-prior-withpic.png}
        \caption{Smoothed prior probability distributions over weather states for each of the nine weather contexts. Participants saw each image and chose a state label from the set: \textit{terrible, bad, neutral, good, amazing}. Probability distributions over weather states were computed by performing Laplace smoothing on the counts for each state label given a weather context and normalizing the counts to sum up to $1$.  
     %Each panel represents a weather context; each line represents the distribution over weather states for a weather context prior to any linguistic input. For example, the vast majority of participants rated weather context 1 as \emph{amazing}, suggesting that the prior probability of someone believing weather context 1 to be \emph{amazing} is extremely high.
     }
        \label{irony-priors}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure}


\subsubsection{Experiment 1a: Background knowledge for verbal irony}
%\subsubsection{Experiment 1a: Prior elicitation}
%textbf{Materials and methods}\\
We selected nine images from Google Images that depict the weather. To cover a range of weather states, three of the images were of sunny weather, three of cloudy weather, and three of rainy or snowy weather. Each of these images represent what we will call a ``weather context,'' as shown in Figure~\ref{irony-priors}.

$49$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw all nine images in random order. In each trial, participants were told that a person (e.g.~Ann) looks out the window and sees the view depicted by the image. They then indicated how Ann would rate the weather using a labeled 5-point scale, ranging from \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, to \textit{amazing}. Participants also used slider bars (end points labeled ``Impossible'' and ``Absolutely certain'') to rate how likely Ann is to feel each of the following seven emotions about the weather: \emph{excited}, \emph{happy}, \emph{content}, \emph{neutral}, \emph{sad}, \emph{disgusted}, and \emph{angry}, which are common emotion categories \cite{ekman1992argument}\footnote{From the most frequently cited set of six basic emotions, we removed \emph{fear} and \emph{surprise} and added \emph{content} and \emph{excited} to have a balanced set of positive and negative emotions. We also added \emph{neutral} to span a wider range of emotional arousal.}.
The order of the emotions was randomized for each participant but remained consistent across trials\footnote{Link to Experiment 1a: \url{http://stanford.edu/~justinek/irony_exp/priors/priors.html}}.
%\textit{Results}

\begin{figure}
\centering
        \includegraphics[width=320pt, height=250pt]{Figures/irony-biplot-labeled.pdf}
        \caption{Biplot of the first two principle components of the seven emotion ratings. The first two PCs correspond roughly to valence and arousal, with positively valenced emotions (\textit{excited, happy, content}) clustering on the right, and more high arousal emotions (\textit{disgusted, excited}) appearing appearing at the top.}
        \label{irony-pca}
\end{figure}

For each of the nine weather contexts, we obtained the number of participants who gave each of the weather state ratings. We performed add-one Laplace smoothing on the counts to compute a smoothed prior distribution over weather states given each context, namely $P(s)$ (Figure~\ref{irony-priors}).
% shows that the sunny and positive weather contexts were more likely to be rated as \texttt{amazing}, while the negative weather contexts were more likely to be rated as \texttt{bad} or \texttt{terrible}.  %\todo[inline]{RDH: obviously, need some way of identifying what the different rows and columns of the grid mean}
To examine participants' ratings of the affect associated with each context, we first performed Principal Component Analysis (PCA) on the seven emotion category ratings. This allowed us to compress the ratings onto a lower-dimensional space and reveal the main affective dimensions that are important in this domain, as is often done in studies of emotion ratings \cite{russell1980circumplex}. We found that the first two principal components corresponded to the dimensions of emotional valence and emotional arousal, accounting for $69.14\%$ and $13.86\%$ of the variance in the data, respectively. As Figure~\ref{irony-pca} shows, the first two principle components successfully distinguish positively valenced emotions (\emph{excited, happy, content}) from negatively valenced emotions (\emph{disgusted, angry, sad)}, as well as high arousal emotions (\emph{excited, disgusted}) from low arousal emotions (\emph{content, neutral, sad}). 
%\todo{we should do a free response version of this task (``Ann feels ..... about the weather.''), before and after a statement, at some point to see if we are missing any affects... probably not needed for cogsci.}

    
\begin{figure}
    %\begin{minipage}{0.45\textwidth}
    \centering
       \includegraphics[width=320pt, height=180pt]{Figures/irony-affect-prior.pdf}
       \caption{Average probabilities of positive valence and high arousal given each weather state. Error bars are $95\%$ confidence intervals. Probability of positive valence increases monotonically over the five weather states; probability of high arousal follows a symmetric U-shaped curve and does not differ significantly for the \textit{terrible} and \textit{amazing} weather states.}        
        \label{irony-affect-prior}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure}

The PCA represents emotion ratings for each trial as real values between negative and positive infinity on each of the dimensions. To map these values onto probability space, we first standardized the scores on each dimension to have zero mean and unit variance. We then used the cumulative distribution function to convert the standardized scores into values between $0$ and $1$. 
This gives us the probabilities of Ann feeling positive (vs.~negative) valence and high (vs.~low) arousal for each trial, which is a two-dimensional probabilistic representation of her affect.
%Although these values are treated as probabilities of a binary variable, they approximately track the \emph{degree} of positive valence and arousal as well; for example, $P(A_v) = 0.6$ suggests that the valence is more positive than neutral, and $P(A_a) = 0.1$ suggests that the arousal is low. 
By calculating the average probabilities of positive valence and high arousal given each weather state rating, we obtain the probability of positive valence and high arousal associated with each weather state, namely $P(A | s)$ (Figure ~\ref{irony-affect-prior}). We observe that the probability of positive valence given a weather state increases monotonically across the ordered set of states: \emph{terrible, bad, neutral, good}, and \emph{amazing}, where the probability of positive valence given a \emph{terrible} state is significantly lower than the probability given an \emph{amazing} state. However, the probability of high arousal given each weather state follows a U-shape curve, where the probability of high arousal given a \emph{terrible} state is approximately equivalent to the probability of high arousal given an \emph{amazing} state. In other words, while the valences associated with \emph{terrible} and \emph{amazing} differ significantly, the arousals evoked by these states are actually very similar.
%\todo[inline]{NDG: does this imply that valence and arousal are independent? they generally aren't.... i'm a bit confused about where the model's $A$ states fall with respect to the valence-arousal quadratic curve....}

%\todo{JTK: something about how this is not a perfect transformation but does approximately the right thing} 

%\begin{figure}
%\begin{minipage}[b]{.5\textwidth}
%\includegraphics[width=5cm, height=3.7cm]{priors.pdf}
%\end{minipage}
%\begin{minipage}[b]{.4\textwidth}
%\includegraphics[width=5cm, height=5cm]{biplot.pdf}
%\end{minipage}
%
%\end{figure}


%\todo{Is this figure useful?}


\subsubsection{Experiment 1b: Interpreting verbal irony}
%In Experiment 1, we obtained the prior distribution over weather states for each weather context as well as the prior probabilities of positive valence and high arousal given each weather state. 
%Results from Experiment 1b give us the components to generate interpretations of utterances from our model. 
We conducted Experiment 1b to elicit people's interpretations of utterances, which we then use to evaluate model predictions. 
%\textbf{Materials and methods}\\
$59$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw all nine images from Figure~\ref{irony-priors} in random order. In each trial, participants were told that a person (e.g.~Ann) and her friend are in a room looking out the window together and see the view depicted by the image. Ann says, ``The weather is \underline{\hspace{1cm}}!'' where the adjective is randomly selected at each trial from the following set: ``terrible,'' ``bad,'' ``ok,'' ``good,'' and ``amazing.'' Participants first rated how likely it is that Ann's statement is ironic using a slider with end points labeled ``Definitely NOT ironic'' and ``Definitely ironic.'' They then indicated how Ann would actually rate the weather using a labeled 5-point scale, ranging from \textit{terrible}, \textit{bad}, \textit{neutral}, \textit{good}, to \textit{amazing}. Finally, participants used sliders to rate how likely Ann is to feel each of seven emotions about the weather \footnote{Link to Experiment 1b: \url{http://stanford.edu/~justinek/irony_exp/interpretation/interpretation_askIrony.html}}. 

%\textbf{Results}\\
We first examined participants' irony ratings for each of the weather context and utterance pairs. We found a basic irony effect, where utterances whose polarities are inconsistent with the polarity of the weather context are rated as significantly more ironic than utterances whose polarities are consistent with the weather context ($t(34.16)= -11.12, p < 0.0001$). For example, ``The weather is terrible'' (a negative utterance) is rated as more ironic in Weather Context 1 (positive context) ($M = 0.90$, $SD = 0.21$) than in Weather Context 7 (negative context) ($M =0.15$,	$SD = 0.27$). A linear regression model with the polarity of the utterance, the polarity of the weather context, and their interaction as predictors of irony ratings produced an adjusted $R^{2}$ of 0.91, capturing most of the variance in the data. This suggests that participants' lay judgments of irony align with its basic definition: utterances whose apparent meanings are opposite in polarity to the speaker's intended meaning.

\begin{figure}
\centering
\scalebox{0.47}{\includegraphics{Figures/irony-state-human-model-withpics.png}}
\caption{Model's and participants' inferences about the weather state (x-axis) given a weather context (column) and an utterance (row). Each panel represents an interpretation given an utterance in a weather context. The dark lines are participants' ratings; the light lines are the model's posterior distributions over weather states.}
\label{irony-model-state}
\end{figure}

\begin{figure}
\centering
\scalebox{0.47}{\includegraphics{Figures/irony-affect-human-model-withpics.png}}
\caption{Model's and participants' inferences about the probability of valence and arousal (row) given a weather context (column) and an utterance (x-axis). The dark lines are participants' ratings; the light lines are the model's posterior probabilities of positive valence and high arousal given an utterance in a weather context. The dotted lines are prior probabilities of positive valence and high arousal for each weather context. Error bars are $95\%$ confidence intervals on the participants' ratings.}
\label{irony-model-affect}
\end{figure}

Given that participants can identify verbal irony based on its inconsistency with context, how do they then use context to determine the speaker's intended meaning? We examined participants' interpretations of utterances given different contexts. For each of the $45$ weather context (9) $\times$ utterance (5) pairs, we obtained the number of participants who gave each of the five weather state ratings (\textit{terrible, bad, neutral, good, amazing}). We performed add-one Laplace smoothing on the counts to obtain a smoothed distribution over weather states given each context and utterance (dark lines in Figure~\ref{irony-model-state}). Results show that participants produce ironic interpretations of utterances, such that the weather is most likely to be \textit{amazing} given that the speaker said ``The weather is terrible'' in Weather Context 1. Participants also produce hyperbolic interpretations, such that the weather is most likely to be \textit{bad} given that the speaker said ``The weather is terrible'' in Weather Context 7. This confirms the intuition that people are highly sensitive to their contextually determined prior beliefs, and use these beliefs both to determine when an utterance is not meant literally and to appropriately recover the intended meaning. 

Finally, we examine participants' inferences about the speaker's affect given utterances in context. We used the loadings from the PCA on emotion ratings from Experiment 1a to project the emotion ratings from Experiment 1b onto the same dimensions. We then standardized and converted the scores into values between $0$ and $1$, as before, which gives us probability ratings of the speaker feeling positive valence and high arousal given an utterance and weather context.
We visualize the valence and arousal expressed by different utterances in different contexts using Figure ~\ref{irony-model-affect}. The dotted gray lines are the average prior probabilities of positive valence and high arousal associated with each weather context (without any linguistic input), taken from Experiment 1a. 
The dark solid lines show participants' average probability ratings of valence and arousal given an utterance in a weather context. We observe that participants' valence and arousal ratings vary systematically based on both the utterance the weather context. %are a function of both the utterance and the weather context. 
%For the \emph{positive} weather contexts, the speaker is interpreted as more likely expressing positive valence given the extremely negative utterance ``terrible'' than given the less negative utterance ``bad.'' On the other hand, for the \emph{negative} weather contexts, the speaker is interpreted as less likely expressing positive valence given the extremely positive utterance ``amazing'' than the less positive utterance ``good'' (STATS). This suggests that extreme utterances that are inconsistent with the valence of the context are more likely to express an opposite affect than its literal meaning.
%\todo[inline]{maybe you could use different names for the weather contexts? Like $\mathcal{W}_{neg}, \mathcal{W}_{neut}, \mathcal{W}_{pos}$, and then refer to $w_i \in \mathcal{W}_{neg}$ and so on.}

%Figure ~\ref{arousal} shows the average probability of \emph{high arousal} given an utterance in a weather context. The dotted gray lines are the average probabilities of high arousal associated with each weather context without any linguistic input, taken from Experiment 1. 
%We see that regardless of the weather context, more extreme utterances (e.g. ``terrible'' and ``amazing'') are more likely to communicate high arousal.
%\todo[inline]{JTK: figure out data take-home point here}
%\todo{it's interesting that ``terrible'' seems to have suppressed arousal compared to ``bad''. any thoughts about why?}


%We call this distribution the \emph{interpreted meaning} of an utterance in context. We found that participants often rated the speaker (e.g. Ann) as judging the actual weather state to be different from what she described in her utterance, suggesting that participants interpreted these utterances non-literally. To further examine the relationship between literal meaning, interpreted meaning, and judgements of irony, we computed the Kullback-Leibler divergence between the literal interpretation of the utterance and the distribution over weather states given the context and utterance. 
%\todo{what is the literal distribution? if it's delta on the word corresponding state, then isn't KL just the interpretation log-prob of the state?}
%\todo[inline]{RDH: I don't quite understand what the KL divergence is measuring in the next paragraph}
%
%We found that adding the KL divergence measure to the linear regression model captured significantly more variance in the irony ratings, with an R-squared of 0.93 (more STATS). 
%\todo{this doesn't seem like the right analysis.... and a 1\% gain is pretty minimal to fuss about.} 
\begin{figure}
\centering
%\begin{subfigure}{0.7\textwidth}
\scalebox{0.6}{\includegraphics{Figures/irony-human-model-scatter-cropped.pdf}}
\caption{Scatter plot showing correlations between model predictions and human ratings for weather state, speaker valence, and speaker affect. Each dot in a panel represents the interpretation of an utterance in a weather situation, along the dimensions of weather state, valence, and arousal. The darkness of the dots indicate participants' irony ratings for the utterances.}
\label{irony-scatter}
%\end{subfigure}
%\begin{subfigure}{0.43\textwidth}
%\includegraphics[width=220pt, height=140pt]{model-arousal.pdf}
%\caption{Average probabilities of speaker feeling high arousal given her utterance in a weather context.}
%\label{arousal}
%\end{subfigure}
%\begin{subfigure}{0.15\textwidth}
%\includegraphics[width=75pt, height=140pt]{scatters.pdf}
%\caption{Scatter plot of human versus model interpretations.}
%\label{scatter}
%\end{subfigure}
\end{figure}

\subsubsection{Irony model evaluation}
From Experiment 1a, we obtained the prior probability of a weather state given a context ($P(s)$) as well as the probability of affect given a weather state ($P(A | s)$). In addition, we fit three free parameters to maximize correlation with data from Experiment 1b: the speaker optimality parameter ($\lambda = 1)$ and the prior probability of each of the three QUDs ($P(q_{state}) = 0.3$, $P(q_{valence}) = 0.3$, $P(q_{arousal}) = 0.4$)\footnote{Since $P(q_{state}) + P(q_{valence}) + P(q_{arousal}) = 1$, $P(q_{arousal})$ is determined by the other two QUD parameters and not a free parameter.}.
%\todo{NDG: the three QUD probs aren't independent, so only 3 free params, right?}  
For each of the $45$ utterance and weather context pairs, the model produced an interpretation consisting of the joint posterior distribution $P(s, \vec A | u)$, where $\vec A$ can be further broken down into valence and arousal dimensions. We will examine the model's performance on each of these state and affect dimensions by marginalizing over the other dimensions.

%\todo[inline]{NDG: for each of these correlations also report percent of explainable variance captured with CI (footnote can explain what that is) JTK: still need to do this.}
Figure ~\ref{irony-scatter} shows scatter plots correlating model predictions with human interpretation data for each of the dimensions: weather state, valence, and arousal. %Figure ~\ref{model-state} shows participants' and the model's inferences about the actual weather state given an utterance and a weather context. 
The model predictions of weather state given utterance match humans' interpretations, with a correlation of $0.86$. Since the split-half correlation for the human data is $\rho=0.898$ ($95\%$CI $= [0.892, 0.903]$)\footnote{Split-half correlations $\rho$ were calculated by repeatedly bootstrapping samples from the data (sample each participant with replacement), computing correlation between two halves of the bootstrapped samples, and using the Spearman-Brown prediction formula to estimate predicted reliability with full sample size. Confidence intervals are $95\%$ CI over $1000$ iterations of bootstrap sampling.\label{splithalf}}  we find that our model captures much of the explainable variance in human judgements. The model predicts humans' interpretations of valence extremely well, with a correlation of $0.96$, capturing essentially all of the explainable variance in the data ($\rho = 0.948\pm0.001$).
%Figure ~\ref{valence} shows participants' and the models' inference about the speaker's valence given an utterance and a weather context. From the tight correspondence between model and human interpretations of valence ($r=0.96$), we see that the model is able to incorporate the valence associated with the utterance's literal meaning (e.g. ``The weather is terrible'') and the valence associated with the weather context (e.g. weather context 1) to interpret the probability of the speaker feeling positive valence. 
\begin{figure}
    %\begin{minipage}{0.45\textwidth}
       \scalebox{0.53}{\includegraphics{Figures/irony-goal-model.pdf}}
       \caption{Model's posterior distributions over QUDs given an utterance (row) in a weather context (column). The darkness of the bars indicate participants' irony ratings for the utterances. }        
        \label{irony-goal-model}
    %\end{minipage}
    %\caption{Nine weather contexts and their empirically measured priors over weather states.}
    %\label{fig:three graphs}
\end{figure}

Importantly, the model infers the appropriate valence even for utterances that are judged as highly ironic (the darker dots in Figure~\ref{irony-scatter}). Thus, the model is able to to recover the intended valence even when it is inconsistent with the valence of the utterance's literal meaning.
%\todo{note that this is the case even when interpreted valence mismatches the prior expected valence given literal meaning -- i.e. the model captures irony in valence.}
The model's predictions for emotional arousal match humans' with a correlation of $0.66$, capturing a substantial amount of the explainable variance ($\rho = 0.763\pm0.005$). However, we see from Figure~\ref{irony-model-affect} that participants tend to infer lower arousal from the utterance ``terrible'' than the model does. This could be due to the amount of noise in the measurement of arousal (given that it is derived from the second principal component of the raw emotion ratings), or to the possibility that the utterances interact with the punctuation ``!'' in unexpected ways, such that ``The weather is bad!'' is interpreted as more likely to have high arousal than ``The weather is terrible!'' because the combination is more unusual.

%Furthermore, the absolute difference between the model's inferred valence and the valence of the utterance's literal meaning correlates significantly with people's irony ratings ($r = 0.86$, $\rho=0.94 \pm 0.005$), suggesting that the model is able to use inconsistencies between literal and interpreted meanings to identify ironic uses.



Finally, the model's inferences about the QUD varies systematically across the utterances and weather contexts (Figure~\ref{irony-goal-model}). For utterances with high irony ratings (e.g. ``terrible'' uttered in Weather Context 1 and ``amazing'' uttered in Weather Context 9), the model infers that the QUD is most likely the speaker's emotional arousal, whereas for utterances with low irony ratings (e.g. ``terrible'' uttered in Weather Context 9 and ``amazing'' uttered in Weather Context 1), the model infers that the QUD is most likely the  weather state. In fact, the model's posterior probability of an arousal QUD captures participants' graded judgments of irony and is highly correlated with irony ratings ($r = 0.88$, $\rho=0.94 \pm 0.005$). This suggests that the model may be able to use inferences about the QUD to identify ironic uses, and also that the degree of perceived irony may be associated with the probability of affective QUDs such as emotional arousal.
%\todo[inline]{Do we need a figure showing correlations for irony ratings? Not as good as straight-up lm with weather context and utterance as predictors}
%\todo[inline]{JTK: still need to do stats for this}
%Figure~\ref{arousal} shows participants' and the model's inferences about the speaker's arousal. 
%\todo{why is this correlation low? is it because of the terrible-bad inversion i asked about above?}

\begin{table}
\centering
\begin{tabular}{ |c | c | c | c | c | }
  \hline
  \textbf{Model} & \textbf{State} & \textbf{Valence} & \textbf{Arousal} & \textbf{Average} \\\hline                        
  Literal & 0.38 & 0.45 & 0.49 & 0.44\\
  Prior & 0.79 & 0.84 & 0.49 &  0.71 \\
  Valence & 0.84 & 0.79 & 0.61 & 0.75 \\
  Valence + arousal & 0.86 & 0.96 & 0.66 & 0.83\\
  \hline 
  Best possible & 0.90 & 0.95 & 0.76 & 0.87\\\hline
\end{tabular}
\caption{Correlation coefficients between model predictions and human interpretations of weather state, valence, and arousal given an utterance and weather context from Experiment 2. \emph{Best possible} gives an estimate of the maximum possible correlation given noise in the data (see footnote \ref{splithalf}).}
\label{table1}
\end{table}

We considered a series of simpler models to show that the full model using a two-dimensional affect space best predicts human interpretations. 
%\todo{JTK:Should I add a line for split-half in table?}
We first examined a model that interprets utterances literally, such that ``The weather is terrible'' is always interpreted as the weather state being \textit{terrible}, along with the valence and arousal associated with \textit{terrible} weather. 
%Such a model produces a distribution over states that correlates with humans' interpretations with $r=0.38$, inference about valence that correlates with humans' with $r=0.45$, and inferences about arousal with a correlation of $r=0.49$. 
We then examined a model that simply ignores the speaker's utterance and takes into account only the state and affect priors associated with each weather context. 
%Such a model produces a distribution over states that correlates with humans' interpretations with $r=0.79$, inferences abut valence that correlate with $r=0.84$, and inferences about arousal that correlate with $r= 0.49$. 
Finally, we examined the performance of the qRSA model with a unidimensional affect space (valence only). 
Table~\ref{table1} shows the models' correlations with human judgements for state, valence, and affect.
A complete model that takes into account prior knowledge, the literal meaning of the utterance, and a two-dimensional affect space outperforms the other models. This outperformance is especially apparent with respect to inferences about valence, which is the most important aspect of understanding an ironic utterance, since the listener must infer the intended positive/negative valence from an ostensibly negative/positive utterance. These comparisons suggest that our full model successfully leverages richer knowledge of affect and uses pragmatic reasoning to produce the appropriate figurative interpretations.
%, both ironic and hyperbolic. 
%Fitting two free parameters to maximize fit with human data, the model obtains a state correlation of $r=0.84$, a valence correlation of $r=0.79$, and an arousal correlation of $r=0.61$.


%\todo[inline]{it would be nice to have some more direct analysis of irony -- that when state/valence mismatches that expected from literal meaning the model can predict it. this may also be a good place to come back to the explicit irony judgements: something like probability the interpretation is on the opposite side of the prior mean? or something more general that would also apply to ironic propositions? oh.. we should totally say something in the discussion about ironic propositions and other non-scalar irony.}

\subsubsection{Discussion}
%\todo[inline]{Add something about common ground and in-group?}
%\todo[inline]{summary of take-home point}
In this section, we formalized intuitions about verbal irony understanding and clarified the role of shared prior knowledge in ironic interpretations. We explored the consequences of expanding the space of affect considered by the qRSA model to account for verbal irony. By making a minimal extension to \citeA{kao2014nonliteral}'s hyperbole model, we were able to capture people's fine-grained interpretations of ironic utterances in addition to hyperbole. This provides evidence that hyperbole and irony may operate using similar underlying principles of communication, namely reasoning about shared background knowledge as well as the speaker's affects and attitudes.
%, which are not immediately predictable from the literal semantics of an utterance. 
%By providing fine-grained and quantitative manipulations of shared prior knowledge, we may begin to examine why using irony highlights common ground and group membership \cite{gibbs2000irony}.

%The novelty of the paper is to present a way of quantifying the prior expectations and clarifying their role in rational inferences to derive an interpretation that is not at all predictable given the semantically encoded information.

%, which aligns with other informal accounts of the pragmatics of figurative language understanding (cite).  

While our model reproduces the main characteristics of verbal irony, there remain important qualities to account for. 
Researchers have observed that ironic statements that are literally positive but express negative opinions (e.g. saying ``You're a great friend'' ironically to a friend who betrayed you) occur more frequently and are easier to comprehend than statements that are literally negative but express positive opinions (e.g. saying ``You're a terrible friend'' ironically to a friend who helped you) \cite{hancock2002production, jorgensen1984test, clark1984pretense}. This phenomenon, termed the \emph{asymmetry of affect}, is fairly robust and has been addressed by both the pretense theory \cite{clark1984pretense} and echoic mention \cite{jorgensen1984test} theory, as well as through analyses of politeness \cite{dews1995muting}
and humor \cite{matthews2006roles}. In our experiments, however, we did not observe an asymmetry of affect in the irony ratings or in the interpretation.
%\todo{stats} 
This could be because the norms and expectations for weather are not strong enough, or that the weather domain is innocuous enough such that politeness concerns are not an issue. Because we did not observe an affect asymmetry in our data, we did not explore specific ways in which the qRSA model could produce the asymmetry documented in previous studies. In future work, we plan to examine and formalize the ways in which prior expectations or the importance of politeness in different domains may account for varying degrees of affect asymmetry in irony interpretation.

%\todo{Social motivations for using irony?}

Thus far, the types of encyclopedic knowledge we explored have been restricted to affects associated with various states of the world. We also assumed that the set of QUDs under consideration includes the speaker's attitude or affect towards a state. While this assumption is supported by previous research on the rhetorical effects of hyperbole and verbal irony, it may not generalize to other types of figurative language such as metaphor. In what follows, we will explore ways to systematically elicit the set of QUDs that listeners consider, as well as manipulate prior probabilities over QUDs using discourse context. We show that considering these additional, non-affective QUDs allows the model to capture interesting effects of metaphor interpretation. 

%\todo{Something about why ``terrible'' has lower arousal.}
%For example, speakers often use verbal irony to remind the listener of previous utterances that turned out to be false, or of positive norms that were violated \cite{sperber1981irony, jorgensen1984test}. On the other hand, pretense theory argues that when a speaker produces an ironic utterance, she is only pretending to be someone who would make such an utterance \cite{clark1984pretense}. 
%In this view, recognizing the pretense is central to interpreting verbal irony.
%The latter point in particular has interesting implications on the experiments we present here. While we asked participants to rate the speaker's affect given her utterance, it may be the case that the speaker is only \emph{pretending} to  
%\todo{JTK: Say something about how terrible may involve pretense?} 
%, it does not account for the intuitions behind echoic mention or pretense theories. 
%We hope to enrich our model's understanding of the social aspects of irony by addressing these intuitions in future research. 


%In addition, we aim to further examine how people identify the particular dimensions of meaning that may be under discussion in a given context. For example, affective dimensions such as valence and arousal may be particularly relevant in domains that involve evaluation (e.g. ``good'' or ``terrible'' weather), while non-affective dimensions may be more salient in other domains \cite{kao2014formalizing}.
%\todo[inline]{JTK: say something about non-scalar irony?}

%\todo{JTK: Is this paragraph necessary? Should we instead talk more about future research regarding common ground manipulations and other kinds of affect?}
%Beyond shedding light on the communicative principles underlying irony understanding, our work also has interesting connections to natural language processing. Many researchers aim to automatically detect sarcasm in order to recover the correct sentiment from large bodies of text. %(e.g. ``I was overjoyed to pay $\$30$ for an overcooked steak'') 
%\cite{davidov2010semi, filatova2012irony}. %
%By integrating background knowledge and linguistic meaning in a principled manner, our model provides a deeper understanding of context and common ground that most NLP approaches currently lack \cite{gonzalez2011identifying, wallacehumans}.


%Overall, our experimental paradigm and modeling framework provide a detailed and precise account of irony understanding. Given the prevalence of irony in everyday language and the social functions it serves, we believe it would be \emph{amazing} to understand how people interpret utterances that convey the opposite of what they ostensibly mean ($\#$notsarcastic). 


%\todo[inline]{first review discourse goals; then list and characteristics of metaphor} 
%\todo[inline]{hyperbolic/strikingness (compare with literal utterance); aptness; efficiency}
%\todo[inline]{one study/model for studying each characteristics}
%\todo[inline]{Basic: how the model gets basic metaphorical/figurative interpretation of scalar metaphors}
%\todo[inline]{Compared to literal alternatives, metaphors are more striking/extreme}
%\todo[inline]{Compared to other metaphorical alternatives, some metaphors are better/more apt}
%\todo[inline]{Metaphors communicate more efficiently. Look at feature metaphors}

%Metaphors are utterances that implicitly compare ideas or concepts from different domains \cite{gibbs1999figurative, roberts1994people}. 
%For example, ``Juliet is the sun'' expresses Juliet's beauty; ``My lawyer is a shark'' communicates the lawyer's ruthlessness; and ``Art washes away from the soul the dust of everyday life'' allows Picasso to compare ``art'' to a cleansing fluid and ``the soul'' to a physical object that collects dust, which gracefully accomplishes two poetic metaphors at once. One can find traces of metaphoricity even in mundane utterances such as ``I waited for a long time,'' where the spatial term ``long'' is used to describe the abstract domain of time \cite{lakoff1993contemporary}. Due in part to its ubiquity and in part to the possibility that metaphor is intimately tied to our ability to create mappings between concrete experiences and abstract concepts \cite{lakoff2008metaphors}, metaphor is by far the most widely studied trope in cognitive science and related fields \cite{gibbs2012interpreting}.
%It has inspired a particularly impressive amount of research in cognitive science, spanning topics such as how metaphors structure and shape our thoughts \cite{ortony1993metaphor, lakoff1993contemporary, thibodeau2011metaphors}, whether metaphor processing recruits the same strategies as standard language processing \cite{giora1997understanding, gibbs2002new, glucksberg1993metaphors} and what factors determine people's interpretation a novel metaphor \cite{gentner1997alignment, blasko1993effects, tourangeau1981aptness, kintsch2002metaphor}. This overwhelming interest in metaphor research is due to both the ubiquity of metaphor in everyday language and the potential role of metaphor for helping us understand how the mind creates meaning.
%
%Why do people choose to use metaphors to communicate? What are some characteristics of metaphor that contribute to its popularity? \citeA{roberts1994people} examined the discourse goals that people have when they use various figurative tropes. They found that the most common goals for using metaphor were to clarify ($82\%$), to add interest ($71\%$), to compare similarities ($35\%$), to provoke thought ($35\%$), and to be eloquent ($35\%$). Interestingly, although metaphors are defined as implicit comparisons, ``to compare similarities'' is less frequently listed as a goal than ``to clarify'' and ``to add interest.'' This suggests that beyond examining the cognitive processes for comparing and aligning concepts that may be involved in metaphor understanding, it is also important to consider other higher-level communicative functions that metaphors may serve.
%
%\subsubsection{Subjective attitude}
%Although emotion goals were not listed among the popular discourse goals for metaphor in \cite{roberts1994people}, other researchers have suggested that metaphors are often used to express subjective attitudes towards the subject \cite{ortony1979beyond}. \citeA{riloff2005exploiting} noted that subjective sentences frequently contain figures of speech such as metaphor and hyperbole. Since metaphor describes an ``experienced'' reality rather than actual reality \cite{ortony1975metaphors}, it is perhaps inherently subjective.  By using metaphorical language, speakers may be signaling to the listener that the information conveyed is a product of the speaker's subjective evaluation and not the objective truth.

%\subsubsection{Efficiency}
%In addition to \cite{roberts1994people}'s exploration of discourse goals, other researchers have suggested that metaphorical utterances can be used to efficiently express complex meanings  \cite{ortony1975metaphors, boerger2005variations,glucksberg1989metaphors}. In communication tasks where pairs of participants are separated by a screen and asked to refer to abstract geometrical objects, participants often prefer to describe objects analogically in terms of other known objects rather than use literal analytical descriptions \cite{clark1986referring, glucksberg1989metaphors}. \citeA{fussell1989effects} found that these analogical and figurative descriptions tend to be shorter than literal descriptions. In addition, they found that figurative descriptions are used significantly more often when the intended audience is one's self, where presumably there is a great deal of common ground, than when the intended audience is a different person. These findings suggest that people may be balancing efficiency and clarity when choosing figurative versus literal descriptions. \cite{glucksberg1990understanding} wrote, ``Metaphors are  used  to  communicate a complex, patterned  set of properties in a shorthand that is understood  by the  members of a speech community  who share relevant mutual  knowledge'' (pp. 16). As a result, a potential benefit of speaking in metaphor may be to utilize common ground to communicate both clearly and efficiently. 
%%
%
%In my dissertation, I plan to examine the following (hypothesized) characteristics of metaphorical utterances: (1) metaphors lead to more striking, or extreme, interpretations than literal descriptions (2) metaphors can communicate information along various dimensions with a minimal number of words (3) the interpretation of a metaphor is highly sensitive to the local context (4) the aptness of a metaphor is related to whether there are alternative utterances that can communicate the same amount of information more efficiently.
%To examine these hypotheses, I will test the qRSA model on different types of metaphorical interpretations.
%
%\subsection{Model sketch}
%In our formalization, a listener assumes that the speaker chooses an utterance to maximize informativeness about a subject along dimensions that are relevant to the QUD and consistent with the speaker's communicative goal. 
%\todo[inline]{Todo: Better segue into metaphor that highlights the differences}
%%\todo[inline]{Contributions of metaphor model: (1) Elicit features/QUDs (2) Manipulate QUD through context and show it changes interpretation (3) Show that metaphors communicate more information along different dimensions than literal statements (4) Show that alternative metaphors affect interpretation}
%\todo[inline]{Todo: Run interpretation experiment with free-response features to show that features relevant for interpretation are pretty consistent with features of the source domain (at least for the metaphors we're looking at)}

\subsection{Metaphor}
What kinds of encyclopedic knowledge and QUDs are involved in metaphor interpretation, and how might we formalize them in a qRSA model?
Suppose Ann hears Bob say the utterance, ``Cam is a shark''\footnote{While metaphoricity can arise from sentences with various types of syntactic forms, to reasonably limit the scope of our work, here we focus on nominal metaphors of the classic form ``$X$ is a $Y$.''}.
Intuitively, Ann interprets the utterance to mean that Cam, the metaphorical target, is a person who shares certain features with a shark, the source domain. To arrive at the appropriate interpretation, Ann needs encyclopedic knowledge about features that sharks are likely to have (e.g. fierceness, fins), as well as features that people are likely to have (e.g. fierceness, arms and legs). She also needs to know that the topic of conversation is not which species Cam belongs to, but his characteristics.
%\todo{incomplete}
%It appears that what Bob is communicating 
%How should a listener interpret this utterance?
By incorporating encyclopedic knowledge about the source and target domains and formalizing the idea that speakers may want to communicate information about specific features of the target, we show that the same qRSA model now produces appropriate interpretations of metaphors.

%Following the qRSA framework, a listener assumes that the speaker chooses an utterance that communicates information  relevant to the QUD. By  
%%Unlike hyperbole and irony, however, these dimensions are not affective in nature. Rather, they are features associated with the metaphorical source, in this case ``shark.''
%In particular, we will focus on three aspects of metaphor understanding that the qRSA model naturally captures. First, interpretation of the same metaphor differs systematically given different discourse contexts, which can be modeled as different prior probabilities over QUDs. Second, metaphors are able to communicate information efficiently along several dimensions and address multiple QUDs at once, which may serve as an advantage over literal statements. 
%Finally, metaphors often communicate more extreme states of the world. \todo{fix this final point}

%specific interpretations of a metaphor are sensitive to prior beliefs, 
%Finally, the specific interpretations of a metaphor are sensitive to the alternative utterances that a speaker could have chosen to address the QUDs under consideration (Experiment 3). 
%show that the RSA model extended to reason about affect and the question under discussion produces non-literal, figurative interpretations that closely match humans'. However, there we assumed that the sets of QUDs under consideration included the speaker's affect, an assumption supported by previous research on the rhetoric effect of hyperbole and verbal irony. In the next section, we will 
%In particular, the model incorporated literal semantics, background knowledge, and assumptions about informativeness in a 


\subsubsection{Model}
To illustrate this idea more concretely and demonstrate how it is implemented in our model, we will continue using the metaphor ``Cam is a shark" as a working example. 
%For simplicity, in this model we restrict the number of possible categories to which a member may belong to $c_a$ and $c_p$, denoting an animal category or a person category, respectively. We also restrict the possible features of John under consideration to a vector of size three: $\vec f = [f_1, f_2, f_3]$, where $f_i$ is either $0$ or $1$ (for example, the three features could be scary, sleek, and finned).\footnote{In principle the model can be extended to accommodate more categories and features.}
We again introduce a literal listener $L_0$, who interprets the utterance as meaning that Cam is literally a member of the category ``shark'' and has corresponding features. 
%Since $L_0$ believes Cam is a shark, she also believes that Cam is likely to have features associated with sharks, for example, being scary or fierce.
The following equation represents the literal listener's interpretation, where $c$ is Cam's category (either \textsc{person} or \textsc{shark}), and $\vec f$ is a vector representation of Cam's features. $P(\vec f | c)$ is thus the prior probability that a member of category $c$ has feature vector $\vec f$.
%The literal listener $L_0$ will interpret the utterance ``Cam is a shark" as meaning that Cam is literally a member of the category ``shark" and has corresponding features. Formally, if $u$ is the uttered category:
%
\begin{equation}
L_0(c, \vec {f} | u) = \left\{ 
  \begin{array}{l l}
    P(\vec {f} | c) & \quad \text{if $c$ = $\llbracket u\rrbracket$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
%
%and conduct behavioral experiments to evaluate the model's performance. We show that peoples' interpretations of metaphors are driven by conversational context and that our model captures this effect. Finally, we show that our model predictions correlate significantly with people's fine-grained interpretations of metaphorical utterances.
%
%
We assume that the speaker's goal is to communicate the values of Cam's features. A QUD is thus a projection from the full feature space to the subset of interest to the speaker. Formally, we represent each QUD as a function $Q_i$, such that $Q_i(\vec f) = f_i$. 
%The QUD may be Cam's species category, or Cam's feature(s). We define the speaker's utility as the negative surprisal of the true state under the listener's distribution, projected along the QUD dimension. 
This leads to the following utility function for speaker $S_1$:
\begin{equation}
U(u | Q, c, \vec f) = \log \sum_{\vec f'} \delta_{Q(\vec f)=Q( \vec f')} L_0(c', \vec f' |u)
\end{equation}
Given this utility function, the speaker chooses an utterance according to a softmax decision rule:
\begin{equation}
S_1(u | Q, c, \vec f) \propto e^{\lambda U(u | Q, c, \vec f)},
\end{equation}
%
Imagine that $S_1$ has the goal to convey $f_1$, scariness, about Cam.
Based on $S_1$'s understanding of $L_0$'s prior knowledge, she knows that if she produces the utterance ``Cam is a shark," $L_0$ will believe that Cam is literally a shark and hence very likely to be \emph{scary}. Since $S_1$'s goal is satisfied if the listener believes that Cam is scary, $S_1$ is motivated to produce such a metaphorical utterance. A pragmatic listener, however, should be able to leverage this pattern to infer that Cam is scary without inferring that Cam is actually a shark.

The pragmatic listener $L_1$ uses Bayesian inference to guess the intended meaning given prior knowledge and his internal model of the speaker:
%To determine the speaker's intended meaning, $L_1$ marginalizes over the possible speaker goals under consideration:
\begin{equation}
L_1 (c, \vec f | u) \propto P(c) P(\vec f | c) \sum_{Q}{P (Q) S_1 (u|Q, c, \vec f)}
\end{equation}
%
If $L_1$ believes it is \emph{a priori} very unlikely that Cam is actually a shark and that $S_1$ may want to communicate about Cam's scariness, she will end up with a posterior distribution where Cam is very likely to be a person who is scary. By combining prior knowledge with reasoning about the QUD, the pragmatics listener can thus arrive at a figurative interpretation of ``Cam is a shark''---Cam is a scary person.

To explore the model's behavior, we make a number of simplifying assumptions. First, we restricted the number of possible categories to which a member may belong to $c_a$ and $c_p$, denoting an animal category (in this case \textsc{shark}) or a person category, respectively. We also restricted the possible features of Cam under consideration to a vector of size three: $\vec f = [f_1, f_2, f_3]$, where $f_i$ is either $0$ or $1$. Finally, we assumed a small and rather impoverished set of alternative utterances that the speaker could have said: the utterance she did say (e.g. ``Cam is a shark''), and a grammatically similar and literally true utterance (e.g. ``Cam is a person.'')%(for example, the three features could be scary, sleek, and finned)
.%\footnote{In principle, the model can be extended to accommodate more categories, features, and alternative utterances.}
% In Experiment 3, in particular, we explore the model's behavior given more animal categories and alternatives.}

%Imagine that $S_1$ had the goal to convey $f_1$, scariness, about John.
%Based on $S_1$'s understanding of $L_0$'s prior knowledge, she knows that if she produces the utterance ``John is a shark,'' $L_0$ will believe that John is literally a shark and hence very likely to be \emph{scary}. Since $S_1$'s goal is satisfied if the listener believes that John is scary, $S_1$ is motivated to produce such a metaphorical utterance. A pragmatic listener, however, should be able to leverage this pattern to infer that John is scary without inferring that John is actually a shark.
%
%%A pragmatic listener $L_n$ now reasons about such a speaker. Based on prior knowledge, $L_n$ knows that John is extremely unlikely to be literally a member of the shark category. On the other hand, $L_n$ knows that the speaker $S_n$ is fairly likely to want to communicate about John's scariness. $L_n$ also knows that $S_n$ knows that \emph{scary} is a high-probability feature of sharks. 
%
%The listener $L_1$ performs Bayesian inference to guess the intended meaning given prior knowledge and his internal model of the speaker. To determine the speaker's intended meaning, $L_n$ will marginalize over the possible speaker goals under consideration:
%$$
%L_1 (c, \vec f | u) \propto P(c) P(\vec f | c) \sum_{g}{P (g) S_1 (u|g, \vec f)}
%$$
%While speaker and listener could continue to reason about each other recursively, resulting in $L_n$, we restrict ourselves to $L_1$ for present purposes. Past work has shown that this first level of pragmatic reasoning is often a good model of human comprehension.
%%\footnote{The current model does not robustly predict metaphorical interpretations at recursion depths greater than 1. Future work will investigate which features of the model lead to this prediction, and whether this remains true under alternative model definitions.}
%If listener $L_1$ thinks it is likely that speaker $S_1$'s goal is to convey scariness but believes it is \emph{a priori} very unlikely that John is actually a shark, she will determine that $S_1$ is using shark metaphorically---that John is a scary person.

% Table 1
\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\begin{table}
\centering
\tabcolsep=0.15cm
%\fontsize{11}{9}\selectfont
\begin{tabular}{|a |c c c | c c c|}\hline
%\toprule
\textbf{Animal} & $f_1=1$ & $f_2=1$ & $f_3=1$ & $f_1=0$ & $f_2=0$ & $f_3=0$ \\\hline
ANT & small & strong & busy & large & weak & idle \\
BAT & scary & blind & nocturnal & unalarming & sighted & diurnal \\
BEAR & scary & big & fierce & unalarming & small & nonviolent  \\
BEE & busy & small & angry & idle & large & unangry \\
BIRD & free & graceful & small & unfree & awkward & large \\
BUFFALO & big & strong & wild & small & weak & tame \\
CAT & independent & lazy & soft & dependent & fast & hard \\
COW & fat & dumb & lazy & thin & smart & fast \\
DOG & loyal & friendly & happy & disloyal & unfriendly & unhappy \\
DOLPHIN & smart & friendly & playful & stupid & unfriendly & unplayful\\
DUCK & loud & cute & quacking & quiet & unattractive & non-quacking \\
ELEPHANT & huge & smart & heavy & small & stupid & light \\
FISH & scaly & wet & smelly & smooth & dry & fragrant \\
FOX & sly & smart & pretty & artless & stupid & ugly \\
FROG & slimy & noisy & jumpy & nonslippery & quiet & relaxed \\
GOAT & funny & hungry & loud & humorless & full & quiet \\
%%%%
GOOSE & loud & mean & annoying & quiet & nice & agreeable \\
HORSE & fast & strong & beautiful & slow & weak & ugly \\
KANGAROO & jumpy & bouncy & cute & relaxed & inelastic & unattractive \\
LION & ferocious & scary & strong & nonviolent & unalarming & weak \\
MONKEY & funny & smart & playful & humorless & stupid & unplayful \\
OWL & wise & quiet & nocturnal & foolish & loud & diurnal \\
OX & strong & big & slow & weak & small & fast\\
PENGUIN & cold & cute & funny & hot & unattractive & humorless \\
PIG & dirty & fat & smelly & clean & thin & fragrant \\
RABBIT & fast & furry & cute & slow & hairless & unattractive \\
SHARK & scary & dangerous & mean & unalarming & safe & nice \\
SHEEP & wooly & fluffy & dumb & hairless & hard & smart\\
TIGER & striped & fierce & scary & unpatterned & nonviolent & unalarming  \\
WHALE & large & graceful & majestic & small & awkward & inferior \\
WOLF & scary & mean & angry & unalarming & nice & unangry \\
ZEBRA & striped & exotic & fast & unpatterned & native & slow\\
\hline

\end{tabular}
\caption{$32$ animal categories, feature adjectives, and their antonyms. Feature adjectives were elicited from Experiment 1a and indicate when a feature is present ($f_i = 1$). Antonyms were generated using WordNet and indicate when a feature is not present ($f_i = 0$). Feature sets shown in Experiment 2b were created with this table, where $\vec f = [1, 0, 0]$ for category ``ant'' is represented by the words $\{$small, weak, idle$\}$. There are $2^3 = 8$ possible feature combinations for each animal category.}
\label{features}
\end{table}
% End of Table 1

Based on this formulation, the listener needs to consider the following prior probabilities to arrive at an interpretation: 
\begin{itemize}
\item[(1)] $P(c)$: the prior probability that the entity discussed belongings to category $c$. We assume that the listener is extremely confident that the person under discussion (e.g. Cam) is a person, but that there is a non-zero probability that Cam is actually a non-human animal. We fit $P(c_a)$ to data with the assumption that $10^{-4} \leq P(c_a) \leq 10^{-1}$.
\item[(2)] $P(\vec f | c)$: the prior probability that a member of category $c$ has feature values $\vec f$. This is empirically estimated in Experiment 2a.
\item[(3)] $P(Q)$: the probability of a question under discussion $Q$. This prior can change based on the conversational context that a question sets up. For example, if the speaker is responding to a vague question about Cam, e.g. ``What is Cam like?'', the prior over goals is uniform. If the question targets a specific features, such as ``Is Cam scary?'', then the speaker is much more likely to have the goal of communicating John's scariness. However, she may still want to communicate other features about Cam that were not asked about. We assume that when the question is specific, the prior probability that $S_1$'s goal is to answer the specific question is greater than $0.5$, fitting the value to data below.
\end{itemize}

To evaluate our model's interpretation of metaphorical utterances, we selected a set of $32$ metaphors comparing human males to various non-human animals. We conducted Experiment 2a and 2b to elicit feature probabilities for the categories of interest. We then conducted Experiment 2c to measure people's interpretations of the set of metaphors. 
\subsubsection{Experiment 2a: Feature Elicitation}
We selected $32$ common non-human animal categories from an online resource for learning English (\url{www.englishclub.com}). The full list is shown in Table~\ref{features}.
$100$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant read $32$ animal category names presented in random order, e.g. ``whale'', ``ant'', ``sheep''. For each animal category, participants were asked to type the first adjective that came to mind in a text box. 
Using participants' responses, we constructed a list of adjectives for each animal category and ordered them by the number of times they were given by a different participant (i.e. their popularity). We removed all color adjectives, such as ``white'' and ``black,'' to eliminate the possibility of interpreting these adjectives as racial descriptions. To avoid redundancy in the feature set, we used WordNet \cite{miller1995wordnet} to identify synonymous adjectives and only kept the most popular adjective among a set of synonyms. We then took the three most popular adjectives for each animal category and used them as the set of features. In what follows, $f_1$ is the most popular adjective, $f_2$ the second, and $f_3$ the third. Table~\ref{features} shows the animal categories and their respective features.

\subsubsection{Experiment 2b: Feature Prior Elicitation}
Using the features collected from Experiment 1a, we elicit the prior probability of a feature vector given an animal or person category (i.e. $P(\vec f | c)$). We assume that the adjective corresponding to a feature (e.g. \emph{scary}) indicates that the value of that feature is $1$ (present), while the adjective's antonym indicates that the value of that feature is $0$ (not present). We used WordNet to construct antonyms for each of the adjective features produced in Experiment 1a. When multiple antonyms existed or when no antonym could be found on WordNet, the first author used her judgment to choose the appropriate antonym. Table~\ref{features} shows the resulting list of antonyms. For each animal category, eight possible feature combinations were constructed from the three features and their antonyms. For example, the possible feature combinations for a member of the category ``ant'' are \{small, strong, busy\}, \{small, strong, idle\}, \{small, weak, busy\}, and so on.

$60$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $16$ trials in random order. Each trial consisted of the eight feature combinations for a particular animal category. Using slider bars with ends marked by ``Impossible'' and ``Absolutely certain,'' participants were asked to rate how likely it is for a member of the animal category to have each of the eight feature combinations. Participants also rated the probabilities of the feature combinations for a male person. We only elicited priors for males to minimize gender variation and to maintain consistency with Experiment 2c.

We normalized each participant's ratings for the eight feature combinations in a trial to sum up to 1, based on the assumption that the feature combinations exhaustively describe a member of a particular category. Using the Spearman-Brown prediction equation, reliability of the ratings was  $0.941$ ($95\%$ CI $=[0.9408, 0.9414] $). Averaging across participants' normalized ratings, we obtained feature priors $P(\vec f | c)$ for $c = c_a$ (animal) and $c = c_p$ (person).
%, where $f_i = 1$ is represented by the feature adjective and $f_i = 0$ is represented by its antonym. 
%Figure~\ref{prior} shows the average marginal probabilities of features being present given an animal category and a person category. 
Since the features were created using the animal categories in Experiment 1a, by construction features are rated as significantly more likely to be present in the animal category than in the person category ($F(1, 190) = 207.1$, $p < 0.0001$). These results confirm that participants are fairly confident that each animal category has certain distinguishing features (mean$= 0.61$, sd$=0.06$), while those same features are rated as appearing in people less often (mean$=0.48$, sd$=0.06$).

%\begin{figure}[t]
%\begin{center}
%\scalebox{0.6}{\includegraphics{Figures/priors_bar.pdf}}
%\end{center}
%\caption{Average marginal prior probability ratings for the three features given an animal category or a person category. Error bars are standard error over the $32$ items.} 
%\label{prior}
%\end{figure}

\subsubsection{Experiment 2c: Metaphor Interpretation}
We created $32$ scenarios based on the animal categories and results from Experiment 2a. In each scenario, a person (e.g. Bob) is having a conversation with his friend about a person that he recently met. Since we are interested in how contextually determined QUDs affect metaphor interpretation as well as the meanings of metaphorical versus literal utterances, we created four conditions for each scenario by crossing vague/specific QUDs and literal/metaphorical utterances. In vague QUD conditions, Bob's friend asks a vague question about the person Bob recently met: ``What is he like?'' In specific QUD conditions, Bob's friend targets $f_1$ and asks a specific question about the person: ``Is he $f_1$?,'' where $f_1$ is the most popular adjective for a given animal category $c_a$. In literal conditions, Bob replies with a literal utterance, either by saying ``He is $f_1$.'' to the question ``What is he like?'' or ``Yes.'' to the question ``Is he $f_1$?''. In Metaphorical conditions, Bob replies with a metaphorical statement, e.g. ``He is a $c_a$.'' where $c_a$ is an animal category. See Table~\ref{example-questions} for examples of each condition.

\begin{figure}[t]
\centering
\scalebox{0.65}{\includegraphics{Figures/metaphor-interp-prior.pdf}}
\caption{The leftmost panel shows participants' average feature probability ratings given a literal utterance (Experiment 2c), subtracted by the average prior probabilities of a person having each of the features \emph{a priori} (Experiment 2b); the middle panel shows participants' average feature probability ratings given a metaphorical utterance, subtracted by the feature prior; and the rightmost panel shows the model's posterior probabilities for each of the features given a metaphorical utterance, again subtracted by the feature prior. Error bars are $95\%$ confidence intervals. %The model facet shows model posterior probabilities for each of the features given a metaphorical utterance and a vague/specific QUD, also subtracted by the average prior probabilities of a person having each of the features. 
Compared to literal utterances, participants' interpretation of metaphorical utterances shows two qualitative effects: (1) feature probabilities are all significantly higher than the prior, meaning metaphors communicate information abut multiple features at once, while literal utterances only communicate information about Feature 1 (2) interpretation is sensitive to the QUD, such that specific QUDs boost the amount of information communicated about a specific feaature. The model captures both qualitative effects.} 
\label{metaphor-human-summary}
\end{figure}

\begin{table}
\tabcolsep=0.2cm
\centering
\begin{tabular}{|llll|}\hline
QUD & Utterance & Example question & Example utterance \\\hline
General & Literal & ``What is he like?'' & ``He is scary.'' \\
Specific  & Literal & ``Is he scary?'' & ``Yes.'' \\
General & Metaphorical & ``What is he like?'' & ``He is a shark.'' \\
Specific & Metaphorical & ``Is he scary?'' & ``He is a shark.'' \\\hline
\end{tabular}
\caption{Example questions and utterances for each of the four experimental conditions in Experiment 2c.}
\label{example-questions}
\end{table}

$49$ native English speakers with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $32$ trials in random order. The $32$ trials were randomly and evenly assigned to one of the four conditions, i.e. each participant read $8$ scenarios for each condition. For each trial, participants used sliders to indicate the probabilities that the person described has features $f_1$, $f_2$, and $f_3$, respectively.

%\begin{figure}[ht]
%\begin{center}f
%\scalebox{0.6}{\includegraphics{Figures/model_literal.pdf}}
%\end{center}
%\caption{Model's average marginal category probabilities given a metaphorical utterance ($P(c | u)$). Model's prediction for the animal category is close to $0$ and close to $1$ for the person category. Error bars are standard error over the $32$ items.} 
%\label{scatter_full}
%\end{figure}

For each condition of each scenario, we obtained the average probability ratings for the three features. The left-most and middle panels in Figure~\ref{metaphor-human-summary} show the average ratings for each feature across animal categories given a literal or metaphorical utterance and a vague or specific QUD, subtracted by the prior feature probabilities obtained in Experiment 2b. When the speaker gives a literal statement directly affirming the presence of $f_1$, participants rate $f_1$ as being significantly more likely than when the speaker gives a metaphorical statement ($F(1, 126) = 52.6$, $p < 0.00001$). However, participants rate $f_2$ and $f_3$ as significantly more likely when the speaker produces a metaphorical utterance than when the utterance is literal ($F(1, 126) = 23.7$, $p < 0.0001$; $F(1, 126) =13.66$, $p < 0.0005$). Comparing feature probability ratings in Experiment 2c to the feature priors obtained in Experiment 2b, we can measure how literal and metaphorical utterances change listeners' inferences about a person's features. Given a literal utterance that directly confirms the existence of $f_1$, probability ratings for $f_1$ are significantly higher than the prior probabilities of $f_1$ for a person ($t(63) = 59.19, p < 0.00001$). However, probability ratings for $f_2$ and $f_3$ are not significantly different from their prior probabilities ($t(63) = -0.13, p = 0.89$; $t(63) = 0.03, p = 0.97$). Given a metaphorical utterance, probability ratings for all three features are significantly higher than the prior probabilities ($t(63) = 15.74, p < 0.0001$; $t(63) = 7.29, p < 0.0001$; $t(63) = 5.91, p < 0.0001$). This analysis suggests that metaphorical utterances may convey richer information and update listeners' beliefs along more dimensions than literal utterances.

We now analyze the effect of contextually determined QUDs on the interpretation of literal or metaphorical utterances. When the speaker's utterance is literal, the probability ratings for $f_1$, $f_2$, and $f_3$ are not significantly different given a vague or a specific question ($(F(1, 62)= 2.73, p =0.1; F(1, 62)=0.0001, p = 0.99; F(1, 62) < 0.0001, p = 0.99$). For metaphorical utterances, however, the question type has an effect on participants' interpretations: participants rate the probability of $f_1$ as significantly higher when the question is specifically about $f_1$ than when it is vague ($F(1, 62) = 10.16$, $p < 0.005$). The probabilities of $f_2$ and $f_3$ are not significantly different given a vague question or a specific question about $f_1$ ($F(1, 62) = 0.04$, $p > 0.05$; $F(1, 62) = 0.8285$, $p > 0.05$). This suggests that people's interpretation of metaphor may be more sensitive to the QUDs set up by context than their interpretation of literal utterances. The left-most and middle panels in Figure~\ref{metaphor-human-summary} show these effects.

\subsubsection{Metaphor model evaluation}
We used the feature priors obtained in Experiment 2b to compute model interpretations of the $32$ metaphors. As discussed in the previous section, the behavioral results in Experiment 2c show evidence that the context set up by a question changes participants' interpretation of a metaphor. Our model naturally accounts for this using the speaker's prior over QUDs $P(Q)$. When a speaker is responding to a vague question, we set the prior distribution for $P(Q)$ as uniform. When the speaker is responding to a question specifically about $f_1$, we assume that $P(Q_1) > 0.5$ and equal between $P(Q_2) = P(Q_3)$. Fitting the goal prior parameter to data yields a prior of $P(Q_1) = 0.9$ when responding to a specific question about $f_1$. We fit the category prior ($P(c_a) = 0.01$) and the speaker optimality parameter ($\lambda = 4$). %note that saying ''g_1'' is a slight abuse of notation given the model definitions above. 

Using these parameters, we obtained interpretation probabilities for each of the $32$ metaphors under both vague and specific QUD conditions. For each metaphor and QUD condition, the model produces a joint posterior distribution $P(c, \vec f | u)$. We first show a basic but important qualitative result, which is that the model is able to interpret utterances metaphorically. Marginalizing over values of $\vec f$, the probability of the person category given the utterance is close to one ($P(c_p | u) = 0.994$), indicating that the pragmatic listener successfully infers that the person described as an animal is actually a person and not an animal. This shows that the model is able to combine prior knowledge and reason about the QUD to arrive at metaphorical interpretations.

%\begin{figure}[t]
%\begin{center}
%\scalebox{0.6}{\includegraphics{Figures/model_bar.pdf}}
%\end{center}
%\caption{Model's average } 
%\label{scatter_full}
%\end{figure}

\begin{figure}
\centering
\scalebox{0.54}{\includegraphics{Figures/metaphor-human-model-scatter-withWords.pdf}}
\caption{Model predictions ($x$ axis) vs participants' probability ratings ($y$ axis) for $192$ items ($32$ metaphors $\times$ $3$ features $\times$ $2$ QUD conditions). Color indicates QUD condition and shape of points indicates feature number. The labeled data points are the $10$ data points with the highest residual values given a linear regression model with the qRSA model results as predictor.} 
\label{scatter_full}
\end{figure}

We now turn to the second component of the interpretation, $P(\vec f | u)$. 
%Figure X shows the average marginal feature probabilities given a vague or specific goal. The model's interpretations are shaped by the communicative goal set up by context, where $f_1$ receives significantly higher marginal probabilities when the speaker is responding to a specific question about $f_1$ ($F(1, 62) =17.92$, $p<0.0001$). This qualitatively matches behavioral results in Figure X. 
To quantitatively evaluate the model's performance, we correlated model predictions with human interpretations of the metaphorical utterances. Given a metaphorical utterance and a vague or specific QUD condition, we computed the model's marginal posterior probabilities for $f_1$, $f_2$, and $f_3$. We then correlate these posterior probabilities with participants' probability ratings from Experiment 2c. Figure~\ref{scatter_full} plots model interpretations for all metaphors, features, and QUD conditions against human judgments. Correlation across the $192$ items ($32$ metaphors $\times$ $3$ features $\times$ $2$ QUD conditions) is $0.64$ ($p < 0.001$)\footnote{We also fit a free parameter ($\alpha=3$) that determines a power-law transformation of the feature priors, where the probability of each feature set was raised to the power of $\alpha$ and renormalized to sum up to $1$. The power-law transformation was introduced based on analyses of the feature priors showing that participants tend to overestimate unlikely features and underestimate likely features, likely due to a tendency to avoid the extreme ends of the slider bars. Without performing the power-law transformation and using the raw normalized feature priors obtained in Experiment 2a, the model correlation is $0.6$ ($p < 0.001$)}. The predicted reliability of participants' ratings using the Spearman-Brown prediction formula is $0.828$ ($95\%$ CI $=[0.827, 0.829]$), suggesting first that people do not agree perfectly on metaphorical interpretations, and second that our model captures a significant amount of the reliable variance in the behavioral data. In particular, our model does especially well on predicting participants' judgments of $f_1$, which are the most salient features of the animal categories and were targeted by specific questions in Experiment 2c. Correlation between model predictions and human judgments for $f_1$ is $0.77$ ($p < 0.0001$), while the predicted reliability of participants' ratings for $f_1$ is $0.82$ ($95\%$ CI $=[0.818, 0.823]$). 

We now compare our model's performance to a baseline model that also considers the feature priors and the conversational context. We constructed a linear regression model of participants' feature ratings that takes as predictors the marginal feature priors for the animal category, the marginal feature priors for the person category, whether the QUD is vague or specific, and their interactions. With eight parameters, this model produced a fit of $r= 0.48$, which is significantly worse than our model ($p < 0.0001$ on a Cox test). This suggests that the qRSA model adequately combines people's encyclopedic knowledge as well as principles of pragmatics to produce metaphorical interpretations that match behavioral data.


\subsubsection{Discussion}
In this section, we showed that metaphorical interpretations can be captured by the general communicative principles formalized in the qRSA model. In particular, our model captures several important aspects of metaphor interpretation. First, the model goes beyond the literal meanings of utterances to infer non-literal interpretations (e.g., Cam is a person and not a shark). Second, the model provides quantitative judgments about the person's features based on fine-grained encyclopedic knowledge of the metaphorical source and target (e.g. sharks are very likely scary; people are very unlikely to be striped). Finally, the model produces different interpretations given different local conversational contexts (e.g. is the topic of conversation specifically about Cam's scariness, or his characteristics more generally?) in a manner that matches human responses. 
%Third, the model successfully captures the empirical finding that metaphors tend to communicate information along more dimensions than literal statements. 
%Finally, a model that takes into account the alternative utterances (both literal and metaphorical) that a speaker could have produced to address specific QUDs predicts people's interpretations with higher quantitative accuracy than one that does not.
 %Furthermore, behavioral results show that the interpretation of a metaphor is shaped in part by the conversational context, which our model naturally accounts for using communicative goals. 
 
Although our model predictions provide a reasonable fit to behavioral data and outperforms a linear regression model using fewer parameters, we observed residual variance that can be further addressed. Fig.~\ref{scatter_full} shows the $10$ metaphor-feature pairs that have the highest residual variance in the regression model. Examining these data points in more detail, we can identify a number of potential reasons for these errors.
%
One potential source of error is the fact that most of the features in this dataset are gradable adjectives (e.g. \emph{strong}, \emph{large}, \emph{soft}) whose meanings vary with context and the types of entities they predicate. For example, a strong ant is probably less strong than a strong person, and even less strong than a strong ox. In addition, our judgment of whether an ant is \emph{strong} depends on the class of entities that we compare it to. An ant can be strong compared to other living things of its size, but not strong compared to all living things in general. In our experiments and model, we did not consider the context-sensitivity of these gradable features. In Experiment 2b, participants rated an ant as being strong with probability $0.69$ on average, but rated a person as being strong with probability $0.54$, presumably because participants are judging \emph{strong} with respect to a class of insects in the \emph{ant} case, and with respect to a class of human beings in the \emph{person} case. In the qRSA model, a pragmatic listener using these priors will interpret ``Cam is an ant'' to mean that Cam is more likely than an average person to be strong ($P(\text{strong}) = 0.7$), because an ant in more likely than a person to be strong. However, the intuitive interpretation of the metaphor is that Cam is less strong than an average person, presumably because people have the encyclopedic knowledge that when the comparison class includes human beings, an ant is very unlikely to be considered \emph{strong}. By using context-sensitive gradable adjectives as features without providing the model with correct context-sensitive feature probabilities, we prevent it from producing the appropriate interpretation for metaphors such as ``Cam is an ant.'' 

%Two reasons may contribute to these types of errors. The first is that the priors are not calibrated on an absolute scale (strong for an animal of an ant's size, not strong absolutely). 
%Another reason why the model has difficulty predicting certain data points accurately has to do with the feature probabilities we elicit in Experiment 2b. In Experiment 2b, we asked participants to rate how likely it is that a member of an animal category (e.g. ``ant'') has various combinations of features (e.g. $\{$ small, strong, busy $\}$). Importantly, lack of calibration . For a human being, 

Another related source of error is the set of alternative utterances that the model considers. 
Previous work has shown that alternative utterances---what the speaker could have said---can strongly affect listeners' interpretation of what the speaker \emph{did} say \cite{bergen2012s}. In this experiment, we did not take into account the range of alternative utterances (both literal and metaphorical) that a listener considers when interpreting a speaker's utterance. 
In the model simulations presented here, we only considered two syntactically similar utterances that a speaker could choose to say: e.g. ``Cam is a whale'', and ``Cam is a person.'' Because the prior probability of a whale being \emph{graceful} is higher than the prior probability of a person being \emph{graceful} ($P(\text{graceful} | \text{whale}) = 0.7$, $P(\text{graceful} | \text{person}) = 0.44$), a speaker is more likely to choose ``Cam is a whale'' over ``Cam is a person'' to communicate that Cam is \emph{graceful}. As a result, a pragmatic listener modeled by qRSA would interpret ``Cam is a whale'' to mean that Cam is more likely to be graceful than the average person ($P(\text{graceful}) = 0.6$). However, if the speaker wanted to communicate that Cam is \emph{graceful}, intuitively it seems that a different metaphor would be more effective for addressing the QUD, for example ``Cam is a swan.'' Since \emph{graceful} is a higher probability feature for ``swan'' than for ``whale,'' the listener reasons that if the speaker had intended to communicate the feature \emph{graceful}, he would have said ``He is a swan'' since it optimally satisfies that goal. Since the speaker did \emph{not} produce the utterance ``He is a swan,'' the listener infers that \emph{graceful} is a less probable feature of Cam. 
We posit that allowing the model to reason over a more realistic set of alternative utterances would produce interpretations that more closely match humans'.

%The model assigns high probabilities for features that are strongly associated with the metaphor source (e.g. \emph{ants} being \emph{strong}, and \emph{whales} being \emph{graceful}), but it appears that humans do not infer high probabilities of these features from the animal metaphors (e.g. ``John is an ant'' is not likely interpreted as \emph{John is strong}, and ``John is a whale'' is not likely interpreted as \emph{John is graceful}). (strong, ox; slow, turtle).
%
%Previous work has shown that alternative utterances---what the speaker could have said---can strongly affect listeners' interpretation of what the speaker \emph{did} say \cite{bergen2012s}. In this experiment, we did not take into account the range of alternative utterances (both literal and metaphorical) that a listener considers when interpreting a speaker's utterance. We posit that other plausible alternative utterances may account for some of the variance in the data that our model does not capture. Consider the metaphor ``He is an ant'' and the corresponding features \emph{small, strong}, and \emph{busy}. Our model currently assigns a high probability to the feature \emph{strong} given the metaphor, while participants assign it a lower probability. Indeed, this data point has the highest residual in our model fit. To test the idea that alternative utterances may account for this discrepancy, we construct a model that has ``He is an ox'' as an alternative utterance. ``Ox'' has features that roughly align with the features of ``ant'': \emph{strong, big}, and \emph{slow}. Since \emph{strong} is a higher probability feature for ``ox'' than for ``ant,'' the listener reasons that if the speaker had intended to communicate the feature \emph{strong}, she would have said ``He is an ox'' since it optimally satisfies that goal. Since the speaker did \emph{not} produce the utterance ``He is an ox,'' the listener infers that \emph{strong} is a less probable feature. Adding this alternative utterance to the model indeed lowers the marginal posterior probability of \emph{strong} given the utterance ``He is an ant.'' Based on this simple error analysis, we posit that adding alternative utterances across all animal categories could help improve our model's performance. 

%Conventionality
Beyond the pragmatic effects of alternative utterances and gradable adjectives, another source of error we observe in our data is the issue of conventional or idiomatic metaphors, whose meanings are no longer directly derived from features of the metaphorical source. For example, the metaphorical meaning of ``fox'' is associated with phrases such as ``silver fox,'' which can be used to describe males. Even though the prior probabilities of a man or a fox being \emph{pretty} are not very high ($0.49$ and $0.67$, respectively), the conventionalized metaphorical meaning of ``fox'' carries a stronger connotation of attractiveness, and may result in a higher probability rating for \emph{pretty} given the metaphor.

%Nonliteral features
Finally, we observe that some features may themselves have nonliteral meaning. For example, it is quite unlikely for a person to literally ``quack'' like a duck quacks ($P(\text{quack} | \text{person}) = 0.21$). However, a person could reasonably make noises or have a voice that resembles quacking in its loudness. While humans have the flexibility of interpreting the features metaphorically and assign a higher probability to Cam being \emph{quacking} given the utterance ``Cam is a duck'' ($0.54$), the model is influenced by the low prior probability of a person literally having the feature (as well as the inference that the QUD is likely about Cam's other features) and ends up with a lower posterior feature probability of Cam \emph{quacking} ($0.1$). In addition, in the case of ``Cam is a cat,'' we observe that the feature \emph{soft} means something quite different when applied to a cat than when applied to a person. When applied to a cat, it refers to the physical softness of a cat's fur or body, while when applied to a person it refers to the person's flexibility or gentleness in personality. Since cats are not strongly associated with flexible, gentle personalities, the metaphor ``Cam is a cat'' does not seem to adequately address the QUD of Cam's \emph{softness} (of personality), and may thus feel less appropriate to human participants. These types of metaphors raise an interesting question about how people identify the appropriate features that are beings transferred from the source to the target domain, especially when some features may have different meanings when applied to the source and target domains \cite{wilson2006metaphor}. 
%Taking into account the This raises the interesting question of how nonliteral interpretations of features are derived to begin with. Emergent features?

Despite important open questions regarding the context-sensitivity of features, the role of alternative utterances, and conventional metaphors, this model takes a promising first step towards formalizing principles of pragmatics in order to predict metaphor interpretation. In particular, we show that several key characteristics of metaphor---dependence on prior knowledge, ability to communicate information along several dimensions, and sensitivity to local context---can be explained by modeling speakers as rational and cooperative agents who wish to be informative and relevant with respect to the QUD.
%
%showing that basic principles of communication can shape metaphor interpretation in important ways. 
%which can be formalized in a general computational framework that assumes speakers to be rational and cooperative agents.
In the next sections, we will begin to address some of these remaining issues (specifically, the issue of alternative utterances, feature calibration, and more abstract features) in more detail. %\todo{make sure we're actually doing that}


%%\todo[inline]{Transition to introduce Experiment 3}
\subsection{Alternative Utterances}

In order to appropriately interpret an utterance, listeners often need to reason about a set of alternative utterances that the speaker could have said but did not. 
For example, in order to interpret the utterance ``Some of the apples are red'' as the pragmatically enriched \emph{some but not all} of the apples are red, listeners need to consider the fact that the speaker could have said the more informative utterance ``all of the apples of red'' if it were indeed true. 
%Since the speaker did \emph{not} say ``All of the apples were red,'' it is likely that ``all'' is not.
While the importance of alternative utterances is supported by a great deal of psycholinguistic evidence \cite{bergen2012s, krifka2007approximate, degen2015availability}, how listeners arrive at a reasonable set of alternative utterances is still an open question \cite{fox2011characterization, ciardelli2015alternatives}. For our purposes, we take the approach of constructing alternative utterances based on the set of QUDs under consideration. Since we assume that the QUDs considered for metaphor interpretation are related to the features associated with the metaphorical source, we assume that the alternative utterances a listener considers in order to interpret a metaphor are in turn associated with those features. More concretely, suppose a speaker produced the metaphor, ``Cam is an ant.'' Intuitively, the interpretation of this metaphor (how likely it is that Cam is small, strong, busy, etc.) will be influenced by other animal metaphors the speaker could have produced to communicate information about Cam's size, strength, and industriousness. If there are better metaphors for communicating Cam's strength than the metaphor that the speaker chose, this gives the pragmatic listener reason to believe that the speaker's goal is not to communicate that Cam is strong.

To test whether adding a richer set of alternative utterances to the qRSA model can improve its performance, we conducted the following experiments to elicit alternative animal metaphors as well as features of those animals. 
%We conducted Experiment 3a and 3b to elicit alternative animal metaphors and explore its effect on the 



\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\begin{table}
\centering
\tabcolsep=0.15cm
%\fontsize{11}{9}\selectfont
\begin{tabular}{|a |c c c | c c c|}\hline
%\toprule
\textbf{Animal} & $f_1$ & $f_2$ & $f_3$ & $a_1$ & $a_2$ & $a_3$ \\\hline

ANT   & small & strong & busy  & mouse & ox & bee\\
BUFFALO & big & strong & wild & elephant & ox & tiger\\
CAT    & independent & lazy & soft  & wolf & sloth & rabbit\\
DUCK   & loud & cute & quacking   &   lion & rabbit&dog\\
FOX  &   sly & smart & pretty  &      snake & dog &cat\\
OX  &  strong & big & slow & horse& elephant &turtle\\
SHEEP &  wooly & fluffy & dumb     &  mammoth & dog &cow\\
WHALE &  large&graceful&majestic &elephant & swan&horse\\\hline
\end{tabular}
\caption{Eight original animals, their features, and alternative animals that are strongly associated with those features.}
\label{features-alternatives}
\end{table}


\subsubsection{Experiment 3a: Alternative Elicitation}

Given the large space of animal categories and features selected in Experiment 2, constructing a complete set of alternative utterances would result in a very large and unwieldy set of animals and features. Instead, to test the idea that alternative utterances factor into metaphor interpretation, we focused on a smaller set of initial animal categories, namely the eight animal metaphors that had the highest residual variance in the linear regression model shown in Figure~\ref{scatter_full}. We then elicit the set of alternative metaphors a listener may reasonably expect a speaker to produce. We will refer to the eight original animals as the \emph{original animals}, and the alternative animals for each original metaphor as \emph{alternative animals}. %In the next section, we describe Experiment 3, where we begin with a set of $12$ animal metaphors and elicit alternative metaphors to examine their effect on the model's behavior.

%We selected 12 common animals that have various distinguishing features: \textit{ant, whale, bird, elephant, panda, monkey, penguin, giraffe, cheetah, turtle, lion}, and \textit{rabbit}, which we will call the core animals. 
Each of the eight original animals have three features elicited in Experiment 2a, as shown in Table~\ref{features-alternatives}. For each of the features, we elicit animals that are strongly associated with those features. 
$36$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant saw $21$ unique feature adjectives presented in random order. Following the same paradigm as Experiment 2a, participants were asked to type in a text box the first animal that came to mind that could be described by that adjective. For each original animal and each feature adjective, we identified the most popular animal associated with the feature. We ensured that each original animal has exactly three unique alternative animals by removing duplicates. Table~\ref{features-alternatives} shows the set of alternative animals elicited in this manner. 
%We removed color adjectives and combined synonymous adjectives by consulting WordNet \cite{miller1995wordnet}. We then identified the two most popular adjectives for each animal category and used those as features \footnote{In Experiment 3, we decided to use two features for each category instead of three due in part to the difficulty of obtaining reliable ratings from human participants for $2^3=8$ possible feature combinations, and in part to restrict the set of alternative utterances we elicit in Experiment 3b to a reasonable size.}.
%
%\subsubsection{Experiment 3b: Alternative Elicitation} 




%Experiment 3a yielded $15$ unique features elicited from the $12$ core animals: $2$ features for each core animal, with $9$ overlapping features across the $12$ animals such as ``small,'' ``big,'' and ``strong.'' $50$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk to produce animals associated with each of the $15$ features. Each participant read the $15$ feature adjectives presented in random order, with the prompt: ``Write down the first animal you can think of that is \underline{\hspace{1cm}},'' where \underline{\hspace{1cm}} is a feature adjective. We tallied the number of times an animal was produced for each of the features and identified the animals most strongly associated with each feature.
%
%For each of the $12$ core animals we started with, we now have $2$ associated features, as well as a set of animals associated with those features. For each core animal, we constructed a set of alternative animals by selecting $2$ animals most strongly associated with each feature, excluding the core animal itself. If there were animals that were strongly associated with both of the two features, we selected an additional animal associated with the first feature, in order to ensure that each core animal had exactly $4$ alternative associated animals. The full set of core animals, features, and alternative animals are shown in Table~\ref{alternatives}. 
%
\subsubsection{Experiment 3b: Feature Prior Elicitation}
With the alternative animals collected from Experiment 3a, we now elicit the prior probability of a feature vector given a member of an animal category (i.e $P(\vec{f} | c)$). For each original animal, we elicited the prior probabilities of the eight feature combinations described in Experiment 2b for the original animal, its three alternatives, and a human male.
%For each set of three features (e.g. $\{$small,strong,busy$\}$), we again constructed antonyms to indicate when the feature is absent, resulting in four possible feature combinations (e.g. $\{$small, industrious$\}$, $\{$small, lazy$\}$,  $\{$big, industrious$\}$, and $\{$big, lazy$\}$). For each feature set, we elicit prior feature probabilities for the following categories: the core animal (e.g. \emph{ant});  the alternative animals (e.g. \emph{mouse, dog, beaver}, and \emph{monkey}); and a human male.

$35$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed $10$ trials in random order. Each trial consisted of the eight feature combinations for a particular animal category. Using slider bars with end points marked by ``very unlikely'' and 	``very likely,'' participants were asked to rate how likely it is for a member of the category to have each of the eight feature combinations. Based on the assumption that the feature combinations exhaustively describe a member of a particular category, we normalized each participant's ratings for the four feature combinations within a trial to sum up to 1.

\subsubsection{Model simulation results}
We used the same model and QUD assumptions described in the Metaphor section. For this model simulation, we expanded the set of utterances $U$ that a speaker considers given a particular QUD. Suppose the listener hears the utterance ``Cam is an ant,'' which raise three potential QUDs: $Q_1$ = size, $Q_2$ = strength, $Q_3$ = busyness. We assume that the alternative utterances a speaker considers depends on the QUD. If the QUD is Cam's size $Q_1$, then the alternative utterances the speaker could have chosen are the utterance the speaker \emph{did} say ($u=$ ``Cam is an ant''), a syntactically similar and literally true utterance about the target's species category ($a_0 =$ ``Cam is a person''), and an alternative metaphor where the source is strongly associated with the QUD feature \emph{small} ($a_1 =$ ``Cam is a mouse''). If, on the other hand, the QUD is Cam's strength $Q_2$, then the alternative metaphor $a_1$ becomes ``Cam is an ox,'' because \textsc{ox} is strongly associated with the QUD feature of \emph{strong}.

\begin{figure}
\centering
\scalebox{0.5}{\includegraphics{Figures/metaphor-alternatives.pdf}}
\caption{Probabilities of a feature given an animal metaphor, as predicted by the original model, a model that considers alternative utterances, and participant responses in Experiment 2c.} 
\label{metaphor-alternatives}
\end{figure}

Using the QUD-sensitive alternative utterances and the feature priors $P(\vec f | c)$ obtained in Experiment 3b, we ran the metaphor model and produced new interpretations for the $10$ outlier data points shown in Figure~\ref{scatter_full}. Figure~\ref{metaphor-alternatives} shows the original model predictions, model predictions with a richer set of alternatives, and human ratings. We see that for some examples, a model that considers alternative utterances produces predictions that are closer to human interpretations. However, for other metaphors such as ``duck'' and ``fox,'' adding alternative metaphors does not change the model's predictions, suggesting that the model does not capture those metaphors for reasons other than a lack of alternative utterances.

\subsubsection{Discussion}
In this section, we explored the idea that listeners may consider alternative metaphorical utterances in order to infer whether a novel metaphor was used to address a particular QUD. This idea, while intuitively appealing, was not fully borne out in our model simulations. We believe that this result may in part be due to the conventionalized meanings of certain metaphors, as well as to the difficulty of obtaining calibrated measures for the feature priors. %For example, the marginal probability of an ant having the feature \emph{strong} ($0.70$) is actually numerically higher than the marginal probability of an ox having the feature \emph{strong} ($0.67$). 
In the next section, we aim to better calibrate the feature measures by properly treating the features as gradable adjectives and measuring their values more precisely. 
%\todo{in flux}

 
%
%\subsubsection{Experiment 3d: Metaphor Interpretation}
%$45$ participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk.
%\todo[inline]{Describe results.}
%\subsubsection{Model evaluation}
%\todo[inline]{Model comparison with and without alternative animals.}
%
%In Study 2, we use a series of experiments to show that some metaphorical utterances such as ``Bob is an ox'' allow speakers to communicate about multiple dimensions of Bob at once---for example, that Bob is strong, big, and slow. In order to communicate the same amount of information using a literal description, a speaker would need to choose a much longer and costlier utterance, such as: ``Bob is strong, big, and slow.'' We show that listeners infer information along more dimensions given a metaphorical utterance than given a single literal description. We then show that the qRSA model is able to capture these differences between literal and metaphorical utterances.

%In Study 3, we use a series of experiments to show that the interpretation of metaphorical utterances such as ``Bob is an ox'' is highly sensitive to the local conversational context. For example, if Liz asks: ``What is Bob like?'' and Sam replies: ``Bob is an ox,'' Liz will believe that Bob is perhaps strong, or big, or slow. However, if Liz instead asks: ``Is Bob strong?'' and Sam replies: ``Bob is an ox,'' Liz will be much more likely to believe that Bob is strong. On the other hand, regardless of Liz's initial question, the interpretation of literal utterances such as ``Bob is strong'' is unlikely to vary with the conversational context. We show that this effect can be modeled in qRSA using different priors over QUDs.


\subsection{Hyperbolic Interpretations of Metaphor}

In the previous metaphor experiments and model simulations, we  assumed that a metaphor communicates binary values for each feature of the target domain. For example, because giraffes have the feature \emph{tall}, comparing Cam with a giraffe can communicate that Cam also has the feature \emph{tall}, leading to the interpretation that Cam is a tall person. However, intuitively, listeners do not interpret the utterance as meaning that Cam's height is the same as a giraffe's height. Instead, listeners use their prior knowledge about people's heights and giraffe's heights to infer that the metaphor is \emph{hyperbolic}---Cam is taller than the average person, but Cam is not literally as tall as a giraffe. Because the precise meaning of the feature \emph{tall} is different when applied to giraffes and when applied to people, in order to arrive at appropriate interpretations, listeners need to adjust the meanings of the features based on the target domain. 

In this section, we examine whether the qRSA model can capture hyperbolic interpretations of metaphors using prior knowledge and a more precise representation of gradable adjectives. In order to quantify and calibrate the meanings of gradable features, we focus only on features that are easily translated to an absolute, continuous scale: \emph{height}, \emph{weight}, and \emph{speed}, where we can assign a precise numerical value to each feature. We show that when using metaphors whose source domains have extreme values on these scales (e.g. \textsc{giraffe}, \textsc{cheetah}, and \textsc{elephant}), people arrive at interpretations that are moderated by prior knowledge, but are still more extreme than interpretations licensed by literal utterances (e.g. ``Cam is tall'').
%
%, but it does not mean that Cam's height is equivalent to a giraffe's height. Metaphors often elicit these hyperbolic interpretations. Our goal with the following experiments is to (1) treat features as having continuous values instead of binary (2) calibrate features for different categories using absolute, quantitative scales, e.g. measure inferences about a person's height instead of whether he/she is ``tall'' (3) show that metaphors can receive a hyperbolic interpretation, i.e. ``Cam is a giraffe'' means that Cam is taller than the average person, but Cam is not as tall as an average giraffe.'' (4) Show that metaphors can produce more extreme interpretations than literal utterances
%
%Metaphors often elicit hyperbolic interpretations and result in more extreme beliefs about the world.
%In this section, we examine people's interpretations of metaphors such as ``Cam is a giraffe,'' where the salient feature being communicated (e.g. height) lies on a continuous scale. We find that the interpretation of these types of metaphors demonstrates two effects: First, given the utterance ``Cam is a giraffe,'' listeners believe that Cam is taller than they do when given the literal description ``Cam is tall.'' Second, listeners believe that Cam is taller than the average person, but much shorter than the average giraffe. 
We also show that the qRSA model naturally captures these effects.
\subsubsection{Model}

Unlike the previous metaphor model where we showed how metaphors can communicate information about several features at once, in this model we only consider one salient feature dimension for each animal metaphor (e.g. \emph{height} for giraffes), but assume that the feature can take on continuous values, such as $6$ or $13$ (feet). 
In addition, we assume that the speaker can choose from a set of five utterances: two literal utterances (e.g. ``Cam is tall,'' ``Cam is not tall''), two metaphors (e.g. ``Cam is a giraffe,'' ``Cam is a penguin''), and the null utterance, which is to say nothing. We assume that saying nothing is the least costly action to take, while saying the literal and figurative utterances are equally costly, as they are similar in length.

The literal listener $L_0$ interprets utterances using their literal semantics.
When $L_0$ hears ``Cam is a giraffe,'' he believes that Cam is literally a member of the category ``giraffe,'' and has the corresponding height distribution $P(f | c)$. To represent the literal semantics of the gradable adjective ``Cam is tall,'' we borrow from 
\citeA{lassiter2013context}, which adopted a degree semantics where adjectives indicate an entity's position relative to a threshold value. Intuitively, the semantics of ``Cam is tall'' can be represented as meaning that Cam's height is greater than a threshold value $\theta$, which is an unknown free variable. Following \citeA{lassiter2013context}, $L_0$ passes the free variable $\theta$ through the speaker to be inferred by the pragmatic listener $L_1$. 
%
%The literal listener's interpretation of different utterances is thus as follows:
%%
%%To answer the question ``Is Cam tall?'' the speaker and listener must have a representation for the meaning of \emph{tall}. Previous work on gradable adjectives has shown that a reasonable representation of gradable adjectives is a \emph{threshold semantics}, where ``Cam is tall'' is true if Cam's height is above a certain threshold. Rational Speech-Acts models have used threshold semantics to model the interpretation of gradable adjectives. \todo{read up and fill in}
%
%
%\begin{equation}
%L_0(c, f | u) = \left\{ 
%  \begin{array}{l l}
%    P(f | c) & \quad \text{if $c$ = $\llbracket u\rrbracket$}\\
%    P(f)  & \quad \text{if $f > \theta$}
%  \end{array} \right.
%\end{equation}
%
From the speaker's perspective, we assume that there are two possible QUDs that the speaker may try to answer: ``What is Cam's height?'' and ``Is Cam tall?'' 
As before, the speaker's choice of utterance given a QUD $Q$, an animal category $c$, and a feature value $h$ is given by the following,
\begin{equation}
S_1(u | Q, c, f, \theta) \propto e^{\lambda U(u | Q, c, f, \theta)}
\end{equation}
where $\theta$ specifies a threshold for satisfying a QUD $Q$. We define two projection functions Q, which formalize the questions ``What is Cam's height?'', and ``Is Cam tall?'' respectively.
\[
Q_{height}(f, \theta) = f
\]
\[
Q_{tall}(f, \theta) 
 = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if $f \geq \theta$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
  \]
These projections are then used in the speaker's utility function:
\begin{equation}
U(u | Q, c, f, \theta) = log \sum_{f'} \delta_{Q(f, \theta) = Q(f', \theta)} L_0(c, f, | u) - Cost(u)
\end{equation}
The pragmatic listener performs Bayesian inference given an utterance, while marginalizing over the two QUDs and possible values of the threshold $\theta$.
\begin{equation}
L_1(c, f | u) \propto P(c) P(f | c) \sum_{Q, \theta} P(Q)P(\theta)S_1(u | Q, c, f, \theta)
\label{scalar-L1}
\end{equation}
The interpretation of a metaphor such as ``Cam is a giraffe'' is thus modeled as the joint distribution over Cam's species category (giraffe or person) and his height. 

To evaluate the model's interpretation, we selected two animal metaphors for each of the three continuous feature dimensions, as shown in Table~\ref{scalar-features}.
We then conducted Experiment 4a to elicit the appropriate prior knowledge for the model, namely the distribution over feature values given a category $P(f | c)$, as well as Experiment 4b to measure people's interpretations of different literal and metaphorical utterances.

\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}c}
\begin{table}
\centering
\tabcolsep=0.15cm
%\fontsize{11}{9}\selectfont
\begin{tabular}{|a |c c c | c c c|}\hline
%\toprule
\textbf{Feature} & Animal (high) & Animal (low) & Bins \\\hline
height & giraffe & penguin & $1, 4, 4.5, 5, 5.5, 6, 6.5, 7, 15$ (feet)\\
weight & elephant & bird & $10, 100, 120, 140, 160, 180, 200, 220, 10000$ (pounds)\\
speed  & cheetah & turtle & $1, 5, 6, 7, 8, 9, 10, 20, 60$ (miles/hour)\\
\hline
\end{tabular}
\caption{Three continuous feature dimensions and two animal metaphors for each dimension. We binned the continuous feature values using different bin boundaries for the three dimensions (see main text for more details).}
\label{scalar-features}
\end{table}


\begin{figure}
\centering
\scalebox{0.53}{\includegraphics{Figures/scalar-priors-labeled.png}}
\caption{Prior probability distributions over feature values for each category and dimension.} 
\label{scalar-priors}
\end{figure}

\subsubsection{Experiment 4a: Prior Elicitation}
We designed Experiment 4a to elicit prior distributions over feature values for different animal categories. Because eliciting a full distribution over a large range of continuous values is too costly and may not be necessary for our purposes, we first created bins for each feature dimension to reduce the number of possible feature values (Table~\ref{scalar-features}). We determined the bin sizes such that they span a range of values, including the average feature values for the animal categories (e.g. giraffes are around $15$ feet tall), and created more fine-grained bins for values neighboring the average feature values for people (e.g. people tend to be around $4.5$-$6.5$ feet tall, so the bin sizes in that range are smaller). 
We elicited feature values for the six animals in Table~\ref{scalar-features}, as well as for human males and females\footnote{We included both genders in order to examine the effect of people's prior knowledge about the metaphorical target (e.g. a male person or a female person) on participants' interpretation)}. 

30 participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed 12 trials in random order, 4 for each of the 3 feature dimensions. Participants were asked to imagine a member of a category (e.g. \textsc{woman}, \textsc{man}, \textsc{giraffe}, \textsc{penguin}), and to use slider bars with end points labeled ``definitely'' and ``impossible'' to rate how likely it is for the member to have various ranges of feature values.
%
We normalized each participant's ratings for the ten ranges to sum up to 1. From Figure~\ref{scalar-priors}, we see that the distributions across the three dimensions differ slightly for the \textsc{man} and \textsc{woman} categories, while the animal categories are likely to have extreme feature values. %\todo{reliability}. 



\subsubsection{Experiment 4b: Metaphor Interpretation}
To measure people's interpretations of metaphors and compare them with interpretations of literal utterances, we designed a scenario in which a speaker (e.g. Bob) is talking to his friend about a man/woman on his team. The friend asks, ``Is he/she tall/heavy/fast?'' Bob then responds with one of the possible utterances shown in Table~\ref{scalar-questions-responses}. 
54 participants with IP addresses in the United States were recruited on Amazon's Mechanical Turk. Each participant completed 6 trials in random order, balanced by the target's category (\textsc{man} vs \textsc{woman}) as well as the type of utterance Bob produces (literal or metaphorical). After reading the scenario, participants were asked to use slider bars with end points labeled ``definitely'' and ``impossible'' to rate how likely it is for the target to have various ranges of feature values.

\begin{table}
\centering
\tabcolsep=0.2cm
\small
\begin{tabular}{|c|c|c|c|c|c|}\hline
Question & Literal yes & Literal no & Metaphor yes & Metaphor no  \\\hline
``Is he tall?'' & ``Yes, he is.'' & ``No, he's not.'' & ``He's a giraffe.'' & ``He's a penguin.'' \\
``Is he heavy?'' & ``Yes, he is.'' & ``No, he's not.'' & ``He's an elephant.'' & ``He's a bird.'' \\
``Is he fast?'' & ``Yes, he is.'' & ``No, he's not.'' & ``He's a cheetah.'' & ``He's a turtle.'' \\\hline
\end{tabular}
\caption{Example questions and utterances in Experiment 4b.}
\label{scalar-questions-responses}
\end{table}


We again normalized each participant's ratings for the ten ranges to sum up to 1.  From the Spearman-Brown prediction equation, reliability of the ratings was  $0.924$ ($95\%$ CI $=[0.922, 0.925] $). From Figure~\ref{scalar-interp}, we see that metaphorical utterances (e.g. ``Cam is a giraffe'' and ``Cam is a penguin'') result in feature value distributions that are slightly more extreme than their literal counterparts (e.g. ``Yes, he is (tall)'' and ``No, he's not (tall)''). However, the interpreted feature values are not as extreme as the prior feature values for animal categories shown in Figure~\ref{scalar-priors}. This result suggests that metaphors may communicate more extreme values than literal descriptions, but also that listeners use their prior knowledge about the target domain to arrive at plausible, hyperbolic interpretations.

\begin{figure}
\centering
\scalebox{0.54}{\includegraphics{Figures/scalar-interp.pdf}}
\caption{Probability distributions over feature values, given a target's gender (man vs woman) and the utterance used to describe him/her. Distributions given metaphorical vs literal utterances (``Cam is a giraffe'' vs ``Cam is tall'') are indicated by line type, while distributions given utterances with different polarities (``Cam is a giraffe'' vs ``Cam is a penguin'') are indicated by color.} 
\label{scalar-interp}
\end{figure}

\begin{figure}
\centering
\scalebox{0.54}{\includegraphics{Figures/model-scalar-interp.pdf}}
\caption{Model predictions over feature values, given a target's gender (man vs woman) and the utterance used to describe him/her.}
%Distributions given metaphorical vs literal utterances (e.g. ``Cam is a giraffe'' vs ``Cam is tall'') are indicated by line type, while distributions given utterances with different polarities (e.g. ``Cam is a giraffe'' vs ``Cam is a penguin'') are indicated by color.} 
\label{model-scalar-interp}
\end{figure}


\subsubsection{Model Evaluation}
We test whether the pragmatic listener described in Equation~\ref{scalar-L1} arrives at these kinds of hyperbolic interpretations, such that metaphors are interpreted as conveying more extreme values than literal utterances, but still less extreme than the literal meanings. We fit two free parameters to the data obtained in Experiment 4b: the speaker rationality parameter ($\lambda=2$), and the cost of utterances ($c=1$). Figure~\ref{model-scalar-interp} shows the model's interpretations of the 24 utterances (3 dimensions $\times$ 2 utterance types $\times$ 2 polarities $\times$ 2 target genders) with these parameter settings. We see that the model also infers more extreme feature values given metaphorical utterances, and that the values are moderated by prior knowledge of the target domain. The model predictions correlate significantly with people's interpretations across all of the utterances ($r= 0.81, p < 0.0001$), with a correlation of $r=0.84$ ($p < 0.0001$) within metaphorical utterances. This suggests that a model that takes into account the semantics of gradable adjectives and reasons about literal alternatives to metaphorical utterances is able to produce interpretations that correspond with humans'. 
\subsubsection{Discussion}
In this section, we showed that representing gradable features as continuous values allows us to examine the differences between interpretations of metaphors and literal utterances in a more precise manner. We find that people demonstrate three effects in their interpretations of metaphors with gradable features. First, metaphorical utterances receive hyperbolic interpretations, such that saying ``Cam is a giraffe'' means that he is tall relative to an average male person, but not as tall as an average giraffe. Second, the interpretation of both metaphors and literal utterances is sensitive to specific prior knowledge about the target domain, such that John's height given the utterance ``John is a giraffe'' is greater than Jane's height given the utterance ``Jane is a giraffe'' due to the gender differences observed in the priors. Third, metaphorical utterances receive more extreme interpretations than literal descriptions, such that Cam's height given ``Cam is a giraffe'' is greater than his height given ``Cam is tall.'' All three of these effects were captured by the qRSA model we described, by incorporating prior beliefs with reasoning pragmatically about what feature values and QUDs would lead a rational speaker to choose certain utterances. 

Previously in Experiment 2c and the corresponding model simulations, we showed that metaphors can communicate information along more feature dimensions than literal utterances. A metaphor such as ``Cam is an ox'' efficiently communicates that Cam is likely \emph{strong, big}, and \emph{slow}, while describing all three features in literal terms would require a much longer utterance, such as ``Cam is strong, big, and slow.'' In Experiment 4b, we show that metaphors can not only efficiently communicate multiple features of a target, but also communicate more extreme values efficiently without using modifiers such as ``Cam is very tall.'' This provides additional evidence that metaphors and other types of figurative uses may be uniquely suited to communicate information efficiently in certain situations.

%\todo{more comments on QUD assumptions, etc}

%\todo[inline]{Basic: how the model gets basic metaphorical/figurative interpretation of scalar metaphors}
%\todo[inline]{Compared to literal alternatives, metaphors are more striking/extreme}

%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 2: Non-paraphrasability}


%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 3: Context-sensitivity}
%\todo[inline]{Metaphors communicate more efficiently. Look at feature metaphors}



%\subsubsection{Experiments}

%\subsubsection{Model}

%\subsection{Study 4: Aptness and alternatives}
%
%%\todo[inline]{Compared to other metaphorical alternatives, some metaphors are better/more apt}
%
%Many researchers have examined the factors that contribute to the ``aptness'' of a metaphor as well as how aptness facilities metaphor processing \cite{tourangeau1981aptness, jones2006roosters, blasko1993effects}. In Study 4, we propose that aptness may in part be related to two pragmatic factors: (1) The presence of multiple salient features of the metaphorical target, which cannot be described literally without using longer utterance (2) The absence of salient alternative metaphors that the speaker could have used to communicate about the dimension(s) under discussion. We will test these ideas using a series of experiments, and show that the qRSA model can capture some of these effects.

\subsection{Toward a Model of Relational Metaphor}
So far In this chapter, we have assumed that the QUDs involved in metaphor interpretation are defined with respect to features of the target entity (e.g. ``Is Cam tall?'', ``What is Cam's height?''). In this section, we explore ways to extend the space of QUDs to address more abstract relational structures, which is central to understanding many metaphors and analogies. 

\citeA{gentner1988metaphor} drew a distinction between attributional and relational metaphors. Attributional metaphors are interpreted by discovering the attributes, or features, that the metaphorical source and target have in common, for example being \emph{tall} in the case of ``Cam is a giraffe.'' Relational metaphors, on the other hand, are understood by discovering the system of relations that are shared between the source and target domains. For example, ``The immune system is an army'' is understood by recognizing that the system of relations that holds in the source domain holds for the target as well, and also appropriately aligning their components. A great deal of research has suggested that people focus on relational commonalities when interpreting metaphors and prefer relational metaphors over attributional ones \cite{gentner1988metaphor, gentner1988evidence, gentner1997structure}. As a result, it is important to show that the qRSA model can be applied to explain the interpretation of relational metaphors in addition to attributional ones. In what follows, we use a simple example of relational metaphor to demonstrate how a qRSA model with relational QUDs could produce appropriate interpretations of relational metaphors through the same basic mechanism of pragmatic reasoning.



\subsubsection{Relational QUDs}

Consider the relational metaphor: ``The immune system is an army.'' Similar to approaches such as the Structure Mapping Engine \cite{falkenhainer1989structure}, we represent an entity as a node in a graph, where the edges represent relations with other nodes. Let us first examine the source domain, \textsc{army}, and identify the neighboring entities that are related to \textsc{army}. We assume that the listener has perfect knowledge about \textsc{army} and the relations held among its neighboring nodes, as shown in Figure~\ref{source-graph}. 
In addition, we assume that the listener has prior knowledge that the \textsc{immune system} is related to the entities \textsc{body} and \textsc{virus} in some unknown way, and also that \textsc{virus} harms \textsc{body}\footnote{We believe it is reasonable to assume that listeners need some prior knowledge regarding the more obvious relationships held in the target domain in order to fully understand these types of relational metaphors. Future work should empirically test the kinds of prior knowledge required to generate appropriate interpretations and inferences for novel relational metaphors.}. However, the listener is uncertain about which relations hold between \textsc{immune system} and the other two entities. This results in the four possible labeled directed graphs shown in Figure~\ref{target-graph}. An interpretation of the metaphor can thus be represented as a distribution over these four possible graphs $G=(N, E)$, where $N$ is an ordered set of labeled nodes, and $E$ is an ordered set of labeled directed edges.

\begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  %\tikzstyle{every state}=[fill=gray,draw=none,text=white]

  \node[main node] (A)                    {army};
  \node[main node]         (B) [above right = 1.2cm and 0.5cm of A] {enemy};
  \node[main node]         (C) [below right = 1.2cm and 0.5cm of B] {state};
%  \node[state]         (C) [below right of=B] {$q_c$};
%  \node[state]         (E) [below of=D]       {$q_e$};

  \path (A) edge              node {fights} (B)
            edge              node {protects} (C)
          (B) edge              node {harms} (C);

\end{tikzpicture}

\caption{A labeled directed graph for the source domain \textsc{army}, where the nodes are entities and the edges are relations between entities.}
\label{source-graph}
\end{figure}

\begin{figure}
\centering
% graph1
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]

  \node[main node] (A)                    {IS};
  \node[main node]         (B) [above right = 1.2cm and 0.5cm of A] {virus};
  \node[main node]         (C) [below right = 1.2cm and 0.5cm of B] {body};
  \path (A) edge              node {fights} (B)
            edge              node {protects} (C)
          (B) edge              node {harms} (C);

% graph2
\begin{scope}[xshift=4cm]
\node[main node] (A)                    {IS};
  \node[main node]         (B) [above right = 1.2cm and 0.5cm of A] {virus};
  \node[main node]         (C) [below right = 1.2cm and 0.5cm of B] {body};
  \path (B) edge  [above left =1cm and 0.5cm]            node {fights} (A)
           (A) edge              node {protects} (C)
          (B) edge              node {harms} (C);
          
   \end{scope}

% graph3
   \begin{scope}[xshift=8cm]
\node[main node] (A)                    {IS};
  \node[main node]         (B) [above right = 1.2cm and 0.5cm of A] {virus};
  \node[main node]         (C) [below right = 1.2cm and 0.5cm of B] {body};
  \path (A) edge              node {fights} (B)
           (C) edge [above]   node {protects} (A)
          (B) edge              node {harms} (C);
          
   \end{scope}
   
 % graph4
   \begin{scope}[xshift=12cm]
\node[main node] (A)                    {IS};
  \node[main node]         (B) [above right = 1.2cm and 0.5cm of A] {virus};
  \node[main node]         (C) [below right = 1.2cm and 0.5cm of B] {body};

  \path (B) edge  [above left =1cm and 0.5cm]             node {fights} (A)
           (C) edge [above]             node {protects} (A)
          (B) edge              node {harms} (C);
          
   \end{scope}

\end{tikzpicture}

\caption{All possible labeled directed graphs that describe the target domain \textsc{immune system} (IS), given knowledge of the relation between \textsc{body} and \textsc{virus} and uncertainty about the other two relations.}
\label{target-graph}
\end{figure}

If relational metaphors communicate a system of relations shared between the source and target graphs, then we can specify the QUD that the speaker wants to address as the question, ``Which system of relations holds for the target domain?'' We formalize the QUD as a function that takes in a fully connected three-node directed labeled graph $G=(N, E)$ and removes the node labels , transforming it into a more abstract graph $G_{abs}=E$.
The QUD thus abstracts away the entities of the target and source domains, while retaining the relations that hold among the entities. We show how a model that reasons about this more abstract, relational QUD can interpret relational metaphors.

Again starting with the literal listener, $L_0$ hears the metaphor ``The immune system is an army,'' and believes that \textsc{immune system} is literally equivalent to \textsc{army}. Given $L_0$'s knowledge of the source domain, she now believes that the \textsc{immune system} protects the \textsc{state} and fights the \textsc{enemy}. From the speaker's perspective, the QUD is such that he does not care about the identity of the nodes and does not mind that $L_0$ ends up with this strange belief. The speaker's utility function is defined as:

\begin{equation}
U(u | Q, G) = log\sum_G' \delta_{Q(G)=Q(G')} L_0(G' | u)
\end{equation}
where the $Q$ is a projection function that takes in a graph $G$ and removes the node labels, resulting in abstract nodes $n_1$, $n_2$, $n_3$ instead of labeled nodes \textsc{immune system}, \textsc{enemy}, and \textsc{state}. The speaker is satisfied with $L_0$ understanding the system of relations that hold among the three nodes, which is that a node $n_1$ fights another node $n_2$; $n_2$ harms another node $n_3$; and $n_1$ protects $n_3$ and is not perturbed by the fact that $L_0$ believes \textsc{immune system} literally protects the \textsc{state}. 
The pragmatic listener $L_1$ combines prior knowledge about which labeled graphs $G$ are probable, together with a model of $S_1$'s utterance choice, to infer the appropriate graph structure and node labels given an utterance. 

\begin{equation}
L_1(G | u) \propto P(G)S_1(u | G)
\end{equation}

Suppose the utterance $u$ is ``The immune system is an army.'' $L_1$ reasons that the speaker chose the utterance in order to communicate to $L_0$ that \textsc{immune system} is the entity that fights an abstract node $n_2$ and protects another node $n_3$, and that $n_2$ harms $n_3$. Given prior knowledge that the set of all possible labeled graphs containing \textsc{immune system} are those listed in Figure~\ref{target-graph} (which is determined by the knowledge that \textsc{virus} harms \textsc{body}), $L_1$ ends up with the interpretation that the correct graph is the leftmost one, such that \textsc{immune system} fights \textsc{virus}, and \textsc{immune system} protects \textsc{body}. 

%\begin{tikzpicture}
%
%    \node[main node] (1) {$1$};
%    \node[main node] (2) [below left = 1.3cm and 0.5cm of 1]  {$2$};
%    \node[main node] (3) [below right = 1.3cm and 0.5cm of 1] {$3$};
%
%    \path[draw,thick]
%    \tikzset{EdgeStyle/.style={->}}
%    \Edge[label=$3$](1)(2)
%    \Edge[label=$10$](2)(3)
%    \Edge[label=$10$](1)(3)
%    %%
%    \begin{scope}[xshift=4cm]
%    \node[main node] (1) {$1$};
%    \node[main node] (2) [right = 2cm  of 1]  {$2$};
%    \node[main node] (3) [below = 2cm  of 1] {$3$};
%    \node[main node] (4) [right = 2cm  of 3] {$4$};
%
%    \path[draw,thick]
%    (1) edge node {} (2)
%    (1) edge node {} (4)
%    (3) edge node {} (2)
%    (3) edge node {} (4)
%    ;
%    \end{scope}
%\end{tikzpicture}

%\begin{tikzpicture}
% \SetUpEdge[lw = 1.5pt,
%            color      = blue ,
%            labelcolor = white]
%  \GraphInit[vstyle=Normal] 
%  \SetGraphUnit{3}
%  \tikzset{VertexStyle/.append  style={fill}}
%  \Vertex{army}
%  \NOEA(army){enemy}  \SOEA(army){country} \NOEA(enemy){country}
%  %\SOEA(B){C}  \SOEA(C){L}
%  \tikzset{EdgeStyle/.style={->}}
%  \Edge[label=fights](army)(enemy)
%  \Edge[label=protects](army)(country)
%  \Edge[label=harms](enemy)(country)
%  %\Edge[label=$10$](B)(P)
%\end{tikzpicture}

%The immune system is an army. An entity can be seen as a node in a graph, where the edges are relations. Relational concept. 
%
%The immune system is an army
%We assume that there are 3 components in the source domain: ["army", "country", "enemy"] and 3 components in the target domain: ["immune system", "body", "virus"]. 
%
%We have prior knowledge about the following relations in the source domain: protects(army, country); harms(enemy, country); fights(army, enemy), and prior knowledge about just one relation in the target domain: harms(virus, body). We can represent each domain as a distribution over labeled directed graphs, where each labeled edge is a relation between two of the components. The QUD is thus which labeled directed graph represents the immune system domain.
%
%Given the analogy "The immune system is an army," a QUD that specifies which labeled directed graph describes the target domain, and the prior knowledge that harms(virus, body), the pragmatic listener infers the correct directed labeled graph and assigns "virus," "body," and "immune system" to the correct nodes of the graph. This results in an alignment between the three components in the target graph and the three components in the source graph.
%
%\subsubsection{Feature and Relational QUDs}
%
%The atom is a solar system
%In this particular implementation I assumed that there are 2 planets and 2 electrons, but we can make this more general by only caring about sets of planets and electrons so the number of components doesn't matter.
%
%We again represent each domain as a labeled directed graph, where the nodes are components e.g. "sun", "planet", "nucleus", "electron." Unlike the immune system metaphor, here we only consider one type of relation: orbits(x, y). We don't have any prior knowledge about which components orbit which other components in the target domain.
%
%However, we do have prior knowledge that orbits(planet, sun); that the planets are small and the sun is big; and that the electrons are small and the nucleus is big. The QUD can be  "which labeled directed graph describes the target domain?", "what are the features of each of the nodes in the graph?", or both.
%
%Given the analogy "The atom is a solar system," the pragmatic listener assigns highest posterior probability to the correct directed labeled graph and assigns "nucleus", "electron1", "electron2",.... to the correct nodes of the graph, such that orbits(electron, nucleus). This is thus a correct alignment between the components in the target graph and the components in the source graph.
%
%The two models work in the same way, with some minor differences in implementation. The main difference is that in the immune system case, the alignment relies on prior knowledge about the relation between two components of the target domain (harms(virus, body)). In the atom case, the alignment relies on prior knowledge about features of the components in the target domain (nucleus is relatively big, electrons are relatively small), and the fact that the speaker may want to communicate both the surface features of the components and the relational structure.
%
%It feels like these two examples illustrate how RSA can be extended to handle structured metaphors and alignments in a way that could make sense to the structure mapping people. If this sounds reasonable, I will start writing it up in a section in the figurative paper and make some potential figures. 

\subsubsection{Discussion}
In this section, we used a relational metaphor as an example to show that QUDs are flexible enough to apply to graph structures, thus allowing speakers to communicate information about a system of relations in a domain. Thus, the QUD can serve as a mechanism for listeners to abstract away surface-level features and to infer the correct relational structure being communicated by a metaphor. 

Previous research has suggested that given different contexts and goals, listeners can produce different inferences and interpretations for the same relational metaphor \cite{spellman1996pragmatics}. These results suggest that pragmatic information may be important in models of analogy understanding. \citeA{holyoak1989analogical} proposed a constraint-satisfaction based model called Analogical Constraint Mapping Engine (ACME), which specifies structural, semantic, and pragmatic constraints for determining the best mapping between source and target domains. We believe that the qRSA model provides a natural way to implement pragmatic constraints via QUDs, and incorporates structural, semantic, and pragmatic information in a principled, theoretically motivated manner. Future work should examine the differences between these pragmatic frameworks for relational metaphor understanding, as well as provide empirical validation for the model assumptions and predictions we describe here. For the purposes of this paper, our goal was to show that the qRSA model can be extended to explain metaphors with more structural and relational information, which this example demonstrates.
%, and that the same components that shape h---literal meaning, prior knowledge, QUD, and pragmatic reasoning---

\section{General Discussion}
In this chapter, we reviewed approaches to studying figurative language understanding from the perspective of pragmatics and communication. We described the communicative principles and extralinguistic information that shape figurative understanding and articulated the need to clarify the interactions among these components in order to more precisely understand how people arrive at figurative meanings in context.
Building upon the RSA framework, we proposed and evaluated a computational model that explicitly describes how people use different sources of information---literal meaning, encyclopedic knowledge, and contextual information---to produce figurative interpretations. Through a series of behavioral experiments, we showed that the model closely matches people's interpretation of hyperbole, verbal irony, and a variety of metaphorical uses, suggesting that different types of figurative language may share the same underlying communicative principles. 

%Based on these results, we argue that the qRSA model incorporates information in a principled and theoretically motivated manner, which provides a useful framework for testing and modeling various phenomena in language understanding. 
%
%In what follows, we will discuss two specific areas in which the qRSA model has made (and will continue to make) contributions.

%\subsection{Computational models of language understanding}
%In recent years, computational models have grown increasingly popular in cognitive psychology and linguistics. The appeal of computational approaches for these fields has roots in both science and engineering. From a scientific perspective, the formal rigor of computational models allows researchers to define theories in a more precise manner, test these theories against empirical data at a higher resolution, and arrive at more accurate and powerful conclusions. From an engineering perspective, implementing formal theories of human behavior in computer programs enables us to build artificial agents that can predict and replicate these behaviors at a large scale. 

We believe that the qRSA model makes several critical advancements to formal models of human language understanding. The model captures key intuitions about communication, including the role of common ground between listener and speaker, the assumption that speakers produce utterances that maximize informativeness, and the idea that informativeness should be considered with respect to the question under discussion. By formalizing these intuitions, the model is able to go beyond the literal meanings of utterances and predict subtleties in interpretation that are sensitive to background knowledge, communicative efficiency, local context, and alternative utterances. From a scientific perspective, our work provides a formal definition of QUD as well as the relevance principle, which allows us to empirically test the prediction that listeners reason about utterances' relevance to the QUD in order to arrive at appropriate interpretations. This formalization also allows us to show that QUD inference is critical for many instances of language understanding, in particular figurative language, where the literal meanings of utterances are often false. By exploiting listeners' assumption that speakers choose utterances in order to communicate information relevant to the QUD (and not irrelevant information or information that is already in common ground), speakers can choose utterances that are not literally true (e.g. ``Cam is a giraffe'') in order to effectively communicate relevant information (e.g. that Cam is unusually tall), without leading the listener to believe false information (e.g. that Cam is a giraffe). 
%
From an engineering perspective, our model provides a step towards building systems that use pragmatic reasoning and representations of the QUD to better interpret and generate utterances in context. We believe that our approach may contribute to the design of artificial agents that use language in more flexible and creative ways.

%a better understanding of how we use pragmatic reasoning to incorporate considering these components can result in dialogue systems that can exploit researchers advancement  should be integexplicitly accounts for QUD and contributes to contributions to NLP, dialogue systems, better model of QUD and extralinguistic knowledge \todo{make this whole paragraph more reasonable}

%\subsection{Explanations for figurative language}
In addition to advancing general models of language understanding, our work may provide explanations for phenomena that particularly interest researchers of figurative language.
One distinctive aspect of figurative utterances is that they communicate multiple dimensions of meaning at once and are often difficult to paraphrase \cite{ortony1975metaphors}. 
By considering encyclopedic knowledge such as affects associated with different states and features associated with different categories, our model naturally accounts for the multi-dimensional quality of figurative meaning. 
%
In addition, many researchers have observed that the interpretation of figurative language is especially sensitive to common ground \cite{pexman2004does, gibbs2000irony}. Given that the literal meanings of figurative utterances are often implausible or false, listeners rely on background knowledge in order to reason about and recover speakers' intended meanings. As a result, it is important for listeners and speakers to share background knowledge (both encyclopedic knowledge and specific prior beliefs) in order to communicate successfully using figurative utterances. Indeed, researchers have found that willingness to use ironic utterances is positively correlated with social intimacy between interlocutors \cite{kreuz1996use}, and interlocutors who use metaphorical speech with each other are perceived as having a closer relationship\cite{horton2007metaphor}. \citeA{kreuz1996use} attributes the effects of social closeness to what he calls a \emph{principle of inferability}: speakers are more likely to use a non-literal utterance when they are more certain that it will be understood appropriately, which in turn is more likely when the speakers and listeners share similar background knowledge and beliefs. For example, suppose Ann saw a watch that cost $\$1000$ and wants to tell Bob that she thinks it is very expensive. In order to communicate her meaning effectively using a hyperbolic utterance: ``That watch cost a million dollars!'', Ann must be fairly confident that Bob's prior beliefs would not lead him to believe that the watch literally cost a million dollars. And since Bob reasons about Ann's motivation for choosing an utterance, given such a hyperbolic utterance, Bob arrives at the inference that Ann must be fairly confident about his prior beliefs. Our model provides a natural way to incorporate inferences about common ground in order to predict and model effects of social closeness.  In its current form, our model assumes that the listener is certain that the speaker has perfect knowledge of the listener's background knowledge and prior beliefs. If we relax this assumption to incorporate uncertainty about the speaker's knowledge of the listener, the pragmatic listener is able to derive information about the speaker's knowledge of the listener given an utterance. Preliminary explorations of our model show that it naturally affords these types of social inferences, which we plan to examine more thoroughly in future work.

%predict when and how listeners infer common ground with the speaker based Because our model explicitly represents the listener's encyclopedic knowledge and prior beliefs, it provides a natural way to extend the model to incorporate infereces
%to The importance of background knowledge
%\subsubsection{Common ground}
%A third 
%Figurative language is especially sensitive to common ground. People tend to use figurative language with people they are close to. 
%Our model formalizes the importance of common ground and prior knowledge. In addition, it makes further predictions abut what happens when listeners are uncertain about common ground.

One of the most important contributions of our work is that it unifies the interpretation of diverse types of figurative language in a single computational model. The generality of this model suggests that separate processing mechanisms may not be necessary to derive different types of figurative interpretations, or to derive figurative interpretations at all. 
%According to our model, listeners consistently incorporate multiple sources of information in order to recover speakers' intended meanings. 
The qRSA model does not distinguish \emph{a priori} between figurative language and other types of language use; instead, the same pragmatic reasoning takes place regardless of whether the model ultimately produces a figurative or a literal interpretation. While our model is a computational-level account of language understanding and makes no process-level claims, we note that the model engages the same reasoning mechanism to interpret both figurative and literal utterances, which is consistent with psycholinguistic evidence that figurative language does not take reliably longer to process. On the other hand, some evidence suggests that prior context reduces the processing time for figurative utterances. Although we again do not make claims with our model regarding processing times, we suggest that this type of contextual effect may be modeled as a reduction of uncertainty in the QUD, which may lead to faster processing. 
%Since the literal meanings of figurative utterances are often implausible or false, listeners need to rely on other sources of information in order to reason about and recover speakers' intended meanings, including the QUD, encyclopedic knowledge, and prior beliefs. Speakers need to 
%For example, the context-sensitivity of figurative interpretation can be modeled as sensitivity to the QUD. Prior context sets up QUD. Since 
%context, figurative meanings are accessed as quickly or even more quickly than literal meanings.
%researchers have observed that the interpretation of figurative utterances is highly sensitive to context. Given enough prior context, figurative meanings are accessed as quickly or even more quickly than literal meanings.
%Figurative utterances such as metaphor receive different interpretations under different contexts. Metaphorical meanings are accessed more quickly when given enough prior context. The qRSA models this using QUD.
%Furthermore, our model provides a formal 
%Researchers who study figurative language have observed several different things about figurative language understanding, which our model captures and may be able to explain.
%
%\subsubsection{Aptness judgements}
Overall, our work suggests that the rich meanings expressed by figurative language can be explained by basic principles of communication, thus demonstrating the importance of considering pragmatics in theories of figurative language.
%\todo[inline]{Not sure whether it's dangerous to mention processing times at all, but I think reviewers will likely ask about it, so might as well say something...}
%The fact that the same basic communicative principles can be formalized to produce fine-grained interpretations of diverse types of figurative language demonstrates the importance of considering the role of pragmatics in shaping many aspects of linguistic meaning. 

%\subsection{Social aspects of communication}
%
%\subsection{Why do people use figurative language?}
%\subsubsection{Intimacy and common ground inference}
%\subsubsection{Humor}


%In addition, previous work has shown that conventional metaphors such as ``He is a pig'' may be processed differently from novel metaphors \cite{bowdle2005career}, which introduces a set of interesting questions to investigate with our model. We believe that our computational framework advances understanding of the computational basis of metaphor and of communication more generally, and we hope that it will continue to shed metaphorical light on related questions.

\subsection{Future directions}

%While the RSA framework provides a promising start to examining figurative language understanding, it also inspires
%many more questions. 
%Each of these components inspire 
%The work we described formalizes figurative language understanding and provides a precise and promising start to examining creative, figurative language use in a concrete and rigorous manner. 
The work we described invites many directions for future research, both for computational models of pragmatics and figurative language understanding. %that can be pursued using both our modeling framework and experimental paradigm.
%\subsubsection{Open questions in the RSA framework}
%How are QUDs determined? How are alternative utterances determined?
While our model explicitly reasons over a set of QUDs and alternative utterances, we have not precisely defined how listeners select the particular set of QUDs and alternatives over which to reason. For example, when should listeners consider speakers' affects and subjective attitudes to be potential QUDs? How do listeners decide that certain features of the metaphorical source are likely QUDs (e.g. \emph{scary} in the case of ``Cam is a shark''), while others are not (e.g. \emph{has teeth} or \emph{swims})? In the work described In this chapter, we made the simplifying assumption that QUDs arise from the literal meanings of utterances in an associative manner, and relied on intuition and previous research to focus on affective QUDs in the case of hyperbole and irony and feature QUDs in the case of metaphor. We then defined a set of QUDs based on the affects and features associated with various states and categories. Future research should examine whether QUDs can be more systematically inferred from prior context or the utterance itself, thus removing the need to predefine a set of QUDs that the model should consider for a given type of figurative utterance. A more flexible and general way to define a set of QUDs could also enable us to determine which types of QUDs are most effectively addressed by different kinds of figurative utterances. 

%Metaphor understanding is related to many other complex issues such as analogy, conventionality, and embodied cognition. To reasonably limit the scope of our work, In this chapter we focused on simple nominal metaphors such as ``Cam is a shark,'' where both the metaphor source and target are concrete objects, and where the source is relatively unconventional (for example, we did not include idiomatic metaphors such as ``Cam is a chicken''). We also only considered attributional metaphors, where the source and target have certain features in common (e.g. ``fierce'' and ``scary''). We have not yet shown that our model can account for other types of metaphors, such as verbal metaphors, where verbs instead of nouns are used non-literally (e.g. ``The ice-skater \textit{flew} across the rink''), or conventional metaphors, or metaphors that appear frequently in everyday language to the point of becoming ``dead'' or lexicalized (e.g. ``The man was \textit{drowning} in sorrow''; ``She's the \textit{head} of the household''). (3) relational metaphors, where the source and target share the same relational structure but not necessarily the same simple attributes (e.g. ``A child's brain is a \textit{sponge}'' means that a child's brain absorbs information the way sponge absorbs water) (4) abstract attributional metaphors, where the source and target only share attributes in an abstracted sense (e.g. ``Cam is a \textit{rock}'' means that Cam is stable and dependable in his personality, but not necessarily physically static). \todo{(3) and (4) may turn out to be the same thing.}
%\todo[inline]{
%Future work will need to look into these other types...need to fill in ideas regarding how.
%}

%We make rather vague assumptions that $\vec m$ arises from $m_0$ in an associative manner, and that the distribution over QUD is sensitive to the local context. Based on the principle of informativity, we also assume that the QUD is unlikely to be about something that the listener already knows. 
%For example, since Liz knows that Bob is a person, it is unlikely that Sam wants to communicate the species to which Bob belongs. 
%To understand the model's behavior more fully, these details should be examined in future research. 
%In this work, we showed that the qRSA model successfully captures three types of figurative interpretation: hyperbole, irony, and metaphor. 
While our model provides a unified explanation for three diverse uses language: hyperbole, irony, and metaphor, future work should examine whether the model extends to other types of figurative language. 
For example, understatement refers to the use of a mild statement to describe an extreme situation, such as saying ``It's a bit rainy today'' in the middle of a rainstorm in order to draw attention to the extreme raininess. Because the literal meaning of ``a bit rainy'' is associated with both neutral valence and low arousal, our model finds very little reason for a speaker to choose this utterance to communicate either negative valence or high arousal. What information, then, is a speaker trying to communicate using a statement that does not match the speaker's valence, arousal, or the objective state of the world? We observe that intuitively, understatements such as ``It's a bit rainy today'' draw attention to the common ground between speaker and listener, such as the fact that they both know that it is extremely rainy. In order to explain other types of figurative language such as understatement, it may be necessary to incorporate these types of common ground and social inferences. %\todo{idioms?}.

Thus far, our work has focused on information-theoretic motivations for using figurative language, such as to efficiently communicate multiple dimensions of meaning or more extreme states of the world. In future work, we plan to explore the social motivations for using figurative language, such as to communicate common ground with the listener or to evoke humor. Earlier in the discussion, we briefly described how figurative language may lead to inferences about common ground and outlined a way to incorporate these inferences in our model. In addition, we observe that figurative utterances such as irony and metaphor are often humorous, which also has social consequences. 
%We propose that the humor of figurative language can be explained by the Incongruity Theory of humor, which posits that situations that afford multiple interpretations at the same time are more likely to be funny . 
We speculate that figurative utterances are often funny because they give rise to dramatically different, and therefore often incongruous, interpretations given different QUDs, which is supported by theories of humor where incongruity is key \cite{suls1983cognitive, attardo1994linguistic}.
By examining the kinds of linguistic input that give rise to humor, we may discover reasons why figurative utterances are uniquely suited for evoking humor, thus furthering our understanding of the social motivations for figurative language use. 
%\todo{by formalizing...we can find more reasons for why figurative language is appealing and what kinds of information it is used to communicate}
%\todo{politeness?}

%How might we use the qRSA model to explain higher-level social functions of nonliteral language, such as humor and conveying social closeness? 
%Are listeners less likely to interpret an utterance figuratively when there is little common ground between speaker and listener? 


%\subsubsection{Other types of figurative language}
%Can the model explain metaphor interpretation that requires feature alignment and identification of analogical relations? 

%Understatement, Idioms. Understatement may require common ground inference. Idioms require a way to represent conventionalized meanings and compositionality.

%We aim to address these questions in future research to further clarify how communication principles interact to produce metaphorical meaning. 
%Can the model generalize beyond metaphor and hyperbole to explain other types of figurative language such as irony? Can the model provide a natural account of metaphor aptness and poeticness? 

%\subsubsection{Social functions}


\section{Conclusion}
%Figurative language is a crowded and somewhat confusing space. 
Figurative language presents a puzzle for communication. While the information encoded in the literal semantics is false or trivial, figurative utterances are often evocative, socially meaningful, and highly informative. In this chapter, we provided an account of figurative language understanding that partially solves this puzzle using a computational model of communication.
%We described a computational model that formalizes figurative communication as recursive reasoning between speaker and listener. 
We showed that a Rational Speech-Acts model extended to accommodate inferences about the QUD successfully recovers true and relevant information from literally false utterances, predicting people's interpretations of hyperbole, irony, and metaphor with high accuracy. We argue that the qRSA model incorporates information in a principled and theoretically motivated manner, providing a useful framework for testing and modeling various phenomena in language understanding. 
By formalizing principles of communication to explain figurative language, our work 
%advances formal models of language understanding and 
sheds light on how our linguistic, cognitive, and social faculties work together to produce figurative meanings.
%Based on these results, we also suggest that We also argue that the 
%suggesting that the same   that there is plenty of room for formal models to carve out various elements and produce explicit, computational-level hypotheses about which elements are necessary as well as how they interact. We argue that the RSA theory provides a useful framework that incorporates information in a principled manner and fits naturally within a general theory of language understanding. By introducing elements central to figurative language 
%such as associated meanings, affect, and communicative intent, 
%this paradigm may advance formal models of language understanding and sheds light on how our linguistic, social, and world knowledge work together to produce meaning.
%together enables us to we add figurative flesh to the bare literal bones of language.


%
%In what follows, we will discuss two specific areas in which the qRSA model has made (and will continue to make) contributions.

\chapter{Social Inferences and Figurative Language}
People frequently use figurative language such as sarcasm and hyperbole to communicate attitudes and opinions. However, figurative utterances are often false under their literal semantics and require sufficient common ground to be appropriately understood.
Why would speakers produce these types of utterances at the risk of serious misunderstanding, and what are some of the social consequences and benefits of nonliteral communication?
In the last two chapters, I described how reasoning about speakers' communicative goals enables listeners to interpret figurative interpretations. 
In this chapter, I examine the motivations for producing figurative utterances by modeling the social inferences that arise during figurative communication. Unlike the previous chapters, this chapter does not report original behavioral experiments or data. Instead, I will reference existing literature to support the behavior of the models proposed. The goal of this chapter is to use these models to demonstrate how pragmatic reasoning could in principle explain empirical and intuitive effects regarding the relationship between figurative language and social intimacy.

\section{Introduction}

Everyday communication is full of creative and figurative uses of language. People often use nonliteral language such as hyperbole and sarcasm to communicate their attitudes, opinions, and subjective experiences of the world, repurposing language that usually means one thing (e.g. ``What a wonderful idea'') to express meanings that are completely different (e.g. ``That is a terrible idea''). The prevalence of figurative language is puzzling to rational accounts of communication. Why would a rational, cooperative speaker risk serious misunderstanding by producing utterances that are literally false? 
%Are there benefits for using figurative utterances that offset this risk and justify the use of nonliteral communication in certain situations?  

A body of work in pragmatics and psycholinguistics has suggested that there may be important benefits for using figurative utterances that can offset the risk of misunderstanding, such as humor, face protection, and effects of keeping exchanges ``off-record'' and thus less face-threatening \cite{roberts1994people, brown1987politeness, pinker2007evolutionary, jorgensen1996functions, brown1995politeness, dews1995not}. In addition, research has also shown interesting connections between nonliteral language and the social relationship between speaker and listener. Given that verbal irony requires a great deal of common ground to be interpreted appropriately \cite{clark1984pretense, gibbs1986psycholinguistics, pexman2004does}, \citeA{jorgensen1996functions} proposed that verbal irony could be used to strengthen close relationships by highlighting shared background between interlocutors. \citeA{kreuz1996use} made the logic behind this idea explicit by proposing the principle of inferability: people are more likely to use figurative language when they are confident that the audience will interpret it correctly. Several pieces of empirical evidence regarding verbal irony appear consistent with this principle. 
For example, people are more likely to use sarcasm with people they are close to \cite{kreuz1996use}; people judge sarcasm  as being more appropriate when it is used with close others \cite{jorgensen1996functions, kreuz1999tag}; and utterances are read faster and rated as more ironic when the interlocutors are described as having a close relationship \cite{kreuz2002asymmetries}. Together, these studies point to one possibility why people choose to communicate non-literally---to highlight the fact that their listeners have privileged knowledge that is required for successful nonliteral communication \cite{fowler1926dictionary}. In other words, figurative language may be a high-reward form of communication precisely because it is high-risk, and can be used only with select audiences.


%Jokes that are not recognized as jokes are socially painful.

%Other studies find that nonliteral language is more frequently and appropriately used between interlocutors who share a great deal of common ground, suggesting that speakers only communicate non-literally when the risk of miscommunication is low. 

While these studies lend support to the idea that signaling common ground may be one reason why people communicate non-literally, it is unclear precisely how listeners arrive at inferences about social intimacy based on a patently neutral linguistic input. In this chapter, we clarify how this process could work by describing a computational model that formalizes communication as recursive social reasoning between speaker and listener. We extend the Rational Speech-Acts model, which has previously been shown to predict ironic interpretations \cite{kao2015let}, to accommodate inferences about common ground between speaker and listener. We focus on verbal irony partly because it has generated the most empirical research regarding social implications, and partly because ironic utterances often run the highest risk of being interpreted literally (e.g. sarcastic utterances such as ``What a wonderful idea'' could be interpreted literally by an uninitiated listener, while hyperboles such as ``It's a million degrees outside'' are often too implausible under their literal semantics to be misinterpreted as literal).

In what follows, we use a series of model simulations to show that listeners can infer rich information about the beliefs and common ground shared with speakers when encountering sarcastic utterances. We also show that a rational speaker may choose to communicate non-literally in order to signal common ground to the listener. Finally, we discuss the implications that these findings may have on the social functions of nonliteral language.


%
%\begin{itemize}
%\item[-] Nonliteral language such as hyperbole and sarcasm is used a lot. This is strange because nonliteral utterances can very easily be misunderstood. Why would speakers take this risk?
%\item[-] Principle of inferability: speakers are more likely to use nonliteral language when they believe the listener will interpret it correctly (Kreuz, 1996).
%\item[-] Review literature on social intimacy and nonliteral language.
%\item[-] Informally describe how inferences about social intimacy and shared beliefs can arise from the principle of inferability
%\item[-] Explain why nonliteral language is especially informative regarding common ground: because without sufficient common ground, speaker would not choose to say something nonliteral, because speaker knows listener would interpret it literally. 
%\item[-] Describe RSA and how it's ideally suited for capturing the recursive social reasoning involved in nonliteral language understanding. Mention irony and hyperbole papers and how they capture the basic effects.
%\item[-] Sketch out how the paper will focus on hyperbolic utterances regarding prices, and why (e.g. simpler, interpretation is sensitive to prior beliefs, easy to manipulate quantitatively)
%\end{itemize}

\section{Modeling Social Inferences}
We explore a series of scenarios in which speaker and listener share varying degrees of common ground and reason about each other at varying levels of recursive depth. We use the model of verbal irony described in Chapter 3 as a starting point to show how increasingly sophisticated social inferences can arise as we extend the model to accommodate uncertainty about common ground and socially-oriented communicative goals. Instead of using weather scenarios as our example domain, here we introduce a domain that involves more subjectivity, where judgments about the same topic may vary dramatically across individuals. This inclusion of subjective beliefs allows us to explore the effect of mutual knowledge between speaker and listener on communication outcomes. 

Let us make this more concrete with an example. 
Suppose Ann and Bob just finished watching a mainstream blockbuster movie together. Regardless of Ann's personal opinion about the movie, she may know Bob well enough to know that he is likely to feel negatively about the movie, because he doesn't like blockbuster movies in general. She may not know exactly how negative Bob feels towards this particular movie, but prior to him saying anything, she knows that his distribution over judgements about this movie is skewed towards the negative. When Bob produces an utterance, for example, ``That movie was terrible,'' Ann interprets the utterance with respect to her knowledge of Bob's prior distribution. Because Bob's utterance is consistent with Ann's knowledge of his taste in movies, Ann interprets the utterance literally and infers that Bob believes the movie to be terrible. Even if Ann herself strongly believes that the movie is amazing, knowing Bob's prior disposition towards movies, she will not interpret his utterance as sarcastic. This is because she interprets the utterance not based on her own beliefs about the movie, but on her prior knowledge of \emph{Bob's} likely beliefs about the movie. This distinction between speaker's and listener's prior beliefs is important to note for our purposes. To make the terminology consistent, in what follows we will refer to the speaker's prior probability distribution for feeling a certain way about a movie as his ``taste profile.''
%respect to t(e.g. \emph{terrible}, \emph{bad}, \emph{ok}, \emph{good}, \emph{amazing}).
%
%Suppose you and a blind date just finished watching an action movie. You don't know your date's taste in movies and have no idea how your date feels about the movie you just watched. He turns to you and says, ``What a terrible movie.'' You receive two pieces of information from this utterance: your date feels very negative about the movie, and probably isn't fond of these types of movies in general.

\subsection{Perfect common ground}
We first revisit the case where the listener assumes herself to have perfect knowledge of the speaker's taste in movies, and also assumes that the speaker knows the listener to have perfect knowledge of his taste. In other words, the speaker's taste profile is in common ground. For simplicity, we assume that there are five different kinds of taste profiles about blockbuster movies in the population, as shown in Figure~\ref{movie-priors}. 



\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-priors.pdf}}
\caption{Five different ``taste profiles'' for mainstream blockbuster movies. Each panel is a taste profile, where the $x$ axis represents judgement about a given blockbuster movie. Taste 1 (The Hipster): very likely to judge a given blockbuster movie as strongly negative; Taste 2 (The Picky Person): very unlikely to judge a blockbuster movie as positive; Taste 3 (The Unbiased Person): equally likely to give a blockbuster movie any judgement; Taste 4 (The Tolerant Person): very unlikely to judge a blockbuster movie negatively; Taste 5 (The Mainstream Person): very likely to judge a blockbuster movie as strongly positive.}
\label{movie-priors}
\end{figure}

Following the qRSA model described in Chapter 3, the speaker chooses an utterance that most effectively communicates information regarding the question under discussion (QUD) to a literal listener, which could be his judgment of the movie or his emotional valence and arousal towards it. We consider a meaning space that consists of the variables $j$, $\vec A$, where $j$ is the judgment, and $\vec A$ represents the speaker's affect associated with the judgment.
Identical to the verbal irony model described in Chapter 3, the pragmatic listener $L_{1}$ takes into account the speaker's prior probability of judgments and affects as well as his internal model of the speaker to interpret the utterance. The only difference is that we specify the speaker's taste profile $t$, from which the listener derives the prior probability of the speaker's judgment $P(j | t)$.
We assume that people's taste profiles only affect the prior probability of having certain judgments towards mainstream movies. The probability of having certain affects given a judgment $P(\vec A | j)$ is the same across taste profiles. 
%
\begin{equation}
L_{1}(j,  \vec A| u, t) \propto P(j | t) P(\vec A | j) \sum_{q}{P (q) S_1(u | j, \vec A, q, t)}
\label{taste1}
\end{equation}
%
Suppose the listener knows that the speaker's taste profile is Taste 1: Hipster. In this case, the speaker is \emph{a priori} very likely to judge a blockbuster movie as \emph{terrible}, moderately likely to judge it as \emph{bad}, and unlikely to judge it as \emph{ok, good}, or \emph{amazing}. 
Given common knowledge that the speaker's taste profile is such that he is \emph{a priori} highly likely to judge a mainstream blockbuster movie to be \emph{terrible}, the listener will interpret the speaker's utterance ``That movie was amazing'' sarcastically to mean ``That movie was terrible'' (Figure~\ref{movie-knowpriors}).
Overall, this model is able to reason about the speaker's communicative goals and use common knowledge about his taste profile to produce interpretations that are consistent with intuition. 

\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-knowpriors.pdf}}
\caption{Interpretation of utterances given common knowledge that the speaker's taste profile is ``Taste 1: Hipster'' (see Fig.~\ref{movie-priors}). Each panel is the interpretation of an utterance. The $x$ axis represents the speaker's judgment.}
\label{movie-knowpriors}
\end{figure}

\subsection{Inferring speaker's priors}
In the last section, we showed that the model uses common ground regarding the speaker's taste profile to produce sarcastic and otherwise nonliteral interpretations. 
%If the listener had believed that the speaker's taste profile was Taste 3, 4, or 5 instead of Taste 1, her interpretation of the utterance ``That movie was amazing'' would have been literal. 
In this section, we examine a situation where the listener has no prior knowledge of the speaker's taste profile, and explore the inferences that she makes given different utterances.

Suppose Amy and her blind date Ben just finished watching a mainstream blockbuster movie. Amy doesn't know Ben's taste in movies and has no idea how he might feel about the movie. Ben turns to Amy and says, ``That movie was amazing.'' Intuitively, Amy receives two pieces of information from this utterance: Ben judges the movie to be \emph{amazing}, and Ben's taste in movies is probably such that he is fond of these types of movies in general\footnote{Another possibility is that Ben is simply trying to be agreeable and polite; however, that is a different type of social goal that is beyond the scope of this chapter.}.
%
To model these intuitive inferences about Ben's judgment and his taste profile, we introduce a simple modification to Equation \ref{taste1}: 
\begin{equation}
L_{1}(j, \vec A, t | u) \propto P(t) P(j | t) P(\vec A | j) \sum_{q}{P (q) S_1(u| j, ]vec A, q, t)}
\end{equation}
Instead of being known to the listener, the variable $t$ now needs to be inferred given the utterance. Note that the speaker's choice of utterance $S_1(u| j, \vec A, q, t)$ also depends on the listener's belief about his taste profile, $t$.
Since Amy has no prior knowledge about her blind date's taste profiles, $P(t)$ is uniform. 

Figure~\ref{movie-noprior-interp} shows Amy's inferences about Ben's judgement about the movie given different utterances he says. Across all five utterances, given no prior knowledge of Ben's taste profile, Amy is most likely to interpret each utterance as literal, such that ``That movie was amazing'' is interpreted as literally meaning that Ben judges the movie to be amazing. We can also query the model for Amy's inferences about Ben's taste profile given each utterance (Figure~\ref{movie-noprior-prior}). If Ben says ``That movie was amazing,'' Amy will end up with the posterior belief that Ben is likely to have a mainstream taste profile.

Contrasting Amy's interpretation of the utterance ``That movie was amazing'' in Figure~\ref{movie-knowpriors} with the interpretation of the same utterance in Figure~\ref{movie-noprior-interp}, we find that extending the model to accommodate inferences about the speaker's taste profile reveals an interesting intuition about figurative utterances. Unless Ben is highly confident that Amy knows his taste profile to be ``hipster,'' it is unwise for him to say ``That movie was amazing'' and expect to be interpreted sarcastically, because Amy would end up with both the wrong interpretation and the wrong inference about his tastes. This model result is consistent with the finding that people are more likely to use verbal irony with close others, who presumably have more knowledge about the speakers' prior dispositions \cite{kreuz1996use}.

More interestingly, contrasting the model described here (where the listener has no prior knowledge of the speaker's taste profile) and the one in the previous section (where the speaker's taste profile is in common ground), we can reason as follows. If the listener has no prior knowledge of the speaker's taste profile, she will interpret utterances literally. If the speaker's taste profile is in common ground, however, the listener will produce appropriate nonliteral interpretations. As a rational speaker, Ben should only say ``That movie was amazing'' sarcastically if he believes his taste profile to be in common ground. In the next section, we allow the pragmatic listener to reason about the speaker at a higher level of recursion and show that she is able to use this intuition to produce inferences about the speaker's beliefs about common ground given his utterance.
 
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-noprior-interp.pdf}}
\caption{Interpretation of utterances given an uninformative prior over the speaker's taste profiles. Each panel is the interpretation of an utterance. The $x$ axis represents the speaker's judgment. The most likely interpretation of each utterance is the literal one.}
\label{movie-noprior-interp}
\end{figure}
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-noprior-prior.pdf}}
\caption{Inferences about the speaker's taste profile given different utterances. Each panel represents the posterior distribution over the speaker's taste profiles given an utterance, where the $x$ axis specifies the taste profile described in Fig.~\ref{movie-priors}.}
\label{movie-noprior-prior}
\end{figure}


\subsection{Inferring speaker's beliefs about common ground}

Suppose Alice and her husband Bill just finished watching a mainstream blockbuster movie. 
Alice knows Bill's taste in movies very well. She is certain that his taste profile is best described by ``Taste 1: Hipster,'' which means he is likely to judge the movie to be terrible. Bill turns to Alice and says, ``That movie was amazing.'' 

In this scenario, Alice could do one of two things. She could interpret the utterance literally, which would require her to revise her beliefs about Bill's taste in movies and question how well she really knows her husband of several decades. Alternatively, Alice could interpret Bill's utterance as sarcastic, which seems intuitively more likely. Interpreting Bill's utterance sarcastically has several implications. If Bill indeed intended to be sarcastic, and if he is a rational, cooperative speaker, he would only choose to be sarcastic with the assumption that Alice knows his taste profile well enough to interpret his utterance correctly. This leads Alice to believe that Bill knows that she knows his taste profile---in other words, she will believe that Bill believes his taste profile to be in common ground. Given such an utterance, Alice still ends up with the initial belief that Bill has a hipster's taste profile; however, she now gains important information about Bill's confidence in her knowledge of him. 

\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-infercg-interp.pdf}}
\caption{Interpretation of utterances given the listener knows that the speaker's taste profile is ``Taste 1: Hipster,'' but with uncertainty about whether this knowledge is in common ground. Each panel represents an utterance.}
\label{movie-infercg-interp}
\end{figure}
%
\begin{figure}[h]
\centering
\scalebox{0.7}{\includegraphics{Figures/movie-infercg-cg.pdf}}
\caption{Inferences about the probability that the speaker believes his taste profile to be in common ground given different utterances. Each panel represents an utterance. The prior probability of common ground is set to be $0.5$.}
\label{movie-infercg-cg}
\end{figure}

Let us formalize this intuition by further extending the model described in the last section. Because Alice now reasons about Bill reasoning about her pragmatic interpretation, Alice needs to reason one level further than Ann did in the last section, and will be modeled as an $L_2$ listener\footnote{Although higher levels of recursion have not been necessary in previous RSA models, given that the phenomenon we are interested in is a higher-order social inference derived from pragmatic interpretation, an $L_2$ level model seems warranted.}. We now introduce a boolean variable $c$, which indicates whether the speaker believes his taste profile to be in common ground with the listener. This variable allows us to capture the intuition that while Alice is certain that Bill has a particular taste profile $t$, she is uncertain about whether Bill believes his taste profile to be in common ground. Alice's inferences can be described by the following equation:
%
%\begin{equation}
%L_{2}(j, A, \zeta | u, t) \propto P(j | t) P( A | j) P(\zeta) \sum_{q}{P (q) S_2(u| j, A, q, t')}
%\end{equation},
%where $t' = t$ if $\zeta = 1$.
%
\begin{equation}
L_{2}(j, \vec A, c | u, t) \propto \left\{ 
  \begin{array}{l l}
    P(c) P(j | t) P( \vec A | j) \sum_{q}{P(q) S_2(u| j, \vec A, q, t)}
 & \quad \text{if $c = 1$}\\
    P(c) P(j | t) P( \vec A | j) \sum_{t', q}{P(t')P(q) S_2(u| j, \vec A, q, t')} & \quad \text{if $c = 0$}
  \end{array} \right.
 \end{equation}
%
where Alice's belief about Bill's taste profile $t$ is passed down and used by $S_2$ if Bill believes it to be in common ground, and is resampled uniformly as $t'$ from all five possible taste profiles if not. In each case, $L_2$ reasons about $S_2$'s choice of utterance given $L_1$'s beliefs about Bill's taste profile.

Figure~\ref{movie-infercg-interp} shows $L_2$'s interpretation of each utterance given knowledge of the speaker's taste profile, but with uncertainty about whether the speaker believes it to be in common ground. We see that $L_2$ again successfully interprets the utterance ``That movie was amazing'' as sarcastically meaning that the speaker thinks the movie was terrible. More interestingly, $L_2$ can also make inferences about the variable $c$---whether the speaker believes his taste profile to be in common ground---given different utterances (Figure~\ref{movie-infercg-cg}). With the prior probability of common ground set as $0.5$, $L_2$ infers a higher posterior probability that the speaker believes his taste profile to be in common ground given the sarcastic utterance ``That movie was amazing,'' and a lower posterior probability of common ground given the literal utterance ``That movie was terrible.'' This results in a formal derivation of the the social inferences licensed by nonliteral, sarcastic uses of language: a strengthened belief that the speaker believes his taste profile to be in common ground, which in turn implies that he believes his relationship with the listener to be close. 

\subsection{Communicating beliefs about common ground}

Thus far in this chapter, we have shown the inferences that listeners can make given literal or sarcastic utterances. In this section, we explore the motivations that could lead a speaker to choose a sarcastic versus a literal utterance. 
%In the last section, we showed how a pragmatic listener $L_2$ makes inferences about common ground given various utterances. 
In particular, we show how a higher-level pragmatic speaker $S_3$ could choose utterances in order to communicate common ground to the listener, specified as follows:
\begin{equation}
S_3(u | j, \vec A, t, c) \propto e^{\lambda U_3( u |  j, \vec A, t, c)}
\end{equation}
where the utility function is defined as the negative surprisal of $c$ under $L_2$'s interpretation distribution:
\begin{equation}
U_3(u | j, \vec A, t, c) = log L_2(c | u, t)
\end{equation}
Intuitively, $S_3$ chooses an utterance to maximize the probability that $L_2$ will infer the correct common ground variable $c$ given the speaker's taste profile. Figure~\ref{movie-communicate-cg} shows that the speaker is more likely to choose a nonliteral utterance (in this case, a positive utterance) in order to communicate his belief that his taste profile is in common ground (in this case, a taste profile that is strongly skewed towards negative judgments).

\begin{figure}
\centering
\scalebox{0.6}{\includegraphics{Figures/movie-communicate-cg.pdf}}
\caption{Probability that speaker would choose an utterance, given whether he wants to communicate to the listener that he believes his taste profile (Taste 1: Hipster) is in common ground. The speaker is more likely to choose nonliteral utterances when he wants to communicate to the listener that his taste profile is in common ground.}
\label{movie-communicate-cg}
\end{figure}



%\subsection{A note on paralinguistic cues}

\section{Discussion}
In this chapter, we described a series of model simulations that show how inferences about common ground and social intimacy could be both a natural consequence of and a motivation for using figurative language. The models we present contain natural but critical extensions to the RSA models introduced in earlier chapters, highlighting the flexibility and generality of the modeling framework. The recursive nature of RSA models allows us to demonstrate how uncertainty about common ground could affect language interpretation, and in turn how interpretation constrains inferences about common ground. 

While these models shed light on the social motivations for nonliteral communication and indirect language more generally, much work remains to be done to elucidate the details of these social inferences and the specific contexts that facilitate them. In the simulations presented here, we chose simple and idealized scenarios where the listener either has perfect knowledge of the speaker's taste profile or no knowledge at all. In real-world situations, listeners are usually somewhere in between, juggling beliefs about speakers while simultaneously reasoning about speakers' beliefs about \emph{them}, all under a great deal of uncertainty. It is possible that under these circumstances, other social and linguistic cues are necessary to guide listeners' inferences about what speakers really mean. Indeed, these cues are abundant in many exchanges involving figurative language \cite{clark1984pretense, caucci2012social}. In the scenario with Amy and her blind date Ben, Ben could accompany the utterance ``That movie was amazing'' with an eye-roll and a laugh, which would signal to Amy that he is being playful and sarcastic, even if she has no prior knowledge about his taste profile. The specific ways in which such paralinguistic cues factor into pragmatic inferences remains to be examined in future research. On the other hand, our models suggest that common ground allows speakers to use figurative language successfully without the aid of paralinguistic cues, which predicts that sarcastic utterances are most often produced in a ``deadpan,'' unmarked manner among very close friends. This prediction is supported by anecdotal evidence, and also by NLP research showing that explicit markers such as $\#$sarcasm are used less often between Twitter users who are socially intimate \cite{bamman2015contextualized}. Without detailed behavioral data to evaluate our models, we can only speculate that the kind of pragmatic reasoning described here is at least partially responsible for the range of phenomena we see in figurative communication in the wild.

%Another limitation our 
%Summary of findings. 
%
%Note limitations: this is a ``how possibly could'' account of these types of social inferences. Need more detailed, quantitative comparisons with empirical data to draw stronger conclusions that this is indeed a reasonable computational-level description of what people do. 
%
%Introducing higher levels of reasoning: does this make claims that it is harder/takes longer to draw such conclusions? Paralinguistic cues? 

%Pretense theory?
Finally, our models make an interesting connection to pretense theory, a classic theory of verbal irony. \citeA{clark1984pretense} first described the pretense theory of irony and attributed its roots to \citeA{grice20134}, who wrote: 
``To be ironical is, among
other things, to pretend (as the etymology suggests),
and while one wants the pretense to be
recognized as such, to announce it as a pretense
would spoil the effect" (p. 125). 
\citeA{clark1997dogmas} describes pretense as a kind of ``layering,'' where the speaker describes and refers to layers of action that are not grounded in the present reality, as in the case in story-telling and other kinds of performances \cite{clark1996using}. We can imagine that the recursive reasoning at the heart of RSA models represents a type of layering, where higher-level agents simulate the behaviors and interpretations of lower-level agents who are performing actions at different layers. When a higher-level speaker produces a sarcastic utterance such as ``That movie was amazing,'' he is simulating (or pretending to be) a person who would find the movie to be amazing, and imagining a listener who would interpret the utterance literally. He also recognizes that a higher-level pragmatic listener would use her knowledge of the speaker to interpret the utterance sarcastically and ``be in on'' the joint pretense. While the connection at the moment is admittedly tenuous, the RSA modeling framework may be able to formalize the ideas behind pretense theory in an intuitively satisfying manner.

We believe the work described here is a promising step towards linking creative language use with social motivations such as increasing closeness and solidarity. Building upon earlier models of pragmatic reasoning and introducing natural extensions through inferences about common ground, this work provides a unified computational basis for a range of social effects in figurative language use. By reasoning about speakers' communicative goals and beliefs about common ground, the RSA model shows how figurative language could accomplish one of the most important goals in communication of all---bringing people closer together. 


\chapter{A Computational Model of Humor in Puns}

Creative uses of language are associated with several desirable consequences. In Chapter 4, I used the Rational Speech-Acts modeling framework to explore how figurative language could communicate speakers' beliefs about common ground and social intimacy. In this chapter, I will use a different modeling approach to address a distinctive consequence of creative language: humor. 
%\section{Abstract}
While humor plays an essential role in human interactions, precisely what makes something funny remains elusive. 
%While research on natural language understanding has made significant advancements in recent years, there has been little direct integration of humor research with computational models of language understanding. 
Here we use a simple model of sentence processing to derive 
two information-theoretic measures---ambiguity and
distinctiveness. We
test these measures on a set of puns and regular sentences and show that
they correlate significantly with human judgments of funniness.
Moreover, within a set of puns, the distinctiveness measure
distinguishes exceptionally funny puns from mediocre ones. Our work is
the first, to our knowledge, to integrate a computational model of
general language understanding and humor theory to quantitatively
predict humor at a fine-grained level. We present it as an example of a
framework for applying models of language processing to understand
the higher-order effects of creative language use\footnote{This chapter is based on \citeA{kao2015computational}}.

\section{Introduction}

Love may make the world go round, but humor is the glue that keeps it
together. Our everyday experiences serve as evidence that humor plays a
critical role in human interactions and composes a significant part of
our linguistic, cognitive, and social lives. Previous research has shown
that humor is ubiquitous across cultures \cite{martin2010psychology, kruger1996nature},
increases interpersonal attraction \cite{lundy1998heterosexual}, helps resolve intergroup conflicts \cite{smith2000resolving},
and improves psychological wellbeing \cite{martin1993humor}. However, little is known about the cognitive basis of such a
pervasive and enjoyable experience. By providing a formal model of
linguistic humor, we aim to solve part of the mystery of what makes us
laugh.

Theories of humor have existed since the time of Plato and Aristotle
(see \citeA{attardo1994linguistic} for review). A leading theory in modern research
posits that incongruity, loosely characterized as the presence of
multiple incompatible meanings in the same input, may be critical for
humor \cite{veale2006computability, forabosco1992cognitive, hurley2011inside, macghee1979humor, vaid2001laughter}. However, despite relative consensus on the importance of
incongruity, definitions of incongruity vary across informal analyses of
jokes. As \citeA{ritchie2009variants} wrote, ``There is still not a rigorously
precise definition that would allow an experimenter to objectively
determine whether or not incongruity was present in a given situation or
stimulus'' (p. 331). This lack of precision makes it difficult to
empirically test the role of incongruity in humor or extend these ideas
to a concrete computational understanding. On the other hand, most work
on computational humor focuses either on joke-specific templates and
schemata \cite{binsted1996machine, taylor2004computationally} or surface features
and properties of individual words \cite{mihalcea2006learning, kiddon2011s, reyes2012humor}. One exception is \citeA{mihalcea2010computational}, which used features inspired by incongruity
theory to detect humorous punch lines; however, the incongruity features
proposed did not significantly outperform a random baseline, leading the
authors to conclude that joke-specific features may be preferable. While
these dominant approaches in computational humor are able to identify
humorous stimuli within certain constraints, they fall short of testing
a more general cognitive theory of humor.

In this work, we suggest that true measures of incongruity in linguistic
humor may require a model that infers meaning from words in a principled
manner. We build upon theories of humor and language processing to
formally measure the multiplicity of meaning in puns -\/- sentences ``in
which two different sets of ideas are expressed, and we are confronted
with only one series of words,'' as described by Philosopher Henri
Bergson \cite{bergson1914laughter}. Puns provide an ideal test bed for our purposes
because they are simple, humorous sentences with multiple meanings. Here
we focus on phonetic puns, defined as puns containing words that sound
identical or similar to other words in English.
%\footnote{An early version of this work appeared in the proceedings of the 35\textsuperscript{th}Annual Meeting of the Cognitive Science Society. In this extended paper, we examine a wider range of sentences, including puns that contain identical homophones as well as puns with words that sound similar (but not identical) to other words in English.}
The following
is an example:

(1) ``The magician got so mad he pulled his hare out.''

Although the sentence's written form unambiguously contains the word
``hare,'' previous work has suggested that phonetic representations play
a central role in language comprehension even during reading \cite{niznikiewicz1996phonological, pexman2001homophone, pollatsek1992phonological}. Taking the lexical ambiguity of its phonetic form into account,
this sentence thus implicitly expresses two ``ideas,'' or
meanings\footnote{In this work we focus on written sentences that
  contain phonetic ambiguity. In the future, it would be interesting to
  examine humorous effects in spoken sentences, where ambiguity cannot
  be partially resolved by the orthographic form.}:

(1a) The magician got so mad he performed the trick of pulling a rabbit
out of his hat.

(1b) The magician got so mad he pulled out the hair on his head.

At the most basic level, the humor in this pun relies on the fact that
it contains the word ``hare,'' which is phonetically confusable with
``hair.'' However, the following sentence also contains a phonetically
ambiguous word, but is clearly not a pun:

(2) ``The hare ran rapidly across the field.''

A critical difference between (1) and (2) is that \emph{hare} and
\emph{hair} are both probable meanings in the context of sentence (1),
whereas \emph{hare} is much more likely than \emph{hair} in sentence
(2). From this informal analysis, it seems that both meanings are
compatible with context in a phonetic pun, suggesting that a sentence
must contain ambiguity to be funny. However, another example shows that
ambiguity alone is insufficient. Consider the sentence:

(3) ``Look at that hare.''

This sentence is also ambiguous between \emph{hare} and \emph{hair}, but
is unlikely to elicit chuckles. A critical difference between (1) and
(3) is that while each meaning is strongly supported by distinct groups
of words in (1) (\emph{hare} is supported by ``magician'' and ``hare'';
\emph{hair} is supported by ``mad'' and ``pulled''), both meanings are
weakly supported by all words in (3). This comparison suggests that in
addition to ambiguity, distinctiveness of support may also be an
important criterion for humor. Observations on the putative roles of
ambiguity of sentence meaning and distinctiveness of support will
motivate our formal measures of humor. \footnote{Note that it is not
  necessary for both meanings to be completely compatible with the full
  context, as illustrated by puns such as \emph{I used to be addicted to
  soap, but I'm clean now}, in which the most common meaning of
  \emph{clean} is actually ruled out, rather than supported, by full
  compositional interpretation of the context. What instead seems
  necessary is that the support derived from the subset of context for
  each meaning is balanced.}

How should we represent the meaning of a sentence in order to measure
its ambiguity and distinctiveness? While formally representing sentence
meanings is a complex and largely unsolved problem \cite{grefenstette2014concrete, socher2012semantic, liang2013learning}, we can utilize certain
properties of phonetically ambiguous sentences to simplify the problem.
We notice that in sentence (1), meaning (1a) arises if the word ``hare''
is interpreted as \emph{hare}, while meaning (1b) arises if ``hare'' is
interpreted as its homophone \emph{hair}. Each sentence-level meaning
directly corresponds to the meaning of a phonetically ambiguous word. As
a result, we can represent sentence meaning (1a) with \emph{hare} and
(1b) with \emph{hair.} This approximation is coarse and captures only
the ``gist'' of a sentence rather than its full meaning. However, we
will show that it is sufficiently powerful for modeling the
interpretation of sentences with only a phonetic ambiguity.

Given the space of candidate sentence meanings, a comprehender's task is
to infer a distribution over these meanings from the words she observes.
Formally, a phonetically ambiguous sentence such as (1) is composed of a
vector of words
\(\overrightarrow{w} = \ \left\{ w_{1},\ldots,\ w_{i},\ h,\ \ w_{i + 1},\ldots,\ \ \ w_{n} \right\}\),
where \emph{h} is phonetically confusable with its homophone \emph{h'}.
The sentence meaning is a latent variable \(m\), which we assume has two
possible values \(m_{a}\)and \(m_{b}\). These sentence meanings can be
identified with \emph{h} and \emph{h'}, respectively. Consistent with a
noisy channel approach \cite{levy2008noisy, levy2009eye, gibson2013rational}, we construe the task of understanding a sentence as inferring
\(m\) using probabilistic integration of noisy evidence given by
\(\overrightarrow{w}\). We construct a simple probabilistic generative
model that captures the relationship between the meaning of a sentence
and the words that compose it (Figure \ref{pun-bayes-net}). If a word is semantically
relevant (\(f_{i} = 1\)), we assume that it is sampled based on semantic
relatedness to the sentence meaning; if the word is irrelevant, or
``noise,'' it only reflects general language statistics and is sampled
from an n-gram model. Because the comprehender maintains uncertainty
about which words are relevant, it is possible for her to arrive at
multiple interpretations of a sentence that are each coherent but
incongruous with one another, a situation that we hypothesize gives rise
to humor. To capture this intuition, we introduce two measures of humor
derived from the distribution over sentence meanings (details in Methods
section).
\begin{figure}
\centering
\scalebox{0.3}{\includegraphics{Figures/pun-figure1.png}}
\caption{Graphical representation of a generative model of a sentence.
If the indicator variable \(f_{i}\) has value 1, \(w_{i}\) is generated
based on semantic relatedness to the sentence meaning \(m\); otherwise,
\(w_{i}\) is sampled from a trigram language model based on the
immediately preceding two words.}
\label{pun-bayes-net}
\end{figure}

Given words in a sentence, we infer the joint probability distribution
over sentence meanings and semantically relevant words, which can be
factorized into the following:

\begin{equation}
P( m,\vec{f}\ |\vec{w}) = P(m\ | \vec{w})P(\vec{f}  | m, \vec{w} )
\end{equation}
We compute a measure of humor from each of the two terms on the
right-hand side. Ambiguity is quantified by the entropy of the
distribution $P( m | \vec{w})$. If entropy is
high, then the sentence is ambiguous because both meanings are
near-equally likely. Distinctiveness captures the degree to which the
relevant words differ given different sentence meanings. Given one
meaning $m_{a}$, we compute
$
F_{a} = P(\vec{f}\ |\ m_{a},\vec{w}).
$
Given another meaning \(m_{b}\), we compute
$
F_{b} = P(\vec{f}\ |\ m_{b},\vec{w}).
$
Distinctiveness is quantified by the symmetrized Kullback-Leibler
divergence between these two distributions, $D_{\text{KL}}(F_{a}\ || F_{b}) + D_{\text{KL}}(F_{b} \||F_{a})$. If the symmetrized
KL distance is high, it suggests that the two sentence meanings are
supported by distinct subsets of words in the sentence. Derivation
details of these two measures are in the Methods section below. We
empirically evaluate ambiguity and distinctiveness as predictors of
humor in a set of phonetically ambiguous sentences.

\section{Methods}

\subsection{Computing model predictions}

We define the ambiguity of a sentence as the entropy of
$P(m | \vec{w})$,
where $\vec{w}$ is a vector of observed content words in a
sentence (which contains a phonetically ambiguous word $h$ and $m$
is the latent sentence meaning. Given the simplifying assumption that
the distribution over sentence meanings is not affected by function
words, each $w_{i}$ in $\vec{w}$ is a content word. The
distribution over sentence meanings given words can be derived using
Bayes' rule:

\begin{equation}
\begin{split}
P(m | \vec{w}) = \sum_{\vec{f}}P( m, \vec{f} | \vec{w} ) \\
\propto \sum_{\vec{f}}P ( \vec{w} | m, \vec{f}) P(m)P(\vec{f}) \\
= \sum_{\vec{f}}\left( P( m)P(\vec{f})\prod_{i}^{}{P ( w_{i}  |m, f_{i})}\right)
\end{split}
\end{equation}

Each value of $m$ is approximated by either the meaning of the
observed phonetically ambiguous word $h$ (e.g. ``hare'' in sentence
(1)) or its unobserved homophone $h'$ (e.g. ``hair''). We can thus
represent $P(m)$ as the unigram frequency of $h$ or $h'$. For
example, $P(m = hare)$ is approximated as proportional to
$P(\text{``hare''})$. We assume equal prior probability that each
subset of the words is semantically relevant, hence
$P(\vec{f})$ is a constant. $P(w_{i}|m,\ f_{i})$ depends
on the value of the semantic relevance indicator variable $f_{i}$. If
$f_{i} = 1$, $w_{i}$ is semantically relevant and is sampled in
proportion to its relatedness with the sentence meaning $m$. If
$f_{i} = 0$, then $w_{i}$ is generated from a noise process and
sampled in proportion to its probability given the previous two words in
the sentence. Formally,

\begin{equation}
    P(w_i | m, f_i) =
    \begin{cases}
      P(w_i | m) & \text{if} f_i=1 \\
      P(w_i | \text{bigram}_i) & \text{if} f_i = 0
    \end{cases}
  \end{equation}


We estimate $P( w_{i} | m )$ using empirical
association measures described in the Experiment section and compute
$P ( w_{i}  | \text{bigram}_{i} )$ using the Google
N-grams corpus (Brants \& Franz, 2006). Once we derive
$M = P(m | \vec{w})$, we compute its
information-theoretic entropy as a measure of ambiguity:
\begin{equation}
\text{Ambiguity}(M) = - \sum_{k \in \{ a,b\}}P( m_{k} | \vec{w} )\log P( m_{k} | \vec{w} )
\end{equation}

We next compute the distinctiveness of words supporting each sentence
meaning. Using Bayes' Rule:
\begin{equation}
P( \vec{f}  | m, \vec{w}) \propto P ( \vec{w}  |m,\vec{f})P(\vec{f} | m )
\end{equation}
Since $\vec{f}$ and $m$ are independent,
$P (\vec{f} | m) = P(\vec{f})$,
which is a constant. Let
$F_{a} = P (\vec{f} | m_{a}, \vec{w})$
and
$F_{b} = P( \vec{f} | m_{b}, \vec{w})$.
We compute the symmetrized Kullback-Leibler divergence score
$D_{\text{KL}}(F_{a} || F_{b}) + D_{\text{KL}}(F_{b}|| 
F_{a})$, which measures the difference between the distribution of
supporting words given one sentence meaning and the distribution of
supporting words given another sentence meaning. This results in the
distinctiveness measure\footnote{In addition to the symmetrized KL
  divergence of Eq. 6, we also experimented with non-symmetrized KL
  divergence in both directions and found qualitatively identical
  results.}:
\begin{equation}
\text{Distinctiveness}(F_{a}, F_{b}) = \sum_{i} \left (\ln \left (\frac{F_a(i)}{F_b(i)} \right ) F_a(i) + \ln \left (\frac{F_b(i)}{F_a(i)} \right ) F_b(i) \right )
\end{equation}
Given these derivations, we conducted the following experiment to
implement and test the ambiguity and distinctiveness measures.

\subsection{Experiment}

We collected 435 sentences consisting of phonetic puns and regular
sentences that contain phonetically ambiguous words. We obtained the
puns from a website called ``Pun of the Day''
(\url{http://www.punoftheday.com}), which at the time of collection contained
over a thousand puns submitted by users. We collected 40 puns where the
phonetically ambiguous word has an identical homophone, for example
``hare.'' Since only a limited number of puns satisfied this criterion,
a research assistant generated an additional 25 pun sentences based on a
separate list of homophone words, resulting in a total of 65
identical-homophone puns. We selected 130 corresponding non-pun
sentences from an online version of Heinle's Newbury House Dictionary of
American English (\url{http://nhd.heinle.com}). 65 of the non-pun
sentences contain the ambiguous words observed in the pun sentences
(e.g. ``hare''); the other 65 contain the unobserved homophone words
(e.g. ``hair'')\footnote{Results for the 195 identical homophone
  sentences were reported in \citeA{kao2013funny}, which was published in
  the proceedings of the 35\textsuperscript{th} Annual Meeting of the
  Cognitive Science Society (a non-archival publication).}. To test
whether our measures generalize to sentences containing phonetically
ambiguous words that do not have identical homophones, we collected 80
puns where the phonetically ambiguous word sounds similar (but not
identical) to other words in English (e.g. ``tooth'' sounds similar to
``truth''). We also collected 160 corresponding non-pun sentences. Table~\ref{pun-examples} shows an example sentence from each category\footnote{The full set of
sentences can be found here:
\url{http://web.stanford.edu/~justinek/pun-paper/results.html}}.

\begin{table}
\begin{longtable}{lll}
\toprule
\textbf{Homophone} & \textbf{Type} & \textbf{Example}\tabularnewline
\midrule
Identical & Pun & The magician was so mad he pulled his hare
out.\tabularnewline
Identical & Non-pun & The hare ran rapidly across the
field.\tabularnewline
Identical & Non-pun & Some people have lots of hair on their
heads.\tabularnewline
Near & Pun & A dentist has to tell a patient the whole
tooth.\tabularnewline
Near & Non-pun & A dentist examines one tooth at a time.\tabularnewline
Near & Non-pun & She always speaks the truth.\tabularnewline
\bottomrule
\end{longtable}
\caption{Example sentence from each category. Identical homophone
sentences contain phonetically ambiguous words that have identical
homophones; near homophone sentences contain phonetically ambiguous
words that have near homophones. Pun sentences were selected from a pun
website; non-pun sentences were selected from an online dictionary (see
main text for details).}
\label{pun-examples}
\end{table}


We obtained funniness ratings for each of the 435 sentences. 100
participants on Amazon's Mechanical Turk\footnote{The sample sizes were
  chosen such that each sentence would receive roughly 20-30 funniness
  ratings, in order for the uncertainty in funniness measurement to be
  reasonably low, while keeping the number of sentences rated by each
  participant manageably small.} rated the 195 sentences that contain
identical homophones. Each participant read roughly 60 sentences in
random order, counterbalanced for the sentence types, and rated each
sentence on funniness (``How funny is this sentence?'') on a scale from
1 (not at all) to 7 (extremely). We removed 7 participants who reported
a native language other than English and z-scored the ratings within
each participant. A separate group of 160 participants on Mechanical
Turk rated the 240 near homophone sentences. Each participant read 40
sentences in random order, counterbalanced for the sentence types, and
rated each sentence on funniness on a scale from 1 to 7. We removed 4
participants who reported a native language other than English and
z-scored the ratings within each participant. We used the average
z-scored ratings across participants as human judgments of funniness for
all 435 sentences.

As described in the measure derivations, computing ambiguity and
distinctiveness requires the conditional probabilities of each word
given a sentence meaning, i.e. \(P\left( w_{i}\  \right|\ m)\). In
practice, this value is difficult to obtain reliably and accurately in
an automated way, such as through WordNet distances or semantic vector
space models \cite{gabrilovich2007computing, zhang2011harnessing, mihalcea2010computational}\footnote{We experimented with computing these
  values from corpora in early stages of this work. However, we found
  that it is difficult to obtain reliable co-occurrence statistics for
  many word pairs of interest (such as ``hare'' and ``magician''), due
  to the sparsity of these topics in most corpora. Future work could
  further explore methods for extracting these types of
  commonsense-based semantic relationships from corpus statistics.}.
Instead of tackling the challenging problem of automatically learning
\(P\left( w_{i}\  \right|\ m)\) from large corpora, we observe that
\(P\left( w_{i}\  \right|\ m)\) is related to point wise mutual
information (PMI) between \(w_{i}\) and \(m\), an information-theoretic
measure defined mathematically as the following:
\begin{equation}
\log\frac{P(w_{i},\ m)}{P\left( w_{i} \right)P(m)} = \log{P\left( w_{i} \middle| m \right) - \log{P(w_{i})}}
\end{equation}
Intuitively, PMI captures the relatedness between \(w_{i}\) and \(m\),
which can be measured empirically by asking people to judge the semantic
relatedness between two words. This allows us to harness people's rich
knowledge of the relationships between word meanings without relying
solely on co-occurrence statistics in corpora. We assume that the
z-scored human ratings of relatedness between two words,
denoted\(\text{\ R}\left( w_{i},\ m \right)\), approximates true PMI.
With the proper substitutions and transformations\footnote{By assuming
  \(R\left( w_{i},\ m \right) = \log\frac{P(w_{i},\ m)}{P\left( w_{i} \right)P(m)}\),
  we get
  \(R\left( w_{i},\ m \right) = \log{P\left( w_{i} \middle| m \right) - \log{P(w_{i})}}\)
  from Eq. 7; exponentiating both sides gives us Eq. 8.} from Eq. 7, we
derive the following:
\begin{equation}
P\left( w_{i}\  \right|\ m) = \ e^{R\left( w_{i},\ m \right)}P\left( w_{i} \right)
\end{equation}
To obtain \(R\left( w_{i},\ m \right)\) for each of the words in the
stimuli sentences, function words were removed from each of the
sentences in our dataset, and the remaining words were paired with the
phonetically ambiguous word \emph{h} and its homophone \emph{h'} (e.g.,
for the pun in Table, {[}``magician'', ``hare''{]} is a legitimate word
pair, as well as {[}``magician'', ``hair''{]}). This resulted in 1460
distinct word pairs for identical homophone sentences and 2056 word
pairs for near homophone sentences. 200 participants on Amazon's
Mechanical Turk rated the semantic relatedness of word pairs for
identical homophone sentences. Each participant saw 146 pairs of words
in random order and were asked to rate how related each word pair is
using a scale from 1 to 10. We removed 5 participants who reported a
native language other than English. A separate group of 120 participants
rated word pairs for near homophone sentences. We removed 2 participants
who reported a native language other than English. Since it is difficult
to measure the relatedness of a word with itself, we assume that it is
constant for all words and treat it as a free parameter, r. After
computing our measures, we fit this parameter to people's funniness
judgments (resulting in $r = 13$). We used the average z-scored
relatedness measure for each word pair to obtain
\(R\left( w_{i},\ m \right)\) and Google Web unigrams to obtain
\(P(w_{i})\). This allowed us to compute \(P\left( w_{i}\  \right|\ m)\)
for all word and meaning pairs.

\section{Results}

We computed an ambiguity and distinctiveness score for each of the 435
sentences (see Methods). We found no significant differences between
identical and near homophone puns in terms of funniness ratings
($t(130.91) = 0.13, p = 0.896$), ambiguity scores ($t(137.80) = 1.13, p =
0.261$), and distinctiveness scores ($t(134.91) = -0.61, p = 0.543$),
suggesting that ambiguity and distinctiveness are fairly robust to the
differences between puns that involve identical or near homophone words.
\begin{table}
\begin{longtable}{llll}
\toprule
& Estimate & Std. Error & p-value\tabularnewline
\midrule
\endhead
Intercept & $-2.139$ & $0.306$ & $< 0.0001$\tabularnewline
Ambiguity & $1.915$ & $0.221$ & $< 0.0001$\tabularnewline
Distinctiveness & $0.264$ & $0.040$ & $<0.0001$\tabularnewline
\bottomrule
\end{longtable}
\caption{Regression coefficients using ambiguity and distinctiveness to
predict funniness ratings for all 435 sentences; $p$-values are
computed assuming that the $t$ statistic is approximately normally
distributed.}
\label{pun-regression}
\end{table}

\begin{figure}
\centering
\scalebox{0.5}{\includegraphics{Figures/pun-figure2.pdf}}
\caption{Standard error ellipses of ambiguity and distinctiveness for
each sentence type. Puns (both identical and near homophone) score
higher on ambiguity and distinctiveness; non-pun sentences score lower.}
\label{pun-ellipses}
\end{figure}
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{Figures/pun-figure3.pdf}}
\caption{Average funniness ratings and distinctiveness of 145 pun
sentences binned according to distinctiveness quartiles. Error bars are
confidence intervals.}
\label{pun-distinctiveness}
\end{figure}
As a result, we collapsed across identical and near homophone sentences
for the remaining analyses. We found that ambiguity was significantly
higher for pun sentences than non-pun sentences ($t(159.48) = 7.89, p
< 0.0001$), which suggests that the ambiguity measure
successfully captures characteristics distinguishing puns from other
phonetically ambiguous sentences. Distinctiveness was also significantly
higher for pun sentences than non-pun sentences ($t(248.99) = 6.11, p
< 0.0001$). Figure~\ref{pun-ellipses} shows the standard error ellipses for the
two sentence types in a two-dimensional space of ambiguity and
distinctiveness. Although there is a fair amount of noise in the
predictors (likely due to simplifying assumptions, the need to use
empirical measures of relatedness, and the inherent complexity of
humor), pun sentences (both identical and near homophone) tend to
cluster at a space with higher ambiguity and distinctiveness, while
non-pun sentences score lower on both measures.

We constructed a linear mixed-effects model of funniness judgments with
ambiguity and distinctiveness as fixed effects, a by-item random
intercept, and by-subject random slopes for entropy and distinctiveness.
We found that ambiguity and distinctiveness were both highly significant
predictors, with funniness increasing as each of ambiguity and
distinctiveness increases (Table~\ref{pun-regression}). Furthermore, the two measures
capture a substantial amount of the reliable variance in funniness
ratings averaged across subjects ($F(2,432) = 74.07, R^2
= 0.25, p < 0.0001$). A linear mixed effects model including a
term for the interaction between ambiguity and distinctiveness (both as
fixed effect and by-subjects random slope) showed no significant
interaction between the two ($t = 1.39, p > 0.15$).

\begin{table}
\begin{longtable}{lllllll}
\toprule
\(m_{a}\) & \(m_{b}\) & Type & Sentence & Amb. & Dist. &
Funni.\tabularnewline
\midrule
\textbf{hare} & \emph{hair} & Pun & The \textbf{magician} got so mad he
\emph{pulled} his \textbf{hare} out. & 0.15 & 7.87 & 1.71\tabularnewline
& & Non & The \textbf{hare ran rapidly} through the \textbf{fields}. &
1.43E\textsuperscript{-5} & 7.25 & -0.40\tabularnewline
\textbf{tooth} & \emph{truth } & Pun & A \textbf{dentist} has to
\emph{tell} a \textbf{patient} the \emph{whole} \textbf{tooth}. & 0.1 &
8.48 & 1.41\tabularnewline
& & Non & A \textbf{dentist} \emph{\textbf{examines}} one \textbf{tooth}
at a time. & 8.92E\textsuperscript{-5} & 7.65 & -0.45\tabularnewline
\bottomrule
\end{longtable}

\caption{Semantically relevant words, ambiguity/distinctiveness scores,
and funniness ratings for sentences from each category. Words in
boldface are semantically relevant to $m_a$; words in
italics are semantically relevant to $m_b$.}
\label{pun-scores}
\end{table}

We then examined whether the measures are able to go beyond
distinguishing puns from non-puns to predicting fine-grained levels of
funniness within puns. We found that ambiguity does not correlate with
human ratings of funniness within the 145 pun sentences ($r = 0.03, p =
0.697$). However, distinctiveness ratings correlate significantly with
human ratings of funniness within pun sentences ($r = 0.28, p <
0.001$). By separating the puns into four equal bins based on their
distinctiveness, we found that puns with distinctiveness measures in the
top-most quartile were significantly funnier than puns with
distinctiveness measures in the lower quartiles ($t(90.15) = 3.41, p
< 0.001$) (Figure~\ref{pun-distinctiveness}). This suggests that while ambiguity helps
distinguish puns from non-puns, high distinctiveness characterizes
exceptionally humorous puns. To our knowledge, our model provides the
first quantitative measure that predicts fine-grained levels of
funniness within humorous stimuli.

In addition to predicting the funniness of a sentence, the model can also be
used to reveal critical features of each pun that make it amusing. For
each sentence, we identified the set of words that is most likely to be
semantically relevant given $\vec{w}$ and each sentence
meaning $m$. Formally, we
computed $P (\vec{f} | m_{a},\vec{w})$
and
$P ( \vec{f} | m_{b}, \vec{w})$.
Table~\ref{pun-scores} shows a group of identical-homophone sentences and a group of
near-homophone sentences. Sentences in each group contain the same pair
of candidate meanings for the homophone; however, they differ on
ambiguity, distinctiveness, and funniness. Words that are most likely to
be relevant given sentence meaning \(m_{a}\ \)are in boldface; words
that are most likely to be relevant given \(m_{b}\ \)are in italics.
Qualitatively, we observe that the two pun sentences (which are
significantly funnier) have more distinct and balanced sets of
meaningful words for each sentence meaning than other sentences in their
groups. Non-pun sentences tend to have no words in support of the
meaning that was not observed. Furthermore, the boldfaced and italicized
words in each pun sentence are what one might intuitively use to explain
why the sentence is funny---for example, the fact that magicians tend to
perform magic tricks with hares, and people tend to be described as
pulling out their hair when angry.

%
%-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-Insert
%Table 3 about here
%-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-\/-

\section{Discussion}

In this chapter, we presented a simple model of gist-level sentence
processing and used it to derive formal measures that predict human
judgments of humor in puns. We showed that a noisy-channel model of
sentence processing facilitates flexible context selection, which
enables a single series of words to express multiple meanings. Our work
is one of the first to integrate a computational model of sentence
processing to analyze humor in a manner that is both intuitive and
quantitative. In addition, it is the first computational work to our
knowledge to go beyond classifying humorous versus regular sentences to
predict fine-grained funniness judgments within humorous stimuli.

The idea of deriving measures of humor from a model of general language
understanding is closely related to previous approaches, where humor is
analyzed within a framework of semantic theory and language
comprehension. \citeA{raskin2012semantic} Semantic Script Theory of Humor (SSTH)
builds upon a theory of language comprehension in which language is
understood in terms of scripts. Under this analysis, a text is funny
when it activates two scripts that are incompatible with each other.
This theory explains a number of classic jokes where the punch line
introduces a script that is incongruous with the script activated by the
joke's setup. \citeA{attardo1991script} proposed a revision to SBST in
the General Theory of Verbal Humor (GTVH), which details six
hierarchically organized knowledge resources that inform the
understanding of texts as well as the detection of humor. \citeA{nirenburg2004ontological} further formalized the ideas proposed in SBST and GTVH by
developing a system for computational semantics termed Ontological
Semantics, which includes a large concept ontology, a repository of
facts, and an analyzer that translates texts into an ontology-based
knowledge representation. This system provides rich ontological
knowledge to support in-depth language comprehension and has been
applied productively to a variety of domains \cite{nirenburg2004ontological, beale2004question, taylor2011disambiguation}. \citeA{hempelmann2006computer} used a classic joke to show that an extension to the Ontological
Semantics system can in principle detect as well as generate humorous
texts. However, to our knowledge the system has not yet been tested on a
larger body of texts to demonstrate its performance in a quantitative
manner \cite{raskin2008primer, Taylor2010}. While providing detailed analyses
that reveal many important characteristics of humor, much of the work on
formalizing humor theories falls short of predicting people's
fine-grained judgments of funniness for a large number of texts \cite{raskin1994non, ritchie2001current, ritchie2001current, attardo2002script, hempelmann2004script, veale2006computability}. In this regard, we believe that our
work advances the current state of formal approaches to humor theory. By
implementing a simple but psychologically motivated computational model
of sentence processing, we derived measures that distinguish puns from
regular sentences and correlate significantly with fine-grained humor
ratings within puns. Our approach also provides an intuitive but
automatic way to identify features that make a pun funny. This suggests
that a probabilistic model of general sentence processing (even without
the support of rich ontological semantics) may enable powerful
explanatory measures of humor.

In addition to advancing computational approaches, our work contributes
to cognitive theories of humor by providing evidence that different
factors may account for separate aspects of humor appreciation. Some
humor theorists argue that while incongruity is necessary for humor,
resolving incongruity---discovering a cognitive rule that explains the
incongruity in a logical manner---is also key \cite{ritchie1999developing, ritchie2009variants, suls1972two}. We can construe our measures as corresponding roughly
to incongruity and resolution in this sense, where ambiguity represents
the presence of incongruous sentence meanings, and distinctiveness
represents the degree to which each meaning is strongly supported by
different parts of the stimulus. Our results would then suggest that
incongruity distinguishes humorous input from regular sentences, while
the intensity of humor may depend on the degree to which incongruity is
resolved by focusing on two different supporting sets. Future work could
more specifically examine the relationship between incongruity
resolution and the measures presented in our framework.

Although our task here was limited in scope, it is a step
towards developing computational models that explain higher-order
linguistic phenomena such as humor. To address more complex jokes,
future work may incorporate more sophisticated models of language
understanding to consider the time course of sentence processing \cite{kamide2003time, mcrae1998modeling}, effects of pragmatic reasoning and
background knowledge \cite{kao2014formalizing, kao2014nonliteral, kao2015let}, and
multi-sentence discourse \cite{polanyi1988formal, chambers2008unsupervised}.
Our approach could also benefit greatly from the rich commonsense
knowledge encoded in the Ontological Semantics system and may be
combined with it to measure ambiguity and distinctiveness at the script
level rather than at the level of the sentence.

Previous research on creative language use such as metaphor, idioms, and
irony has contributed a great deal to our understanding of the cognitive
mechanisms that enable people to infer rich meanings from sparse and
often ambiguous linguistic input \cite{lakoff2009more, nunberg1994idioms, gibbs1991psychological}. We hope that our work on humor
contributes to theories of language understanding to account for a wider
range of linguistic behaviors and the social and affective functions
they serve. By deriving the precise properties of sentences that make us
laugh, our work brings us one step closer to understanding that funny
thing called humor (pun intended).

%Acknowledgements
%
%This work was supported by the National Science Foundation Graduate
%Research Fellowship to JTK, research grant NSF 0953870 and fellowships
%from the Alfred P. Sloan Foundation and the Center for Advanced Study in
%the Behavioral Sciences to RL, and a James S. McDonnell Foundation
%Scholar Award to NDG.



\chapter{Conclusions}
%


%``Words are a pretext. It is the inner bond that draws one person to another, not words.''
%? Rumi


This thesis set out to answer two questions: how do people understand creative uses of language, and how do these types of uses give rise to the range of social and emotional effects that we experience in everyday communication? To answer these questions, we took a modeling approach that builds upon general theories of language comprehension and produces graded, quantitative results that capture the subtleties of human interpretation. In Chapter 2, we introduced the idea of communicative goals to the RSA framework and formalized a notion of the relevance principle. We showed that basic principles of communication, combined with background knowledge and social reasoning, allows the model to appropriately interpret hyperbolic utterances as well as their associated affects. 
%These results suggest that the affective subtext of creative and figurative uses of language may arise naturally from standard language understanding mechanisms. 
In Chapter 3, we further extended the space of communicative goals to include a richer representation of emotions as well as contextually determined topics. We showed that this extension allowed RSA models to interpret a range of figurative uses such as verbal irony and metaphor, again using the same basic principles of communication. In Chapter 4, we relaxed the assumption that speaker and listener share the same background knowledge, and allowed the listener to reason about common ground given various utterances. We showed that the model infers a higher probability of common ground given figurative utterances than literal ones, suggesting that figurative language may strengthen social bonds via inferences about common ground. Finally, in Chapter 5, we used a simple model of sentence comprehension to derive quantitative measures of humor that are motivated by both general theories of language comprehension under noise and the incongruity theory of humor. 
With the Rational Speech-Acts framework, we were able to introduce increasingly rich and psychologically realistic components of communication, and to show how they are responsible for different effects.
Taken together, this work sheds light on scientific theories of creative language use and advances formal approaches to language, such that computational models can explain a broader range of phenomena that enrich our linguistic and social lives.

\section{Limitations and future directions}

``The past is always tense, the future perfect.'' 
--- Zadie Smith

The models we described capture rich aspects of language use that were previously beyond the reach of quantitative study. However, it is only the beginning. Each chapter opens up new directions for future research. While Chapter 2 focused on the nonliteral interpretation of number words, many hyperbolic uses involve non-numeric utterances, such as ``Andrew \emph{never} does the dishes," or ``Bob is the best cook in the world.'' Because the semantics of these types of utterances are more complex than number words, there is more opportunity to explore how semantic and pragmatic information interact to produce various types of interpretations.
Some preliminary work on universal quantifiers (e.g. ``Cam ate \emph{all} of the cookies'') shows how different contexts may shift the interpretation from literal (Cam ate literally all of the cookies), to imprecise (Cam ate almost all of the cookies), to domain restricted (Cam ate all of the \emph{chocolate chip} cookies) to hyperbolic (Cam ate way too many cookies, and I am upset!) \cite{kaoall}. In Chapter 3, we showed how ironic interpretations could arise through reasoning about speakers' emotional valence and arousal. However, our model did not demonstrate a classic asymmetry effect in verbal irony, where ironic insults (e.g. ``What a wonderful idea'') are more common and rated as more appropriate than ironic compliments (e.g. ``What a terrible idea''). This asymmetry is not explained by inferences about speakers' emotional goals alone, and may require additional assumptions in the model to capture. And while the extended RSA model produces appropriate interpretations of metaphor, so far it has only been tested on simple nominal metaphors with a limited set of features, and may require significant extensions to account for the more complex or extended metaphors that we see in literature and poetry. Furthermore, more work is needed to test the boundaries of the RSA framework when applied to figurative language, for example whether it can predict other types of figurative use such as understatement and metonymy where the relevant goals and questions under discussion may be less clear.

Regarding the social implications of figurative language discussed in Chapter 4, our model currently does not account for individual or cultural differences in norms of nonliteral use. Some individuals may be more or less prone to speak sarcastically, and some cultures may be more or less tolerant of nonliteral communication. These types of high-level knowledge of personality traits and social norms may also play into people's inferences regarding nonliteral uses of language and are useful to incorporate in our models \cite{katz2004saying}. Finally, in Chapter 5, we used puns as a case study for formalizing the concept of incongruity and deriving quantitative measures of humor. Through preliminary analysis, we find that the same measures of incongruity---ambiguity and distinctiveness---can also be used to quantify the humor in figurative uses such as hyperbole and irony. Future work could examine this generalization in more detail, with the goal of predicting humor in a broader class of creative language.

Beyond the need for extensions to help cover a wider set of creative language use, a deeper question regarding this work is: how ``creative'' \emph{are} these uses of language? Many of the examples of hyperbole, verbal irony, and metaphor examined in this thesis occur in everyday language in fairly predictable and conventionalized ways. While adding interest and color to communication, they are not the most ingenious demonstrations of linguistic creativity. Could the models described here also interpret and appreciate the ``true'' creativity that we see in the works of Shakespeare, Emily Dickinson, and Pablo Neruda?
%
The answer is, at least in its current form, no. 
Fully understanding figurative uses such as ``Hope is the thing with feathers'' and ``I want to do with you what spring does with the cherry trees'' requires an amount of world knowledge that may not yet be fully amenable to quantification. Many of the most evocative metaphors communicate information along many dimensions at once, and it may be challenging to identify all of the features of the source and target domains that are relevant for producing figurative interpretations and well as associated poetic effects\footnote{For example, what the poet wants to do with the addressee is metaphorically similar to what spring does with the cherry trees, but it is difficult to put into words precisely what the action could be, and in what dimensions are the two actions similar (except, perhaps, that they both change the receiver of the action in positive ways).}. 

%We believe that the models we described bring us closer to understanding how people interpret creative and figurative language. However, it remains to be seen whether the principles of communication that we use to understand simple metaphors such as ``John is a shark'' are sufficient for poetic metaphors as well. 
In order to truly understand and appreciate the best examples of linguistic creativity, more research is needed to identify predictors of what makes a metaphor or figurative use ``good.'' For example, is the aptness of a metaphor predicted by the number of dimensions of meaning it can communicate? Could aptness and aesthetic value be related to other types of measures, such as ambiguity (existence of multiple interpretations) and distinctiveness (difference in contextual support for one interpretation versus the other)? How do features such as concreteness and imageability factor into measures of aesthetic quality, or phonological features such as rhyme and alliteration? While some work has been done in this direction to identify the features of high-quality poetic language \cite{kao2012computational, kao2015computationalpoetry}, these questions are currently beyond the scope of the models described in this thesis. In future work, an approach that combines figurative interpretation with features of aesthetic value could be the most promising for understanding poetic language in a quantitative and psychologically motivated manner.

%Metaphor aptness, aesthetics, poetic language. Creativity? 

Finally, the models presented in Chapters 2 and 3 were tested on specific domains (e.g. prices, weather, animals), where we elicited people's background knowledge through behavioral experiments. While this approach directly taps into people's knowledge and is relatively precise, it is difficult to scale up these elicitation experiments to cover a wide range of domains. By using data mining and machine learning methods (e.g. scraping websites for distributions of prices, or automatically learning features of animals from online encyclopedias), we may be able to automate the knowledge elicitation process. This would allow us to test the models on a larger set of utterances and domains, including naturally occurring examples that we could mine from corpora. Ultimately, a combination of methods---targeted Bayesian cognitive models, behavioral experiments, and large-scale corpus analyses---could help us make the most progress towards understanding the many aspects of creative language use.
%
% meant to 
%While the specific models  believe our modeling approach can make good progress on these questions, but also has limitations. Requires empirical work, together with analysis on large corpora.

\section{Final remarks}

Modeling psychological phenomena is often a process of peeling away layers of skin to take apart and reconstruct the skeleton underneath. In this thesis, I dissected several cases of creative language use and discovered that it is held up by the same structure that supports many other aspects of cognition, such as reasoning about other people and reasoning under uncertainty more generally.
By discovering this common structure, I showed how people harness basic principles of language comprehension to derive socially meaningful information from a range of creative use, suggesting that our ability to understand and appreciate creative uses of language arises organically from our general capacity to understand language and to simulate other minds. 
%The idea that creative and social uses of language such as figurative meaning, humor, and even poetry are natural consequences of human intelligence is, I think, a rather beautiful thing.
In addition, I identified and formalized other components that may be critical for standard language understanding, such as reasoning about speakers' communicative goals, emotional attitudes, and uncertainty over shared background knowledge.
%One of the goals of this thesis was to use a novel computational framework to formalize ideas established by great thinkers in the field, and in the process discover new insights that add to the scientific discourse. 
By modeling the ways in which people interpret and appreciate creative uses of language, I hope that this work %examining the details of figurative interpretation as well as 
bridges rational accounts of language understanding with the creative uses that express our individuality and bring us closer together.

%are not superfluous artifacts, and may instead be built into the fabric of human cognition, is,
%
%Understand- ing how people harness the power of words to influence and inspire is important both for understanding our literary heritage and for shedding light on how meaningful communication occurs in a digital world overflowing with information. My research combines computational and experimental approaches to in- vestigate the ways in which people use language to evoke deeply human sensations such as poetic beauty, humor, and social intimacy. By incorporating behavioral experiments, text corpora, and probabilistic models to examine creative language use, my research advances our knowledge of how people derive rich social and emotional information from natural language.
%
%
%I hope I have succeeded in convincing you of this too.
%
%This thesis aims to dissect creative language and identify the skeleton that holds it up. Through the process, we show how underneath a layer of sparkly skin, many creative and playful uses of language are powered by the same abilities that show up in all other aspects of our lives and that we often take for granted, such as reasoning under uncertainty, identifying  



\appendix

\bibliographystyle{apacite}
\bibliography{mybib}
\end{document}